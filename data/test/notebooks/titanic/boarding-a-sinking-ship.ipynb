{"cells":[
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "#Boarding A Sinking Ship\n#### A Deep Dive Into The Unknown Waters of Machine Learning\n\n## Introduction\n\nHaving done a data science online course for a few months I have decided to try and put what I have learned to the test rather than just completing assignment after assignment.\n\nHaving looked around Kaggle and with the advise of a couple of friends I have decided to have a crack at the Titanic Dataset.\n\n## Aim\n\nMy main aim will be to create a machine learning script that can predict whether a passenger survived dependent on a certain array of variables. I will do this by employing a random forest approach (as recommended in the competition!)\n\n## Step 1: Importing Libraries\n\nI will first import all of the necessary libraries I'll need"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "#import the below mathematical libraries\nimport pandas as pd\nimport numpy as np\nimport csv as csv\n\n#import libraries for plotting data\nimport matplotlib.pyplot as plt\nimport seaborn as sb\n\n#import machine learning libraries\nfrom sklearn.ensemble import RandomForestClassifier "
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "## Step 2: Import & Check the Dataset\n\nI will now import the training data (train.csv) and use pd.describe() to get a summary of all of the fields"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "#Read in the train.csv file\ntrain_data = pd.read_csv(\"../input/train.csv\")\n\n#Print the columns and their corresponding types\nprint(train_data.info())\n\n#Print a summary of all the numerical fields\nprint(train_data.describe())"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "As can be seen a runtime error is thrown due to the fact that the age field has unknown values. From this I can see two options:\n\n1. Delete all the rows with NaN values. Though this could potentially skew the data.\n\n2. We can estimate the ages for rows with NaN values using the median values of passengers on board.\n\nTo decide this I will look at the distribution of ages for this patient."
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# We have to temporarily drop the rows with 'NA' values\n# because the Seaborn plotting function does not know\n# what to do with them\n\nsb.pairplot(train_data[['Sex','Pclass', 'Age', 'Survived']].dropna(), hue='Sex')"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "As can be seen the age seems to be positively skewed, meaning it will probably be best for us to replace the ages rather than remove the ones with missing ages. \n\nWhat is also interesting to notice is the gender of those who died compared to those who survived. Those who died were primarily male and those who survived were primarily female.\n\nSo we now have a course of action, I plan to create an estimated age for all the null age values, I will also give the sex field a numeric value so that these fields can be utilised by the random forest process.\n\n## Step 3: Data Munging\n\n### Age_All & Age_Estimated Fields\n\nI will first create the estimated age field. I am going to do this by calculating the median age for each passenger class and then use these medians to replace any age that we don't have."
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "#I'll first calculate the median age for each pclass field and insert it into a dictionary\n#Create dictionary and list of all the unique pclass values\npclass_values = train_data['Pclass'].unique()\nmedian_age_pclass = {}\n\n#Loop through all pclass values and calculate the median for each of them\nfor i in pclass_values:\n    median_age_pclass[i] = train_data[train_data['Pclass'] == i]['Age'].median()\n\n#print dictionary to check results are as expected\nprint(median_age_pclass)\n\n#We now have the median value for each task!"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "We now have the medians. So I am now going to create an additional field called Age_All which will include the estimated along with the actual ages we have. I'll also include a flag called Age_Estimated that denotes whether the value in Age_All is estimated or not. "
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "#Create the new age_all field which is initially a direct copy of Age\ntrain_data['Age_All'] = train_data['Age']\n\n#Create the Age_Estimated Flag\ntrain_data['Age_Estimated'] = pd.isnull(train_data['Age']).astype(int)\n\n#Now Update all instances where Age_Estimated flag = 1 with the corresponding Pclass median\nfor i in pclass_values:\n    train_data.loc[(train_data['Age_Estimated'] == 1) & (train_data['Pclass'] == i),'Age_All'] = median_age_pclass[i]\n\n#Print the top 20 rows of a few select columns to spot check if they correspond accordingly\nprint(train_data[['Sex','Pclass','Age','Age_All','Age_Estimated','Survived']].head(20))"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": ""
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "#I'll first calculate the median age for each pclass field and insert it into a dictionary\n#Create dictionary and list of all the unique pclass values\nmedian_fare_pclass = {}\n\n#Create subset of data where Fare > 0\ntd_fare = train_data.loc[train_data['Fare'] >0,('Pclass','Fare')]\n\n#Loop through all pclass values and calculate the median for each of them\nfor i in pclass_values:\n    median_fare_pclass[i] = td_fare[td_fare['Pclass'] == i]['Fare'].median()\n    \n#Create the new age_all field which is initially a direct copy of Age\ntrain_data['Fare_All'] = train_data['Fare']\n\n#Now Update all instances where Age_Estimated flag = 1 with the corresponding Pclass median\nfor i in pclass_values:\n    train_data.loc[((train_data['Fare_All'] == 0) | (train_data['Fare_All'].isnull())) & (train_data['Pclass'] == i),'Fare_All'] = median_fare_pclass[i]    \n\n#Print the all rows where Fare = 0\nprint(train_data.loc[(train_data['Fare'] == 0),('Sex','Pclass','Fare','Fare_All','Survived')])"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "As can be seen just quickly overlooking the table we have successfully created both the Age_Estimated abd Age_All fields. \n\nThis means we can now go on to creating a numeric field that represents the string! Once we have done this we can create our cleaned dataset and get on with some Machine Learning!!!\n\n### Numerical Field with Sex Data\n\nHere I am going to map values from the Sex field, give them a numerical value and then put it into the newly created Gender Field.\n\nThe mapping will be as follows, 'female' = 1 and 'male' = 2"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "#Create a mapping dictionary\nSex_Map = {'female': 1,'male': 2}\nEmbarked_Map = {'Q':1,'C':2,'S':3,'X':0}\n\n#Set all NaN values in Embarked Field = X\ntrain_data.loc[(train_data['Embarked'].isnull()),'Embarked'] = 'X'\n\n#Use mapping dictionary to map values from 'Sex' field to the new field 'Gender'\ntrain_data['Gender'] = train_data['Sex'].map(Sex_Map).astype(int)\n\n#Use mapping dictionary to map values from 'Embarked' field to the new field 'Embarked_From'\ntrain_data['Embarked_From'] = train_data['Embarked'].map(Embarked_Map).astype(int)\n\n#Print the top 20 rows of a few select columns to spot check if they correspond accordingly\nprint(train_data[['Sex','Gender','Embarked','Embarked_From','Survived']].head(20))"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "We now have a gender column! Now onto the final step.\n\n### Creating Cleaned Dataset\n\nWe now have all of the fields we will require for use in our Random Forest Algorithm. We now need to reduce the columns down to just the ones that are int or float and then transfer the dataset from the Dataframe we are currently using to a NumPy array.\n\nI am firstly going to analyse the all the columns we have available"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "#Print the columns and their corresponding types\nprint(train_data.info())"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "From this I think we will make the cleaned dataset as follows:\n\n1. Survived\n2. Pclass\n3. Age_All\n4. Age_Estimated\n5. Gender\n6. SibSp\n7. Parch\n8. Fare_All\n9. Embarked_From\n\nI will now create this dataset.\n"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "#Make the clean dataset and put it into a NumPy array\ntrain = train_data[['Survived','Pclass','Age_All','Age_Estimated','Gender','SibSp','Parch','Fare_All','Embarked_From']].values\n\n#Print top 5 rows of NumPy array to double check the format is correct\nprint(train[:][0:5])"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "Thats all good! So now that we have the cleaned training dataset we are now going it to train the random forest algorithm. We will then run it on the test data to see how accurate our model is.\n\n## Step 4: Machine Learning\n\n### Cleaning Test Data\n\nSo the first thing we are going to do is clean the test data in the same fashion that we cleaned the training data. This can be seen below"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "#Read in the test.csv file\ntest_data = pd.read_csv(\"../input/test.csv\")\n\n#I'll first calculate the median age for each pclass field and insert it into a dictionary\n#Create dictionary and list of all the unique pclass values\nmedian_test_pclass = {}\n\n#Loop through all pclass values and calculate the median for each of them\nfor i in pclass_values:\n    median_test_pclass[i] = test_data[test_data['Pclass'] == i]['Age'].median()\n\n#Create the new age_all field which is initially a direct copy of Age\ntest_data['Age_All'] = test_data['Age']\n\n#Create the Age_Estimated Flag\ntest_data['Age_Estimated'] = pd.isnull(test_data['Age']).astype(int)\n\n#Now Update all instances where Age_Estimated flag = 1 with the corresponding Pclass median\nfor i in pclass_values:\n    test_data.loc[(test_data['Age_Estimated'] == 1) & (test_data['Pclass'] == i),'Age_All'] = median_age_pclass[i]\n    \n#Create the new age_all field which is initially a direct copy of Age\ntest_data['Fare_All'] = test_data['Fare']\n\n#Now Update all instances where Age_Estimated flag = 1 with the corresponding Pclass median\nfor i in pclass_values:\n    test_data.loc[((test_data['Fare_All'] == 0) | (test_data['Fare_All'].isnull())) & (test_data['Pclass'] == i),'Fare_All'] = median_fare_pclass[i]    \n    \n#Set all NaN values in Embarked Field = X\ntest_data.loc[(test_data['Embarked'].isnull()),'Embarked'] = 'X'\n\n#Use mapping dictionary to map values from 'Sex' field to the new field 'Gender'\ntest_data['Gender'] = test_data['Sex'].map(Sex_Map).astype(int)\n\n#Use mapping dictionary to map values from 'Embarked' field to the new field 'Embarked_From'\ntest_data['Embarked_From'] = test_data['Embarked'].map(Embarked_Map).astype(int)\n\n#Make the clean dataset and put it into a NumPy array\ntest = test_data[['Pclass','Age_All','Age_Estimated','Gender','SibSp','Parch','Fare_All','Embarked_From']].values\n\n#Make a table with the passenger ids for use in the output file\nPasID = test_data['PassengerId']\n\n#Print top 5 rows of NumPy array to double check the format is correct\nprint(test[:][0:6])\n\n"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "print(test_data.loc[(test_data['Fare_All'].isnull()),'Fare_All'])"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "We now have the test file in a NumPy Array. Do you know what this means?! We can finally get to the machine learning!!!\n\n### Run Random Forrest Algorithm\n\nI will now train the algorithm and then apply it to the test data and get it to predict the survived value."
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "print('Training!')\n\n#Create the random forest object which will include all the parameters\n#for the fit\nforest = RandomForestClassifier(n_estimators = 1000)\n\n#Fit the training data to the Survived labels and create the decision trees\nforest = forest.fit(train[0::,1::],train[0::,0])\n\nprint('Test!')\n\n#Take the same decision trees and run it on the test data\noutput = forest.predict(test)\n\n#Print top 5 rows\nprint(output[:][0:5])"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "And thats all been done successfully! Now we need to output a CSV file so that we can submit the answers to Kaggle and test the accuracy of the data.\n\n### Export CSV File\n\nThe rules state that we need a csv file with 418 rows (the number of rows in the test data) with two columns, PassengerID and Survived. "
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "#Create Dataframe with correct structure and using the output and ids arrays\npredictions_file = pd.DataFrame({\n        \"PassengerId\": PasID\n       ,\"Survived\"   : output\n    })\n\n#Write DataFrame to CSV\npredictions_file.to_csv('submission.csv', index=False)\n\n#Print first 5 rows of file and close\nprint(predictions_file.head())\n\n"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "We have now exported the CSV. Now to submit I will summarise the result I get below\n\n## Results:\n### Score: 0.29\n### To-Do: \n\n1. Could possibly add fares back in once I remove the nulls or add predicted results. \n2. Add a numerical input for departure origin\n3. Possibly use a better machine learning algorithm"
 }
],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}}, "nbformat": 4, "nbformat_minor": 0}