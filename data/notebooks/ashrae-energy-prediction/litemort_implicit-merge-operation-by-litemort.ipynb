{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge operation would generate very large dataframes and need huge memory. [LiteMORT](https://github.com/closest-git/LiteMORT)  support implicit merging operation. That is,only send the merge information to LiteMORT. Then LiteMORT will generate every merged feature and its histogram  automatically. Modern GBDT only needs these histograms. So saved huge memory, while the accuracy remains the same. \n",
    "\n",
    "In this case(to merge 25 features), implicit merging only need about 4G memory,  which is much less than 12G memory needed by standard merge operation.  It's easy to implicit merge hundreds of features if you want.\n",
    "\n",
    "The following code showes the merge information to deal with weather and building dataframe. \n",
    "\n",
    "```python\n",
    "merge_infos = [  \n",
    "\t{'on': ['site_id', 'timestamp'], 'dataset': weather_df, \"desc\": \"weather\"},  \n",
    "\t{'on': ['building_id'], 'dataset': building_merge_, \"desc\": \"building\", \"feat_info\": \t\t\tfeat_infos},  \n",
    "]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building_metadata.csv  test.csv   weather_test.csv\r\n",
      "sample_submission.csv  train.csv  weather_train.csv\r\n",
      "__notebook__.ipynb  __output__.json\r\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold,KFold\n",
    "from sklearn.metrics import log_loss, mean_squared_error\n",
    "import datetime\n",
    "import time\n",
    "import seaborn as sns; sns.set(style=\"ticks\", color_codes=True)\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "# import shap as shap\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from pandas.api.types import is_datetime64_any_dtype as is_datetime\n",
    "from pandas.api.types import is_categorical_dtype\n",
    "%load_ext wurlitzer\n",
    "\n",
    "!ls '../input/ashrae-energy-prediction/'\n",
    "!ls '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LiteMORT is a new open source gradient boosting lib( https://github.com/closest-git/LiteMORT).  Install and import it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://test.pypi.org/simple/\r\n",
      "Collecting litemort==0.1.18\r\n",
      "\u001b[?25l  Downloading https://test-files.pythonhosted.org/packages/84/d2/2295a104a69d20ccb2b5734da2a4d7256744b6afbb6d0c4ee45f3905a7c8/litemort-0.1.18-py3-none-any.whl (1.2MB)\r\n",
      "\u001b[K     |████████████████████████████████| 1.2MB 2.8MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from litemort==0.1.18) (1.2.1)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.6/site-packages (from litemort==0.1.18) (0.21.3)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from litemort==0.1.18) (1.16.4)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->litemort==0.1.18) (0.13.2)\r\n",
      "Installing collected packages: litemort\r\n",
      "Successfully installed litemort-0.1.18\r\n",
      "0.1.18\n"
     ]
    }
   ],
   "source": [
    "!pip install -i https://test.pypi.org/simple/  litemort==0.1.18\n",
    "import litemort\n",
    "from litemort import * \n",
    "print(litemort.__version__)\n",
    "#profile = LiteMORT_profile()\n",
    "#profile.Snapshot(\":\");          profile.Stat(\":\",\"::\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some control flags. Switch these flags, you can compare the memory with and without implicit merging.  \n",
    "You can also compare the performance between LiteMORT and lightGBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== ImplicitMerge=True gbm=MORT ======\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#isMORT = len(sys.argv)>1 and sys.argv[1] == \"mort\"\n",
    "isMORT = True    #Switch this flag to compare the performance between LiteMORT and lightGBM\n",
    "isImplicitMerge = True   #Switch to compare the memory usage \n",
    "gbm='MORT' if isMORT else 'LGB'\n",
    "use_ucf=True\n",
    "nTargetMeter=4\n",
    "data_root = '../input/ashrae-energy-prediction/'\n",
    "print(f\"====== ImplicitMerge={isImplicitMerge} gbm={gbm} ======\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UCF functions,  from https://www.kaggle.com/yamsam/new-ucf-starter-kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2260080\n"
     ]
    }
   ],
   "source": [
    "def LoadUCF(data_root):\n",
    "    ucf_root = '../input/ashrae-ucf-spider-and-eda-full-test-labels'\n",
    "    ucf_leak_df = pd.read_pickle(f'{ucf_root}/site0.pkl')\n",
    "    ucf_leak_df['meter_reading'] = ucf_leak_df.meter_reading_scraped\n",
    "    ucf_leak_df.drop(['meter_reading_original', 'meter_reading_scraped'], axis=1, inplace=True)\n",
    "    ucf_leak_df.fillna(0, inplace=True)\n",
    "    ucf_leak_df.loc[ucf_leak_df.meter_reading < 0, 'meter_reading'] = 0\n",
    "    ucf_leak_df = ucf_leak_df[ucf_leak_df.timestamp.dt.year > 2016]\n",
    "    print(len(ucf_leak_df))\n",
    "    return ucf_leak_df\n",
    "LoadUCF(\"\")    #some testing code\n",
    "    \n",
    "def ReplaceUCF():\n",
    "    print('ReplaceUCF......')  \n",
    "    leak_score = 0\n",
    "    leak_df = LoadUCF(data_root)\n",
    "    sample_submission.loc[sample_submission.meter_reading < 0, 'meter_reading'] = 0\n",
    "    for bid in leak_df.building_id.unique():\n",
    "        temp_df = leak_df[(leak_df.building_id == bid)]\n",
    "        for m in temp_df.meter.unique():\n",
    "            v0 = sample_submission.loc[(test_df.building_id == bid) & (test_df.meter == m), 'meter_reading'].values\n",
    "            v1 = temp_df[temp_df.meter == m].meter_reading.values\n",
    "            leak_score += mean_squared_error(np.log1p(v0), np.log1p(v1)) * len(v0)\n",
    "            sample_submission.loc[(test_df.building_id == bid) & (test_df.meter == m), 'meter_reading'] = temp_df[\n",
    "                temp_df.meter == m].meter_reading.values\n",
    "    print('UCF score = ', np.sqrt(leak_score / len(leak_df)))\n",
    "    sample_submission.to_csv('submission.csv', index=False, float_format='%.4f')\n",
    "    print(sample_submission.head(100),sample_submission.tail(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class for whether data and some lag features.\n",
    "Just modify lag_day to add more features.   \n",
    "For example self.lag_day==[3,72] or self.lag_day==[3,24,72]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Whether(object):\n",
    "    def __init__(self, source, data_root,params=None):\n",
    "        self.source = source\n",
    "        self.data_root = data_root\n",
    "        self.lag_day=[3,72]     #,[]72\n",
    "        #self.pkl_path = f'{pickle_root}/Whether_{source}_[{self.lag_day}]_.pickle'\n",
    "        self.lag_feat_list=[]\n",
    "\n",
    "    def TimeAlignment(self,weather_df):   #https://www.kaggle.com/nz0722/aligned-timestamp-lgbm-by-meter-type\n",
    "        print(f\"TimeAlignment@{self.source}\\tdf{weather_df.shape}......\")\n",
    "        weather_key = ['site_id', 'timestamp']\n",
    "        temp_skeleton = weather_df[weather_key + ['air_temperature']].drop_duplicates(subset=weather_key).\\\n",
    "            sort_values(by=weather_key).copy()\n",
    "        temp_skeleton['temp_rank'] = temp_skeleton.groupby(['site_id', temp_skeleton.timestamp.dt.date])[\n",
    "            'air_temperature'].rank('average')\n",
    "        # create a dataframe of site_ids (0-16) x mean hour rank of temperature within day (0-23)\n",
    "        df_2d = temp_skeleton.groupby(['site_id', temp_skeleton.timestamp.dt.hour])['temp_rank'].mean().unstack(level=1)\n",
    "        # Subtract the columnID of temperature peak by 14, getting the timestamp alignment gap.\n",
    "        site_ids_offsets = pd.Series(df_2d.values.argmax(axis=1) - 14)\n",
    "        site_ids_offsets.index.name = 'site_id'\n",
    "\n",
    "        weather_df['offset'] = weather_df.site_id.map(site_ids_offsets)\n",
    "        weather_df['timestamp_aligned'] = (weather_df.timestamp - pd.to_timedelta(weather_df.offset, unit='H'))\n",
    "        weather_df['timestamp'] = weather_df['timestamp_aligned']\n",
    "        del weather_df['timestamp_aligned']        \n",
    "        return weather_df\n",
    "\n",
    "    def df(self):        \n",
    "        weather_df = pd.read_csv(f'{self.data_root}/weather_{self.source}.csv', parse_dates=['timestamp'],\n",
    "                    dtype={'site_id': np.uint8, 'air_temperature': np.float16,\n",
    "                           'cloud_coverage': np.float16, 'dew_temperature': np.float16,'precip_depth_1_hr': np.float16})\n",
    "        print(f\"{weather_df.shape}\\n{weather_df.isna().sum()}\")\n",
    "        weather_df = self.TimeAlignment(weather_df)\n",
    "        #w_sum = weather_df.groupby('site_id').apply(lambda group: group.isna().sum())\n",
    "        weather_df = weather_df.groupby('site_id').apply(lambda group: group.interpolate(limit_direction='both'))\n",
    "        w_sum = weather_df.groupby('site_id').apply(lambda group: group.isna().sum())\n",
    "        for days in self.lag_day:\n",
    "            f_list = self.add_lag_feature(weather_df, window=days)\n",
    "            self.lag_feat_list.extend(f_list)\n",
    "        print(f\"weather_{self.source}={weather_df.shape} features={weather_df.columns}\")  #weather_df.head()\n",
    "        return weather_df\n",
    "\n",
    "    def add_lag_feature(self,weather_df, window=3):\n",
    "        group_df = weather_df.groupby('site_id')\n",
    "        feat_list=[]\n",
    "        cols = ['air_temperature', 'cloud_coverage', 'dew_temperature', 'precip_depth_1_hr', 'sea_level_pressure',\n",
    "                'wind_direction', 'wind_speed']\n",
    "        rolled = group_df[cols].rolling(window=window, min_periods=0)\n",
    "        lag_mean = rolled.mean().reset_index().astype(np.float16)\n",
    "        lag_max = rolled.max().reset_index().astype(np.float16)\n",
    "        lag_min = rolled.min().reset_index().astype(np.float16)\n",
    "        lag_std = rolled.std().reset_index().astype(np.float16)\n",
    "        for col in cols:\n",
    "            feat_list.append(f'{col}_mean_lag{window}')\n",
    "            weather_df[f'{col}_mean_lag{window}'] = lag_mean[col]\n",
    "            if col=='air_temperature':\n",
    "                feat_list.append(f'{col}_max_lag{window}')\n",
    "                weather_df[f'{col}_max_lag{window}'] = lag_max[col]\n",
    "                feat_list.append(f'{col}_min_lag{window}')\n",
    "                weather_df[f'{col}_min_lag{window}'] = lag_min[col]\n",
    "                feat_list.append(f'{col}_std_lag{window}')\n",
    "                weather_df[f'{col}_std_lag{window}'] = lag_std[col]\n",
    "        return feat_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If isImplicitMerge is False, the ASHRAE_data class would generate dataframe with all-features. The following is the standard merge code:\n",
    "\n",
    "\n",
    "```python\n",
    "target_train_df = target_train_df.merge(self.building_meta_df, on='building_id', how='left')\n",
    "                target_train_df = target_train_df.merge(self.weather_df, on=['site_id', 'timestamp'], how='left')\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "If isImplicitMerge is True, no merging operation. Only record the merge infomation like this  \n",
    "\n",
    "\n",
    "```python\n",
    "self.merge_infos = [\n",
    "                    {'on': ['site_id', 'timestamp'], 'dataset': self.weather_df, \"desc\": \"weather\"},\n",
    "                    {'on': ['building_id'], 'dataset': self.building_merge_, \"desc\": \"building\",\n",
    "                     \"feat_info\": feat_infos},\n",
    "                ]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASHRAE_data(object):\n",
    "    def __init__(self, source,data_root,building_meta_df,weather_df):\n",
    "        self.category_cols = ['building_id', 'site_id', 'primary_use']  # , 'meter'\n",
    "\n",
    "\n",
    "        self.source = source\n",
    "        self.data_root = data_root\n",
    "        self.building_meta_df = building_meta_df\n",
    "        self.weather_df = weather_df\n",
    "        feats_whether =[e for e in list(self.weather_df.columns) if e not in ('site_id','offset', 'timestamp')]\n",
    "        self.feature_cols = ['square_feet', 'year_built'] + [\n",
    "            'hour', 'weekend',  # 'month' , 'dayofweek'\n",
    "            'building_median']+feats_whether\n",
    "        print(f\"ASHRAE_data_{self.source}......category={self.category_cols}\\nfeature_cols={self.feature_cols}......\")\n",
    "        #self.some_rows = 15000\n",
    "        self.some_rows = None\n",
    "        self.df_base = self.Load_Processing()\n",
    "        self.df_base_shape = self.df_base.shape\n",
    "        print(f\"ASHRAE_data_ df_base={self.df_base_shape}\")\n",
    "\n",
    "    def fit(cls, df):\n",
    "        pass\n",
    "\n",
    "    def data_X_y(self,target_meter):\n",
    "        feat_v0 = self.feature_cols + self.category_cols\n",
    "        feat_infos = {\"categorical\": self.category_cols}\n",
    "        train_df = self.df_base\n",
    "        print(f\"{self.source}_X_y@{target_meter} df_base={train_df.shape}......\")\n",
    "        pkl_path = f'./_ashrae_{self.source}_T{target_meter}_{self.some_rows}_{\"Mg\"if isImplicitMerge else \"\"}_.pickle'\n",
    "        self.merge_infos = []\n",
    "\n",
    "        target_train_df = train_df[train_df['meter'] == target_meter]\n",
    "        print(f\"target@{target_meter}={target_train_df.shape}\")\n",
    "        if isImplicitMerge:\n",
    "            building_site = self.building_meta_df[['building_id', 'site_id']]\n",
    "            target_train_df = target_train_df.merge(building_site, on='building_id', how='left')  # add 'site_id'\n",
    "            self.building_merge_ = self.building_meta_df[['building_id', 'primary_use', 'square_feet', 'year_built']]  # only need 3 col for merge\n",
    "            feat_v0 = feat_v0 + ['timestamp']\n",
    "            # self.weather_df = self.weather_df[:1100]\n",
    "            feat_v1 = list(set(feat_v0).intersection(set(list(self.weather_df.columns))))\n",
    "            # feat_v1 = ['site_id','timestamp','precip_depth_1_hr']       #测试需要\n",
    "            self.weather_df = self.weather_df[feat_v1]\n",
    "            self.merge_infos = [\n",
    "                {'on': ['site_id', 'timestamp'], 'dataset': self.weather_df, \"desc\": \"weather\"},\n",
    "                {'on': ['building_id'], 'dataset': self.building_merge_, \"desc\": \"building\",\n",
    "                 \"feat_info\": feat_infos},\n",
    "            ]\n",
    "        else:\n",
    "            target_train_df = target_train_df.merge(self.building_meta_df, on='building_id', how='left')\n",
    "            target_train_df = target_train_df.merge(self.weather_df, on=['site_id', 'timestamp'], how='left')\n",
    "        feat_v1 = list(set(feat_v0).intersection(set(list(target_train_df.columns))))\n",
    "        X_train = target_train_df[feat_v1]\n",
    "        print(f\"data_X__@{target_meter}={X_train.shape}\\toriginal={target_train_df.shape}\\tmerge={isImplicitMerge}\")\n",
    "        if (self.source == \"train\"):\n",
    "            y_train = target_train_df['meter_reading_log1p'].values\n",
    "        else:\n",
    "            y_train=None\n",
    "        del target_train_df\n",
    "        gc.collect()\n",
    "\n",
    "        return X_train, y_train\n",
    "\n",
    "    def Load_Processing(self):\n",
    "        df = pd.read_csv(f'{self.data_root}/{self.source}.csv', dtype={'building_id': np.uint16, 'meter': np.uint8},\n",
    "                         parse_dates=['timestamp'])\n",
    "        ucf_leak_df = LoadUCF(data_root)\n",
    "        #df = pd.concat([df, ucf_leak_df])\n",
    "        if self.source==\"train\":    #All electricity meter is 0 until May 20 for site_id == 0\n",
    "            df = df.query('not (building_id <= 104 & meter == 0 & timestamp <= \"2016-05-20\")')\n",
    "            if use_ucf:\n",
    "                ucf_year = [2017, 2018]  # ucf data year used in train\n",
    "                if True:  # del_2016:\n",
    "                    print('delete all buildings site0 in 2016')\n",
    "                    bids = ucf_leak_df.building_id.unique()\n",
    "                    df = df[df.building_id.isin(bids) == False]\n",
    "                ucf_leak_df = ucf_leak_df[ucf_leak_df.timestamp.dt.year.isin(ucf_year)]\n",
    "                df = pd.concat([df, ucf_leak_df])\n",
    "                df.reset_index(inplace=True)\n",
    "            if self.some_rows is not None:\n",
    "                df, _ = Mort_PickSamples(self.some_rows, df, None)\n",
    "                print(f'====== Some Samples@{self.source} ... data={df.shape}')\n",
    "        #df['date'] = df['timestamp'].dt.date\n",
    "        df[\"hour\"] = np.uint8(df[\"timestamp\"].dt.hour)\n",
    "        df[\"weekend\"] = np.uint8(df[\"timestamp\"].dt.weekday)    #cys\n",
    "        df[\"month\"] = np.uint8(df[\"timestamp\"].dt.month)\n",
    "        df[\"dayofweek\"] = np.uint8(df[\"timestamp\"].dt.dayofweek)\n",
    "        if self.source == \"train\":\n",
    "            df['meter_reading_log1p'] = np.log1p(df['meter_reading'])\n",
    "            df_group = df.groupby('building_id')['meter_reading_log1p']\n",
    "            self.building_mean = df_group.mean().astype(np.float16)\n",
    "            self.building_median = df_group.median().astype(np.float16)\n",
    "            self.building_min = df_group.min().astype(np.float16)\n",
    "            self.building_max = df_group.max().astype(np.float16)\n",
    "            self.building_std = df_group.std().astype(np.float16)\n",
    "            print(f\"building_mean={self.building_mean.head()}\")\n",
    "        else:        #for the testing dataframe,just use group infomation from training dataframe\n",
    "            self.building_mean=train_datas.building_mean\n",
    "            print(f\"test_datas.building_mean={self.building_mean.shape}\")\n",
    "            self.building_median=train_datas.building_median\n",
    "            self.building_min=train_datas.building_min\n",
    "            self.building_max=train_datas.building_max\n",
    "            self.building_std=train_datas.building_std\n",
    "\n",
    "        df['building_mean'] = df['building_id'].map(self.building_mean)\n",
    "        df['building_median'] = df['building_id'].map(self.building_median)\n",
    "        df['building_min'] = df['building_id'].map(self.building_min)\n",
    "        df['building_max'] = df['building_id'].map(self.building_max)\n",
    "        df['building_std'] = df['building_id'].map(self.building_std)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load building_meta and whether datasets. Then use Whether class to generate many lag features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "primary_use_dict:  {'Education': 0, 'Lodging/residential': 1, 'Office': 2, 'Entertainment/public assembly': 3, 'Other': 4, 'Retail': 5, 'Parking': 6, 'Public services': 7, 'Warehouse/storage': 8, 'Food sales and service': 9, 'Religious worship': 10, 'Healthcare': 11, 'Utility': 12, 'Technology/science': 13, 'Manufacturing/industrial': 14, 'Services': 15}\n",
      "(1449, 6)\n",
      "   site_id  building_id  primary_use  square_feet  year_built  floor_count\n",
      "0        0            0            0         7432      2008.0          NaN\n",
      "1        0            1            0         2720      2004.0          NaN\n",
      "2        0            2            0         5376      1991.0          NaN\n",
      "3        0            3            0        23685      2002.0          NaN\n",
      "4        0            4            0       116607      1975.0          NaN\n",
      "(277243, 9)\n",
      "site_id                    0\n",
      "timestamp                  0\n",
      "air_temperature          104\n",
      "cloud_coverage        140448\n",
      "dew_temperature          327\n",
      "precip_depth_1_hr      95588\n",
      "sea_level_pressure     21265\n",
      "wind_direction         12370\n",
      "wind_speed               460\n",
      "dtype: int64\n",
      "TimeAlignment@test\tdf(277243, 9)......\n",
      "weather_test=(277243, 30) features=Index(['site_id', 'timestamp', 'air_temperature', 'cloud_coverage',\n",
      "       'dew_temperature', 'precip_depth_1_hr', 'sea_level_pressure',\n",
      "       'wind_direction', 'wind_speed', 'offset', 'air_temperature_mean_lag3',\n",
      "       'air_temperature_max_lag3', 'air_temperature_min_lag3',\n",
      "       'air_temperature_std_lag3', 'cloud_coverage_mean_lag3',\n",
      "       'dew_temperature_mean_lag3', 'precip_depth_1_hr_mean_lag3',\n",
      "       'sea_level_pressure_mean_lag3', 'wind_direction_mean_lag3',\n",
      "       'wind_speed_mean_lag3', 'air_temperature_mean_lag72',\n",
      "       'air_temperature_max_lag72', 'air_temperature_min_lag72',\n",
      "       'air_temperature_std_lag72', 'cloud_coverage_mean_lag72',\n",
      "       'dew_temperature_mean_lag72', 'precip_depth_1_hr_mean_lag72',\n",
      "       'sea_level_pressure_mean_lag72', 'wind_direction_mean_lag72',\n",
      "       'wind_speed_mean_lag72'],\n",
      "      dtype='object')\n",
      "weather_test_df=(277243, 30) \n",
      "(139773, 9)\n",
      "site_id                   0\n",
      "timestamp                 0\n",
      "air_temperature          55\n",
      "cloud_coverage        69173\n",
      "dew_temperature         113\n",
      "precip_depth_1_hr     50289\n",
      "sea_level_pressure    10618\n",
      "wind_direction         6268\n",
      "wind_speed              304\n",
      "dtype: int64\n",
      "TimeAlignment@train\tdf(139773, 9)......\n",
      "weather_train=(139773, 30) features=Index(['site_id', 'timestamp', 'air_temperature', 'cloud_coverage',\n",
      "       'dew_temperature', 'precip_depth_1_hr', 'sea_level_pressure',\n",
      "       'wind_direction', 'wind_speed', 'offset', 'air_temperature_mean_lag3',\n",
      "       'air_temperature_max_lag3', 'air_temperature_min_lag3',\n",
      "       'air_temperature_std_lag3', 'cloud_coverage_mean_lag3',\n",
      "       'dew_temperature_mean_lag3', 'precip_depth_1_hr_mean_lag3',\n",
      "       'sea_level_pressure_mean_lag3', 'wind_direction_mean_lag3',\n",
      "       'wind_speed_mean_lag3', 'air_temperature_mean_lag72',\n",
      "       'air_temperature_max_lag72', 'air_temperature_min_lag72',\n",
      "       'air_temperature_std_lag72', 'cloud_coverage_mean_lag72',\n",
      "       'dew_temperature_mean_lag72', 'precip_depth_1_hr_mean_lag72',\n",
      "       'sea_level_pressure_mean_lag72', 'wind_direction_mean_lag72',\n",
      "       'wind_speed_mean_lag72'],\n",
      "      dtype='object')\n",
      "weather_train_df=(139773, 30)\n"
     ]
    }
   ],
   "source": [
    "def LoadBuilding(data_root):\n",
    "    building_meta_df = pd.read_csv(f'{data_root}/building_metadata.csv')\n",
    "    primary_use_list = building_meta_df['primary_use'].unique()\n",
    "    primary_use_dict = {key: value for value, key in enumerate(primary_use_list)}\n",
    "    print('primary_use_dict: ', primary_use_dict)\n",
    "    building_meta_df['primary_use'] = building_meta_df['primary_use'].map(primary_use_dict)\n",
    "    print(f\"{building_meta_df.shape}\\n{building_meta_df.head()}\")\n",
    "    return building_meta_df\n",
    "\n",
    "building_meta_df=LoadBuilding(data_root)\n",
    "weather_test_df = Whether('test', data_root).df()\n",
    "print(f\"weather_test_df={weather_test_df.shape} \")    #weather_test_df.head()\n",
    "weather_train_df = Whether('train', data_root).df()\n",
    "print(f\"weather_train_df={weather_train_df.shape}\")   #weather_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core GBDT functons(LiteMORT/lightGBM) and its parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install wurlitzer\n",
    "early_stop = 20\n",
    "verbose_eval = 5\n",
    "metric = 'l2'\n",
    "#num_rounds=1000, lr=0.05, bf=0.3\n",
    "num_rounds = 1000;      lr = 0.05;          bf = 0.3\n",
    "params = {'num_leaves': 31, 'n_estimators': num_rounds,\n",
    "              'objective': 'regression',\n",
    "              'max_bin': 256,\n",
    "              #               'max_depth': -1,\n",
    "              'learning_rate': lr,\n",
    "              \"boosting\": \"gbdt\",\n",
    "              \"bagging_freq\": 5,\n",
    "              \"bagging_fraction\": bf,\n",
    "              \"feature_fraction\": 0.9,  # STRANGE GBDT  why(\"bagging_freq\": 5 \"feature_fraction\": 0.9)!!!\n",
    "              \"metric\": metric, \"verbose_eval\": verbose_eval, 'n_jobs': 8, \"elitism\": 0,\"debug\":'1',\n",
    "              \"early_stopping_rounds\": early_stop, \"adaptive\": 'weight1', 'verbose': 0, 'min_data_in_leaf': 200,\n",
    "              #               \"verbosity\": -1,\n",
    "              #               'reg_alpha': 0.1,\n",
    "              #               'reg_lambda': 0.3\n",
    "              }\n",
    "#model = LiteMORT(params)\n",
    "\n",
    "def fit_regressor(train, val,target_meter,fold, some_params, devices=(-1,), merge_info=None, cat_features=None):\n",
    "    t0=time.time()\n",
    "    X_train, y_train = train\n",
    "    X_valid, y_valid = val\n",
    "\n",
    "    device = devices[0]\n",
    "    if device == -1:        # use cpu\n",
    "        pass\n",
    "    else:        # use gpu\n",
    "        print(f'using gpu device_id {device}...')\n",
    "        params.update({'device': 'gpu', 'gpu_device_id': device})\n",
    "\n",
    "\n",
    "    if False:\n",
    "        col_y = pd.DataFrame(y_train)\n",
    "        col_X = X_train.reset_index(drop=True)\n",
    "        d_train = pd.concat([col_y, col_X], ignore_index=True, axis=1)\n",
    "        np.savetxt(\"E:/2/LightGBM-master/examples/regression/case_cys_.csv\", d_train, delimiter='\\t')\n",
    "        print(\"X_train={}, y_train={} d_train={}\".format(col_X.shape, col_y.shape, d_train.shape))\n",
    "\n",
    "    if isMORT:        \n",
    "        some_params['verbose']=666 if fold==0 else 0\n",
    "        merge_datas=[]\n",
    "        model = LiteMORT(some_params,merge_infos=merge_info)   # all train,eval,predict would use same merge infomation\n",
    "        model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], categorical_feature=cat_features)\n",
    "        fold_importance = None\n",
    "        log = \"\"\n",
    "    else:\n",
    "        some_params['verbose'] = 0\n",
    "        d_train = lgb.Dataset(X_train, label=y_train, categorical_feature=cat_features)\n",
    "        d_valid = lgb.Dataset(X_valid, label=y_valid, categorical_feature=cat_features)\n",
    "        watchlist = [d_train, d_valid]\n",
    "        print('training LGB: parmas=',params)\n",
    "        model = lgb.train(some_params,\n",
    "                          train_set=d_train,\n",
    "                          num_boost_round=num_rounds,\n",
    "                          valid_sets=watchlist,\n",
    "                          verbose_eval=verbose_eval,\n",
    "                          early_stopping_rounds=early_stop)\n",
    "        print('best_score', model.best_score)\n",
    "        log = {'train/mae': model.best_score['training'][metric], 'valid/mae': model.best_score['valid_1'][metric]}\n",
    "    # predictions\n",
    "    y_pred_valid = model.predict(X_valid, num_iteration=model.best_iteration)\n",
    "    oof_loss = mean_squared_error(y_valid, y_pred_valid)  # target is already in log scale\n",
    "    print(f'METER:{target_meter} Fold:{fold} MSE: {oof_loss:.4f} time={time.time() - t0:.5g}', flush=True)\n",
    "    #input(\"......\");   os._exit(-200)      #\n",
    "    return model, y_pred_valid, log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate train dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASHRAE_data_train......category=['building_id', 'site_id', 'primary_use']\n",
      "feature_cols=['square_feet', 'year_built', 'hour', 'weekend', 'building_median', 'air_temperature', 'cloud_coverage', 'dew_temperature', 'precip_depth_1_hr', 'sea_level_pressure', 'wind_direction', 'wind_speed', 'air_temperature_mean_lag3', 'air_temperature_max_lag3', 'air_temperature_min_lag3', 'air_temperature_std_lag3', 'cloud_coverage_mean_lag3', 'dew_temperature_mean_lag3', 'precip_depth_1_hr_mean_lag3', 'sea_level_pressure_mean_lag3', 'wind_direction_mean_lag3', 'wind_speed_mean_lag3', 'air_temperature_mean_lag72', 'air_temperature_max_lag72', 'air_temperature_min_lag72', 'air_temperature_std_lag72', 'cloud_coverage_mean_lag72', 'dew_temperature_mean_lag72', 'precip_depth_1_hr_mean_lag72', 'sea_level_pressure_mean_lag72', 'wind_direction_mean_lag72', 'wind_speed_mean_lag72']......\n",
      "2260080\n",
      "delete all buildings site0 in 2016\n",
      "building_mean=building_id\n",
      "0    5.417969\n",
      "1    4.683594\n",
      "2    3.072266\n",
      "3    5.386719\n",
      "4    7.167969\n",
      "Name: meter_reading_log1p, dtype: float16\n",
      "ASHRAE_data_ df_base=(21399518, 15)\n",
      "(1449,)\n",
      "building_id\n",
      "0    5.417969\n",
      "1    4.683594\n",
      "2    3.072266\n",
      "3    5.386719\n",
      "4    7.167969\n",
      "Name: meter_reading_log1p, dtype: float16\n"
     ]
    }
   ],
   "source": [
    "train_datas = ASHRAE_data(\"train\",data_root,building_meta_df,weather_train_df)\n",
    "print(train_datas.building_mean.shape)\n",
    "print(train_datas.building_mean.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5-folds training on 4 target meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X_y@0 df_base=(21399518, 15)......\n",
      "target@0=(12992101, 15)\n",
      "data_X__@0=(12992101, 6)\toriginal=(12992101, 16)\tmerge=True\n",
      "target_meter=0 X_train=(12992101, 6)\n",
      "features=Index(['building_median', 'building_id', 'timestamp', 'site_id', 'weekend',\n",
      "       'hour'],\n",
      "      dtype='object')\n",
      "cat_features ['building_id', 'site_id', 'primary_use']\n",
      "fold=0 train=(11368088, 6),valid=(1624013, 6)\n",
      "======Load LiteMORT library @/opt/conda/lib/python3.6/site-packages/litemort/libLiteMORT.so\n",
      "Found `bagging_fraction`(alias of subsample) in params. Will use it instead of argument\n",
      "Found `feature_fraction`(alias of sub_feature) in params. Will use it instead of argument\n",
      "Found `n_jobs`(alias of n_threads) in params. Will use it instead of argument\n",
      "Found `early_stopping_rounds`(alias of early_stop) in params. Will use it instead of argument\n",
      "Found `n_estimators`(alias of num_trees) in params. Will use it instead of argument\n",
      "********************************************************************\n",
      "*                          LiteMORT-beta                           *\n",
      "*                   for personal, non-commercial use.              *\n",
      "*    Copyright (c) 2018-2019 by YingShiChen. All Rights Reserved.  *\n",
      "*                         gsp@grusoft.com                          *\n",
      "********************************************************************\n",
      "\n",
      "======LiteMORT_api init......********* OnUserParams ********* \n",
      "\n",
      "======LiteMORT_api init @0x561be9beb710(hEDA=0x561be9d03b20,hGBRT=(nil))...OK\n",
      "\n",
      "********* Fold_[weather] nSamp=139774 nFeat=27(const=0) QUANT=0 Total Bins=0\n",
      "\tsparse=0.0891942 NAN=0.0415557 nLocalConst=0 time=251.801 sec\r\n",
      "\n",
      "********* Fold_[building] nSamp=1450 nFeat=3(const=0) QUANT=0 Total Bins=0\n",
      "\tsparse=0.126207 NAN=0.178621 nLocalConst=0 time=0.302 sec\r\n",
      "====== LiteMORT_fit X_train_0=(11368088, 6) y_train=(11368088,)......\n",
      "AddPivotOnMerge NAN@['site_id', 'timestamp']\tnNA=1877600/16.5%%\n",
      "AddPivotOnMerge NAN@['site_id', 'timestamp']\tnNA=12391/0.763%%\n",
      "\n",
      "********* LiteMORT_fit nSamp=11368088,nFeat_0=5 nFeat@Merge=[30@2] hEDA=0x561be9d03b20********* \n",
      "\n",
      "********* FeatVec_LOSS::EDA@\"train\"\tsamp_weight=(nil)...\n",
      "  -1               [0-11.286902]\tBIG=0\tnBin=0[0,0,0,0,0]\tsparse=0.0168,nana=0 \n",
      "\t----1\t FeatVec_Q<uint16_t>@\"*building_id\" nBins=1414\n",
      "********* Fold_[train] nSamp=11368088 nFeat=35(const=0) QUANT=5 Total Bins=8132\n",
      "\tsparse=0.0851205 NAN=0.15716 nLocalConst=0 time=32752.4 sec\r\n",
      "********* FeatVec_LOSS::EDA@\"eval\"\tsamp_weight=(nil)...\n",
      "  -1               [0-10.289838]\tBIG=0\tnBin=0[0,0,0,0,0]\tsparse=0.0179,nana=0 \n",
      "\t----1\t FeatVec_Q<uint16_t>@\"*building_id\" nBins=1414\n",
      "********* Fold_[eval] nSamp=1624013 nFeat=35(const=0) QUANT=2 Total Bins=8132\n",
      "\tsparse=0.0976004 NAN=0.0416083 nLocalConst=0 time=2290.97 sec\r\n",
      "\n",
      "********* HistoGRAM_BUFFER MEM=16.3769(M) nMostBin=504184\n",
      "********* HistoGRAM_BUFFER nMostFeat=35,nMostNode=62 zero=0\n",
      "\n",
      "\n",
      "********* GBRT[REGRESSION]\n",
      "\tnTrainSamp=11368088,nTree=1000,thread=8...\n",
      "\tlr=0.05 sample=[0.3,0.9] min@leaf=200 stop=20 drop=1 num_leaves=31 feat_quanti=256\n",
      "\tOBJECTIVE=\"regression\"\teval_metric=\"mse\"\tleaf_optimal=\"lambda_0\"\n",
      "\t init=mean maxDepth=-3\n",
      "\tL2=0\tLf=1\tImputation=OFF\tNormal=OFF\n",
      "\tnElitism=0,Iter_refine=0 \tRefine_split=0\n",
      "\tnMostPrune=0 node_task=split_X debug=Debug_1\n",
      "\tnMostSalp4Bins=0 histo_bin_::map=\"frequency\"\n",
      "********* GBRT *********\n",
      "----MERGE_sets=2\n",
      "----Start training from score 4.2176\n",
      "eval_0=18.858   \n",
      "\t0: ERR@train=  2.6232 nNode=0 nPick=[0,0] time=0.079804======\n",
      "eval_5=1.6657   eval_10=1.1721   eval_15=0.87665  eval_20=0.69747  eval_25=0.59091  eval_30=0.52517  eval_35=0.48466  eval_40=0.46119  eval_45=0.44743  eval_50=0.43811  eval_55=0.43038  eval_60=0.42674  eval_65=0.42361  eval_70=0.42037  eval_75=0.41846  eval_80=0.4159   eval_85=0.41311  eval_90=0.41093  \n",
      "-------- Oscillate@(94,0.410731) best=(93,0.410647) -------- \n",
      "eval_95=0.41014  eval_100=0.40863  eval_105=0.40709  tX=0 eval_110=0.40642  tX=0 eval_115=0.40528  tX=0 eval_120=0.40515  tX=0 eval_125=0.4041   tX=0 eval_130=0.40504  tX=0 eval_135=0.40524  tX=0 eval_140=0.40492  tX=0 eval_145=0.40419  tX=0 \n",
      "====== LOOP=145: ERR=[~0.29885 ,0.4041  ] time=247(0) ======\n",
      "\n",
      "********* early_stopping@[125,125]!!!\n",
      "********* GBRT::Train ERR@train=0.29885  E_best@eval=0.4041   nTree=125 nFeat={31-31} aNode=61 maxDepth=14 thread=8\n",
      "********* train=247.335(hTree->Train=138.011,tCheckGain=95.6027,tHisto=46.7404(0,210.66),tX=0) sec\r\n",
      "\n",
      "********* LiteMORT_fit_1  time=282(0)......OK\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MEMORY@[fit_0-fit_1]: physical=682.54(M) virtual=508.45(M) begin=4779.05(M)\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "AddPivotOnMerge NAN@['site_id', 'timestamp']\tnNA=12391/0.763%%\n",
      "********* FeatVec_LOSS::EDA@\"predict_raw\"\tsamp_weight=(nil)...\n",
      "\t----1\t FeatVec_Q<uint16_t>@\"*building_id\" nBins=1414\n",
      "********* Fold_[predict_raw] nSamp=1624013 nFeat=35(const=0) QUANT=2 Total Bins=8132\n",
      "\tsparse=0.0976004 NAN=0.0416083 nLocalConst=0 time=2176.82 sec\r\n",
      "\n",
      "********* LiteMORT_predict nSamp=1624013,nFeat=35 hEDA=0x561be9d03b20********* \n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MEMORY@[PRED_0-PRED_1]: physical=0.00(M) virtual=20.14(M) begin=5461.59(M)\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "METER:0 Fold:0 MSE: 0.4041 time=308.79\n",
      "fold=1 train=(11368088, 6),valid=(1624013, 6)\n",
      "======Load LiteMORT library @/opt/conda/lib/python3.6/site-packages/litemort/libLiteMORT.so\n",
      "\n",
      "======LiteMORT_api init......======LiteMORT_api init @0x561bedd918f0(hEDA=0x561bee36af20,hGBRT=(nil))...OK\n",
      "\n",
      "\t------MERGE@[\"weather\"](139774x27)......\n",
      "\t------MERGE@[\"building\"](1450x3)......====== LiteMORT_fit X_train_0=(11368088, 6) y_train=(11368088,)......\n",
      "AddPivotOnMerge NAN@['site_id', 'timestamp']\tnNA=1886165/16.6%%\n",
      "AddPivotOnMerge NAN@['site_id', 'timestamp']\tnNA=3826/0.236%%\n",
      "\n",
      "********* LiteMORT_fit nSamp=11368088,nFeat_0=5 nFeat@Merge=[30@2] hEDA=0x561bee36af20********* \n",
      "\n",
      "  -1               [0-11.286902]\tBIG=0\tnBin=0[0,0,0,0,0]\tsparse=0.0173,nana=0 \n",
      "  -1               [0-10.699586]\tBIG=0\tnBin=0[0,0,0,0,0]\tsparse=0.0147,nana=0 \n",
      "\n",
      "\n",
      "********* GBRT[REGRESSION]\n",
      "\tnTrainSamp=11368088,nTree=1000,thread=8...\n",
      "********* GBRT *********\n",
      "----MERGE_sets=2\n",
      "\n",
      "eval_0=18.569   eval_5=1.6688   eval_10=1.1546   eval_15=0.84333  eval_20=0.65229  eval_25=0.53636  eval_30=0.46294  eval_35=0.41707  eval_40=0.38876  eval_45=0.37106  eval_50=0.35778  eval_55=0.34885  eval_60=0.3418   eval_65=0.33622  eval_70=0.33167  eval_75=0.32773  eval_80=0.3248   eval_85=0.32182  eval_90=0.32116  \n",
      "-------- Oscillate@(91,0.321158) best=(90,0.32067) -------- \n",
      "eval_95=0.31967  eval_100=0.3178   eval_105=0.31659  tX=0 eval_110=0.3145   tX=0 eval_115=0.31307  tX=0 eval_120=0.31301  tX=0 eval_125=0.31251  tX=0 eval_130=0.3113   tX=0 eval_135=0.30991  tX=0 eval_140=0.30953  tX=0 eval_145=0.30928  tX=0 eval_150=0.30899  tX=0 eval_155=0.30837  tX=0 eval_160=0.30881  tX=0 eval_165=0.30935  tX=0 eval_170=0.30875  tX=0 \n",
      "====== LOOP=173: ERR=[~0.28885 ,0.30824 ] time=284(0) ======\n",
      "\n",
      "********* early_stopping@[153,153]!!!\n",
      "********* LiteMORT_fit_1  time=320(0)......OK\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MEMORY@[fit_0-fit_1]: physical=614.24(M) virtual=466.62(M) begin=5591.07(M)\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "AddPivotOnMerge NAN@['site_id', 'timestamp']\tnNA=3826/0.236%%\n",
      "METER:0 Fold:1 MSE: 0.3082 time=350.09\n",
      "fold=2 train=(11368088, 6),valid=(1624013, 6)\n",
      "======Load LiteMORT library @/opt/conda/lib/python3.6/site-packages/litemort/libLiteMORT.so\n",
      "\n",
      "======LiteMORT_api init......======LiteMORT_api init @0x561c21f94860(hEDA=0x561c22157730,hGBRT=(nil))...OK\n",
      "\n",
      "\t------MERGE@[\"weather\"](139774x27)......====== LiteMORT_fit X_train_0=(11368088, 6) y_train=(11368088,)......\n",
      "\t------MERGE@[\"building\"](1450x3)......\n",
      "AddPivotOnMerge NAN@['site_id', 'timestamp']\tnNA=1886688/16.6%%\n",
      "AddPivotOnMerge NAN@['site_id', 'timestamp']\tnNA=3303/0.203%%\n",
      "\n",
      "********* LiteMORT_fit nSamp=11368088,nFeat_0=5 nFeat@Merge=[30@2] hEDA=0x561c22157730********* \n",
      "\n",
      "  -1               [0-11.286902]\tBIG=0\tnBin=0[0,0,0,0,0]\tsparse=0.0176,nana=0 \n",
      "  -1               [0-9.1487837]\tBIG=0\tnBin=0[0,0,0,0,0]\tsparse=0.0129,nana=0 \n",
      "\n",
      "\n",
      "********* GBRT[REGRESSION]\n",
      "\tnTrainSamp=11368088,nTree=1000,thread=8...\n",
      "********* GBRT *********\n",
      "----MERGE_sets=2\n",
      "\n",
      "eval_0=18.589   eval_5=1.6231   eval_10=1.1089   eval_15=0.79754  eval_20=0.60403  eval_25=0.48792  eval_30=0.41309  eval_35=0.36632  eval_40=0.33711  eval_45=0.31895  eval_50=0.30636  eval_55=0.29779  eval_60=0.29114  eval_65=0.28597  eval_70=0.28231  \n",
      "-------- Oscillate@(72,0.28251) best=(71,0.282311) -------- \n",
      "eval_75=0.2816   eval_80=0.28002  eval_85=0.27837  eval_90=0.27765  eval_95=0.27745  eval_100=0.27621  eval_105=0.27618  tX=0 eval_110=0.27642  tX=0 eval_115=0.27721  tX=0 eval_120=0.28043  tX=0 eval_125=0.27988  tX=0 \n",
      "====== LOOP=128: ERR=[~0.31931 ,0.27595 ] time=225(0) ======\n",
      "\n",
      "********* early_stopping@[108,108]!!!\n",
      "********* LiteMORT_fit_1  time=260(0)......OK\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MEMORY@[fit_0-fit_1]: physical=539.43(M) virtual=393.15(M) begin=6461.36(M)\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "AddPivotOnMerge NAN@['site_id', 'timestamp']\tnNA=3303/0.203%%\n",
      "METER:0 Fold:2 MSE: 0.2759 time=284.24\n",
      "fold=3 train=(11368088, 6),valid=(1624013, 6)\n",
      "======Load LiteMORT library @/opt/conda/lib/python3.6/site-packages/litemort/libLiteMORT.so\n",
      "\n",
      "======LiteMORT_api init......======LiteMORT_api init @0x561c0405e390(hEDA=0x561bfe8eabb0,hGBRT=(nil))...OK\n",
      "\n",
      "\t------MERGE@[\"weather\"](139774x27)......\n",
      "\t------MERGE@[\"building\"](1450x3)......====== LiteMORT_fit X_train_0=(11368088, 6) y_train=(11368088,)......\n",
      "AddPivotOnMerge NAN@['site_id', 'timestamp']\tnNA=1879957/16.5%%\n",
      "AddPivotOnMerge NAN@['site_id', 'timestamp']\tnNA=10034/0.618%%\n",
      "\n",
      "********* LiteMORT_fit nSamp=11368088,nFeat_0=5 nFeat@Merge=[30@2] hEDA=0x561bfe8eabb0********* \n",
      "\n",
      "  -1               [0-11.286902]\tBIG=0\tnBin=0[0,0,0,0,0]\tsparse=0.016,nana=0 \n",
      "  -1               [0-9.8787851]\tBIG=0\tnBin=0[0,0,0,0,0]\tsparse=0.024,nana=0 \n",
      "\n",
      "\n",
      "********* GBRT[REGRESSION]\n",
      "\tnTrainSamp=11368088,nTree=1000,thread=8...\n",
      "********* GBRT *********\n",
      "----MERGE_sets=2\n",
      "\n",
      "eval_0=18.916   eval_5=1.7984   eval_10=1.2972   eval_15=0.99591  eval_20=0.81083  eval_25=0.69969  eval_30=0.62888  eval_35=0.58503  eval_40=0.55884  eval_45=0.54012  eval_50=0.52608  eval_55=0.51534  eval_60=0.50658  eval_65=0.49988  eval_70=0.49506  eval_75=0.49002  eval_80=0.48568  eval_85=0.48049  eval_90=0.47769  \n",
      "-------- Oscillate@(95,0.475613) best=(94,0.475361) -------- \n",
      "eval_95=0.47492  eval_100=0.47213  eval_105=0.46877  tX=0 eval_110=0.46717  tX=0 eval_115=0.46481  tX=0 eval_120=0.46371  tX=0 eval_125=0.46285  tX=0 eval_130=0.46248  tX=0 eval_135=0.46075  tX=0 eval_140=0.45991  tX=0 eval_145=0.45944  tX=0 eval_150=0.45904  tX=0 eval_155=0.45923  tX=0 eval_160=0.45921  tX=0 eval_165=0.45909  tX=0 eval_170=0.45932  tX=0 \n",
      "====== LOOP=171: ERR=[~0.27773 ,0.45865 ] time=285(0) ======\n",
      "\n",
      "********* early_stopping@[151,151]!!!\n",
      "********* LiteMORT_fit_1  time=320(0)......OK\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MEMORY@[fit_0-fit_1]: physical=575.39(M) virtual=422.57(M) begin=7165.90(M)\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "AddPivotOnMerge NAN@['site_id', 'timestamp']\tnNA=10034/0.618%%\n",
      "METER:0 Fold:3 MSE: 0.4586 time=347.16\n",
      "fold=4 train=(11368088, 6),valid=(1624013, 6)\n",
      "======Load LiteMORT library @/opt/conda/lib/python3.6/site-packages/litemort/libLiteMORT.so\n",
      "\n",
      "======LiteMORT_api init......======LiteMORT_api init @0x561c2a9cb380(hEDA=0x561c21725d20,hGBRT=(nil))...OK\n",
      "\n",
      "\t------MERGE@[\"weather\"](139774x27)......\n",
      "\t------MERGE@[\"building\"](1450x3)......====== LiteMORT_fit X_train_0=(11368088, 6) y_train=(11368088,)......\n",
      "AddPivotOnMerge NAN@['site_id', 'timestamp']\tnNA=1881957/16.6%%\n",
      "AddPivotOnMerge NAN@['site_id', 'timestamp']\tnNA=8034/0.495%%\n",
      "\n",
      "********* LiteMORT_fit nSamp=11368088,nFeat_0=5 nFeat@Merge=[30@2] hEDA=0x561c21725d20********* \n",
      "\n",
      "  -1               [0-11.286902]\tBIG=0\tnBin=0[0,0,0,0,0]\tsparse=0.0176,nana=0 \n",
      "  -1               [0-9.2395105]\tBIG=0\tnBin=0[0,0,0,0,0]\tsparse=0.0125,nana=0 \n",
      "\n",
      "\n",
      "********* GBRT[REGRESSION]\n",
      "\tnTrainSamp=11368088,nTree=1000,thread=8...\n",
      "********* GBRT *********\n",
      "----MERGE_sets=2\n",
      "\n",
      "eval_0=19.478   eval_5=1.6326   eval_10=1.1316   eval_15=0.82799  eval_20=0.64267  eval_25=0.53041  eval_30=0.46089  eval_35=0.41802  eval_40=0.39021  eval_45=0.37252  eval_50=0.35926  eval_55=0.35012  eval_60=0.34223  eval_65=0.33609  eval_70=0.32954  \n",
      "-------- Oscillate@(74,0.327669) best=(73,0.327562) -------- \n",
      "eval_75=0.32545  eval_80=0.32264  eval_85=0.3189   eval_90=0.31676  eval_95=0.31376  eval_100=0.31135  eval_105=0.30976  tX=0 eval_110=0.30763  tX=0 eval_115=0.3065   tX=0 eval_120=0.30606  tX=0 eval_125=0.30683  tX=0 eval_130=0.30552  tX=0 eval_135=0.30434  tX=0 eval_140=0.30309  tX=0 eval_145=0.30275  tX=0 eval_150=0.30328  tX=0 eval_155=0.30336  tX=0 eval_160=0.30459  tX=0 eval_165=0.30509  tX=0 \n",
      "====== LOOP=167: ERR=[~0.29907 ,0.3022  ] time=263(0) ======\n",
      "\n",
      "********* early_stopping@[147,147]!!!\n",
      "********* LiteMORT_fit_1  time=299(0)......OK\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MEMORY@[fit_0-fit_1]: physical=604.78(M) virtual=465.04(M) begin=7893.38(M)\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "AddPivotOnMerge NAN@['site_id', 'timestamp']\tnNA=8034/0.495%%\n",
      "METER:0 Fold:4 MSE: 0.3022 time=324.85\n",
      "fold=5 train=(11368089, 6),valid=(1624012, 6)\n",
      "======Load LiteMORT library @/opt/conda/lib/python3.6/site-packages/litemort/libLiteMORT.so\n",
      "\n",
      "======LiteMORT_api init......======LiteMORT_api init @0x561bfe78d1e0(hEDA=0x561bfe779560,hGBRT=(nil))...OK\n",
      "\n",
      "\t------MERGE@[\"weather\"](139774x27)......\n",
      "\t------MERGE@[\"building\"](1450x3)......====== LiteMORT_fit X_train_0=(11368089, 6) y_train=(11368089,)......\n",
      "AddPivotOnMerge NAN@['site_id', 'timestamp']\tnNA=1887185/16.6%%\n",
      "AddPivotOnMerge NAN@['site_id', 'timestamp']\tnNA=2806/0.173%%\n",
      "\n",
      "********* LiteMORT_fit nSamp=11368089,nFeat_0=5 nFeat@Merge=[30@2] hEDA=0x561bfe779560********* \n",
      "\n",
      "  -1               [0-10.699586]\tBIG=0\tnBin=0[0,0,0,0,0]\tsparse=0.0161,nana=0 \n",
      "  -1               [0-11.286902]\tBIG=0\tnBin=0[0,0,0,0,0]\tsparse=0.0231,nana=0 \n",
      "\n",
      "\n",
      "********* GBRT[REGRESSION]\n",
      "\tnTrainSamp=11368089,nTree=1000,thread=8...\n",
      "********* GBRT *********\n",
      "----MERGE_sets=2\n",
      "\n",
      "eval_0=18.456   eval_5=1.8448   eval_10=1.3571   eval_15=1.066    eval_20=0.89029  eval_25=0.78729  eval_30=0.7232   eval_35=0.68453  eval_40=0.66154  eval_45=0.64726  eval_50=0.63982  eval_55=0.63289  eval_60=0.62824  eval_65=0.62491  \n",
      "-------- Oscillate@(70,0.623749) best=(69,0.623524) -------- \n",
      "eval_70=0.62318  eval_75=0.62041  eval_80=0.61829  eval_85=0.61664  eval_90=0.61533  eval_95=0.61431  eval_100=0.61316  eval_105=0.6118   tX=0 eval_110=0.61216  tX=0 eval_115=0.61197  tX=0 eval_120=0.61227  tX=0 eval_125=0.61165  tX=0 eval_130=0.61059  tX=0 eval_135=0.61096  tX=0 eval_140=0.6104   tX=0 eval_145=0.61183  tX=0 eval_150=0.61126  tX=0 eval_155=0.61164  tX=0 eval_160=0.61089  tX=0 \n",
      "====== LOOP=160: ERR=[~0.28055 ,0.6104  ] time=273(0) ======\n",
      "\n",
      "********* early_stopping@[140,140]!!!\n",
      "********* LiteMORT_fit_1  time=307(0)......OK\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MEMORY@[fit_0-fit_1]: physical=602.46(M) virtual=480.87(M) begin=8637.19(M)\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "AddPivotOnMerge NAN@['site_id', 'timestamp']\tnNA=2806/0.173%%\n",
      "METER:0 Fold:5 MSE: 0.6104 time=332.75\n",
      "fold=6 train=(11368089, 6),valid=(1624012, 6)\n",
      "======Load LiteMORT library @/opt/conda/lib/python3.6/site-packages/litemort/libLiteMORT.so\n",
      "\n",
      "======LiteMORT_api init......======LiteMORT_api init @0x561c26fcceb0(hEDA=0x561c52100900,hGBRT=(nil))...OK\n",
      "\n",
      "\t------MERGE@[\"weather\"](139774x27)......\n",
      "\t------MERGE@[\"building\"](1450x3)......====== LiteMORT_fit X_train_0=(11368089, 6) y_train=(11368089,)......\n",
      "AddPivotOnMerge NAN@['site_id', 'timestamp']\tnNA=1664406/14.6%%\n",
      "AddPivotOnMerge NAN@['site_id', 'timestamp']\tnNA=225585/13.9%%\n",
      "\n",
      "********* LiteMORT_fit nSamp=11368089,nFeat_0=5 nFeat@Merge=[30@2] hEDA=0x561c52100900********* \n",
      "\n",
      "  -1               [0-11.286902]\tBIG=0\tnBin=0[0,0,0,0,0]\tsparse=0.0182,nana=0 \n",
      "  -1               [0-9.7981272]\tBIG=0\tnBin=0[0,0,0,0,0]\tsparse=0.00858,nana=0 \n",
      "\n",
      "\n",
      "********* GBRT[REGRESSION]\n",
      "\tnTrainSamp=11368089,nTree=1000,thread=8...\n",
      "********* GBRT *********\n",
      "----MERGE_sets=2\n",
      "\n",
      "eval_0=19.937   eval_5=1.6182   eval_10=1.1249   eval_15=0.83357  eval_20=0.66183  eval_25=0.56179  eval_30=0.50328  eval_35=0.47053  eval_40=0.45203  eval_45=0.44308  \n",
      "-------- Oscillate@(50,0.438753) best=(49,0.438745) -------- \n",
      "eval_50=0.43803  eval_55=0.43427  eval_60=0.43435  eval_65=0.43443  eval_70=0.43444  eval_75=0.43389  eval_80=0.43241  eval_85=0.43311  eval_90=0.43453  eval_95=0.43485  eval_100=0.4353   \n",
      "====== LOOP=104: ERR=[~0.31958 ,0.43203 ] time=186(0) ======\n",
      "\n",
      "********* early_stopping@[84,84]!!!\n",
      "********* LiteMORT_fit_1  time=222(0)......OK\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "MEMORY@[fit_0-fit_1]: physical=484.66(M) virtual=363.23(M) begin=9495.71(M)\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "AddPivotOnMerge NAN@['site_id', 'timestamp']\tnNA=225585/13.9%%\n",
      "METER:0 Fold:6 MSE: 0.4320 time=244.09\n",
      "fold=7 train=(11368089, 6),valid=(1624012, 6)\n",
      "======Load LiteMORT library @/opt/conda/lib/python3.6/site-packages/litemort/libLiteMORT.so\n",
      "\n",
      "======LiteMORT_api init......======LiteMORT_api init @0x561c3e1d1b80(hEDA=0x561c3e1d2430,hGBRT=(nil))...OK\n",
      "\n",
      "\t------MERGE@[\"weather\"](139774x27)......\n",
      "\t------MERGE@[\"building\"](1450x3)......====== LiteMORT_fit X_train_0=(11368089, 6) y_train=(11368089,)......\n",
      "AddPivotOnMerge NAN@['site_id', 'timestamp']\tnNA=265979/2.34%%\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "AddPivotOnMerge Skip All-NAN@['site_id', 'timestamp']\tnNA=1624012/100%%",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-7f2ee625bed1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'fold={fold} train={train_data[0].shape},valid={valid_data[0].shape}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m#     model, y_pred_valid, log = fit_cb(train_data, valid_data, cat_features=cat_features, devices=[0,])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_regressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_meter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msome_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmerge_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_datas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_infos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcat_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0my_valid_pred_total\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred_valid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mmodels_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-8e38bb6598b4>\u001b[0m in \u001b[0;36mfit_regressor\u001b[0;34m(train, val, target_meter, fold, some_params, devices, merge_info, cat_features)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mmerge_datas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLiteMORT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msome_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmerge_infos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmerge_info\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# all train,eval,predict would use same merge infomation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcat_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mfold_importance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mlog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/litemort/LiteMORT.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train_0, y_train, eval_set, categorical_feature, discrete_feature, params, flag)\u001b[0m\n\u001b[1;32m    487\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misUpdate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m                     \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_eval_update\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m                 \u001b[0meval_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMort_Preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"eval\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeat_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeat_info\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmerge_infos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_infos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_sets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpp_dat_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m                 \u001b[0;31m#self.eval_sets.append(eval_set.cpp_dat_)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/litemort/LiteMORT_preprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name_, X, y, params, features, feat_info, merge_infos, **kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmerge_infos\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAddPivotOnMerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmerge_infos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m         \u001b[0;32mreturn\u001b[0m    \u001b[0;31m#please implement this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/litemort/LiteMORT_preprocess.py\u001b[0m in \u001b[0;36mAddPivotOnMerge\u001b[0;34m(self, df_base, merge_infos)\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;31m#print(f\"left={df_left.shape} rigt={df_rigt.shape} nNA={nNA}\" )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnNA\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnSample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m     \u001b[0;31m#必须退出\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"AddPivotOnMerge Skip All-NAN@{cols_on}\\tnNA={nNA}/{nNA * 100.0 / self.nSample:.3g}%%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnNA\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m      \u001b[0;31m#真麻烦！！！\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: AddPivotOnMerge Skip All-NAN@['site_id', 'timestamp']\tnNA=1624012/100%%"
     ]
    }
   ],
   "source": [
    "folds = 8\n",
    "seed = 666\n",
    "shuffle = False\n",
    "kf = KFold(n_splits=folds, shuffle=shuffle, random_state=seed)\n",
    "#kf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "cat_features=None\n",
    "meter_models=[]\n",
    "\n",
    "losses=[]\n",
    "\n",
    "for target_meter in range(nTargetMeter):\n",
    "    X_train, y_train = train_datas.data_X_y(target_meter)\n",
    "    #X_train = X_train[feat_fix]\n",
    "    y_valid_pred_total = np.zeros(X_train.shape[0])\n",
    "    gc.collect()\n",
    "    print(f'target_meter={target_meter} X_train={X_train.shape}\\nfeatures={X_train.columns}')\n",
    "    cat_features = train_datas.category_cols\n",
    "    # cat_features = ['building_id']\n",
    "    # [X_train.columns.get_loc(cat_col) for cat_col in train_datas.category_cols]\n",
    "    print('cat_features', cat_features)\n",
    "    if False :\n",
    "        feat_select = X_train.columns\n",
    "        feat_select = list(set(feat_select) - set(feat_fix))\n",
    "        params['early_stopping_rounds'] = 50        #不宜太大，掉到坑里\n",
    "        params['category_features'] = cat_features\n",
    "        MORT_feat_select_(X_train, y_train, feat_fix, feat_select,params,nMostSelect=(int)(len(feat_select)/2))\n",
    "        input(\"......MORT_feat_search......\")\n",
    "        sys.exit(-100)\n",
    "\n",
    "    t0=time.time()\n",
    "    fold = 0\n",
    "    models_ = []\n",
    "    for train_idx, valid_idx in kf.split(X_train, y_train):\n",
    "    #for (train_idx, valid_idx) in kf.split(X_train, X_train['building_id']):\n",
    "        train_data = X_train.iloc[train_idx, :], y_train[train_idx]\n",
    "        valid_data = X_train.iloc[valid_idx, :], y_train[valid_idx]\n",
    "        params['seed'] = seed\n",
    "        print(f'fold={fold} train={train_data[0].shape},valid={valid_data[0].shape}')\n",
    "        #     model, y_pred_valid, log = fit_cb(train_data, valid_data, cat_features=cat_features, devices=[0,])\n",
    "        model, y_pred_valid, log = fit_regressor(train_data, valid_data,target_meter,fold,some_params=params,merge_info=train_datas.merge_infos, cat_features=cat_features)\n",
    "        y_valid_pred_total[valid_idx] = y_pred_valid\n",
    "        models_.append(model)\n",
    "        del train_data,valid_data\n",
    "        gc.collect()\n",
    "        fold=fold+1\n",
    "        #break\n",
    "    meter_loss = mean_squared_error(y_train, y_valid_pred_total)\n",
    "    print(f'======METER:{target_meter} MSE: {meter_loss:.4f} time={time.time() - t0:.5g}\\n')\n",
    "    losses.append(meter_loss)\n",
    "    meter_models.append(models_)\n",
    "    #sns.distplot(y_train)\n",
    "    del X_train, y_train\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load sample_submission and reduce its memomy usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 636.26 MB\n",
      "Memory usage after optimization is: 198.83 MB\n",
      "Decreased by 68.7%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>meter_reading</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697595</th>\n",
       "      <td>41697595</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697596</th>\n",
       "      <td>41697596</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697597</th>\n",
       "      <td>41697597</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697598</th>\n",
       "      <td>41697598</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41697599</th>\n",
       "      <td>41697599</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41697600 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            row_id  meter_reading\n",
       "0                0              0\n",
       "1                1              0\n",
       "2                2              0\n",
       "3                3              0\n",
       "4                4              0\n",
       "...            ...            ...\n",
       "41697595  41697595              0\n",
       "41697596  41697596              0\n",
       "41697597  41697597              0\n",
       "41697598  41697598              0\n",
       "41697599  41697599              0\n",
       "\n",
       "[41697600 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reduce_mem_usage(df, use_float16=False):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n",
    "            # skip datetime type or categorical type\n",
    "            continue\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "sample_submission = pd.read_csv(os.path.join(data_root, 'sample_submission.csv'))\n",
    "reduce_mem_usage(sample_submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate testing dataframe.\n",
    "If isImplicitMerge is False, the test_datas has all-features and is very large. If isImplicitMerge is True, no merging operation and its size is small. So we can use much larger batch size(*10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASHRAE_data_test......category=['building_id', 'site_id', 'primary_use']\n",
      "feature_cols=['square_feet', 'year_built', 'hour', 'weekend', 'building_median', 'air_temperature', 'cloud_coverage', 'dew_temperature', 'precip_depth_1_hr', 'sea_level_pressure', 'wind_direction', 'wind_speed', 'air_temperature_mean_lag3', 'air_temperature_max_lag3', 'air_temperature_min_lag3', 'air_temperature_std_lag3', 'cloud_coverage_mean_lag3', 'dew_temperature_mean_lag3', 'precip_depth_1_hr_mean_lag3', 'sea_level_pressure_mean_lag3', 'wind_direction_mean_lag3', 'wind_speed_mean_lag3', 'air_temperature_mean_lag72', 'air_temperature_max_lag72', 'air_temperature_min_lag72', 'air_temperature_std_lag72', 'cloud_coverage_mean_lag72', 'dew_temperature_mean_lag72', 'precip_depth_1_hr_mean_lag72', 'sea_level_pressure_mean_lag72', 'wind_direction_mean_lag72', 'wind_speed_mean_lag72']......\n",
      "2260080\n",
      "test_datas.building_mean=(1449,)\n",
      "ASHRAE_data_ df_base=(41697600, 13)\n"
     ]
    }
   ],
   "source": [
    "test_datas = ASHRAE_data(\"test\",data_root,building_meta_df,weather_test_df)\n",
    "del train_datas\n",
    "gc.collect()\n",
    "test_df = test_datas.df_base\n",
    "\n",
    "def pred(X_test, models, batch_size=1000000):\n",
    "    if isMORT and isImplicitMerge:\n",
    "        batch_size=batch_size*10\n",
    "    iterations = (X_test.shape[0] + batch_size -1) // batch_size\n",
    "    nSamp = X_test.shape[0]\n",
    "    print(f'iterations={iterations}\\tnSamp={nSamp}\\tbatch_size={batch_size}' )\n",
    "\n",
    "    y_test_pred_total = np.zeros(X_test.shape[0])\n",
    "    for i, model in enumerate(models):\n",
    "        print(f'predicting {i}-th model')\n",
    "        for k in tqdm(range(iterations)):\n",
    "            n_1=min(nSamp,(k+1)*batch_size)\n",
    "            y_pred_test = model.predict(X_test[k*batch_size:n_1], num_iteration=model.best_iteration)\n",
    "            y_test_pred_total[k*batch_size:n_1] += y_pred_test\n",
    "\n",
    "    y_test_pred_total /= len(models)\n",
    "    return y_test_pred_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each target_meter, call each models to get predictions.  \n",
    "Since the testing dataset merged with different whether data. **Don't forget to call  model.MergeDataSets(test_datas.merge_infos)**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "if isMORT and isImplicitMerge:\n",
    "        for i, model in enumerate(meter_models[target_meter]):\n",
    "            model.MergeDataSets(test_datas.merge_infos,comment=\"_predict\")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t target_meter=0......\n",
      "test_X_y@0 df_base=(41697600, 13)......\n",
      "target@0=(24755760, 13)\n",
      "data_X__@0=(24755760, 6)\toriginal=(24755760, 14)\tmerge=True\n",
      "\t target_meter=0 X_test=(24755760, 6)\n",
      "features=Index(['building_median', 'building_id', 'timestamp', 'site_id', 'weekend',\n",
      "       'hour'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-2945090c8aa2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misMORT\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misImplicitMerge\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeter_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_meter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMergeDataSets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_datas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_infos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcomment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"_predict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0my_test0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeter_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_meter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for target_meter in range(nTargetMeter):\n",
    "    print(f'\\t target_meter={target_meter}......')\n",
    "    X_test,_ = test_datas.data_X_y(target_meter)\n",
    "    print(f'\\t target_meter={target_meter} X_test={X_test.shape}\\nfeatures={X_test.columns}')\n",
    "    gc.collect()\n",
    "    if isMORT and isImplicitMerge:\n",
    "        for i, model in enumerate(meter_models[target_meter]):\n",
    "            model.MergeDataSets(test_datas.merge_infos,comment=\"_predict\")\n",
    "    y_test0 = pred(X_test, meter_models[target_meter])\n",
    "    #sns.distplot(y_test0); plt.show()\n",
    "    sample_submission.loc[test_df['meter'] == target_meter, 'meter_reading'] = np.expm1(y_test0)\n",
    "    del X_test\n",
    "    gc.collect()\n",
    "if use_ucf:\n",
    "    pass\n",
    "else:\n",
    "    submit_path = f'submission_{gbm}_.csv.gz'#f'./[{gbm}]_[{losses}].csv.gz'\n",
    "    sample_submission.to_csv(submit_path, index=False, float_format='%.4f',compression='gzip')\n",
    "    print(sample_submission.head(10),sample_submission.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the data leak from ucf to get more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReplaceUCF......\n",
      "2260080\n",
      "UCF score =  5.629970421767181\n",
      "    row_id  meter_reading\n",
      "0        0     173.370293\n",
      "1        1      53.512720\n",
      "2        2       6.143042\n",
      "3        3     101.701470\n",
      "4        4    1141.240666\n",
      "..     ...            ...\n",
      "95      95     439.841801\n",
      "96      96    1053.872971\n",
      "97      97      17.746565\n",
      "98      98      90.643994\n",
      "99      99     168.840950\n",
      "\n",
      "[100 rows x 2 columns]             row_id  meter_reading\n",
      "41697500  41697500            0.0\n",
      "41697501  41697501            0.0\n",
      "41697502  41697502            0.0\n",
      "41697503  41697503            0.0\n",
      "41697504  41697504            0.0\n",
      "...            ...            ...\n",
      "41697595  41697595            0.0\n",
      "41697596  41697596            0.0\n",
      "41697597  41697597            0.0\n",
      "41697598  41697598            0.0\n",
      "41697599  41697599            0.0\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "if use_ucf:\n",
    "    ReplaceUCF()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
