{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# Stacked Regression\n",
    "\n",
    "This notebook demonstrates a simple example of stacking regression models using [Mlxtend](http://rasbt.github.io/mlxtend/) (machine learning extensions) library. \n",
    "\n",
    "Code for data preprocessing and feature engineering is adapted from other kernels and slightly modified. \n",
    "\n",
    "There might be some mistakes in code or logic since this is my first kernel, as well as the first Kaggle competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Multiple regression models could be combined together via so-called meta-regressor. This ensemble learning technique is called stacking regression. Each individual regression model is trained first, and then the meta-regressor is fitted based on the outputs (meta-features) of those models in the ensemble. This process is shown on the figure below.\n",
    "\n",
    "<img src=\"http://rasbt.github.io/mlxtend/user_guide/regressor/StackingRegressor_files/stackingregression_overview.png\" width=\"500px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train data import and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import category_encoders\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "from mlxtend.regressor import StackingRegressor\n",
    "\n",
    "from pandas.api.types import is_categorical_dtype\n",
    "from pandas.api.types import is_datetime64_any_dtype as is_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory optimization\n",
    "\n",
    "# Original code from https://www.kaggle.com/gemartin/load-data-reduce-memory-usage by @gemartin\n",
    "# Modified to support timestamp type, categorical type\n",
    "# Modified to add option to use float16\n",
    "def reduce_mem_usage(data, use_float16=False) -> pd.DataFrame:\n",
    "    start_mem = data.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in data.columns:\n",
    "        if is_datetime(data[col]) or is_categorical_dtype(data[col]):\n",
    "            continue\n",
    "        col_type = data[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = data[col].min()\n",
    "            c_max = data[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    data[col] = data[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    data[col] = data[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    data[col] = data[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    data[col] = data[col].astype(np.int64)\n",
    "            else:\n",
    "                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    data[col] = data[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    data[col] = data[col].astype(np.float32)\n",
    "                else:\n",
    "                    data[col] = data[col].astype(np.float64)\n",
    "        else:\n",
    "            data[col] = data[col].astype('category')\n",
    "\n",
    "    end_mem = data.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.2f}%'.format(\n",
    "        100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to data \n",
    "PATH = '../input/ashrae-energy-prediction/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train data\n",
    "train = pd.read_csv(f'{PATH}train.csv')\n",
    "weather_train = pd.read_csv(f'{PATH}weather_train.csv')\n",
    "\n",
    "# Import metadata\n",
    "metadata = pd.read_csv(f'{PATH}building_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers in train data\n",
    "train = train[train['building_id'] != 1099]\n",
    "train = train.query('not (building_id <= 104 & meter == 0 & timestamp <= \"2016-05-20\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for weather data processing\n",
    "def weather_data_parser(weather_data) -> pd.DataFrame:\n",
    "    time_format = '%Y-%m-%d %H:%M:%S'\n",
    "    start_date = datetime.datetime.strptime(weather_data['timestamp'].min(), time_format)\n",
    "    end_date = datetime.datetime.strptime(weather_data['timestamp'].max(), time_format)\n",
    "    total_hours = int(((end_date - start_date).total_seconds() + 3600) / 3600)\n",
    "    hours_list = [(end_date - datetime.timedelta(hours=x)).strftime(time_format) for x in range(total_hours)]\n",
    "\n",
    "    for site_id in range(16):\n",
    "        site_hours = np.array(weather_data[weather_data['site_id'] == site_id]['timestamp'])\n",
    "        new_rows = pd.DataFrame(np.setdiff1d(hours_list, site_hours), columns=['timestamp'])\n",
    "        new_rows['site_id'] = site_id\n",
    "        weather_data = pd.concat([weather_data, new_rows], sort=True)\n",
    "        weather_data = weather_data.reset_index(drop=True)           \n",
    "\n",
    "    weather_data['datetime'] = pd.to_datetime(weather_data['timestamp'])\n",
    "    weather_data['day'] = weather_data['datetime'].dt.day\n",
    "    weather_data['week'] = weather_data['datetime'].dt.week\n",
    "    weather_data['month'] = weather_data['datetime'].dt.month\n",
    "\n",
    "    weather_data = weather_data.set_index(['site_id', 'day', 'month'])\n",
    "\n",
    "    air_temperature_filler = pd.DataFrame(weather_data.groupby(['site_id','day','month'])['air_temperature'].median(), columns=['air_temperature'])\n",
    "    weather_data.update(air_temperature_filler, overwrite=False)\n",
    "\n",
    "    cloud_coverage_filler = weather_data.groupby(['site_id', 'day', 'month'])['cloud_coverage'].median()\n",
    "    cloud_coverage_filler = pd.DataFrame(cloud_coverage_filler.fillna(method='ffill'), columns=['cloud_coverage'])\n",
    "\n",
    "    weather_data.update(cloud_coverage_filler, overwrite=False)\n",
    "\n",
    "    due_temperature_filler = pd.DataFrame(weather_data.groupby(['site_id','day','month'])['dew_temperature'].median(), columns=['dew_temperature'])\n",
    "    weather_data.update(due_temperature_filler, overwrite=False)\n",
    "\n",
    "    sea_level_filler = weather_data.groupby(['site_id','day','month'])['sea_level_pressure'].median()\n",
    "    sea_level_filler = pd.DataFrame(sea_level_filler.fillna(method='ffill'), columns=['sea_level_pressure'])\n",
    "\n",
    "    weather_data.update(sea_level_filler, overwrite=False)\n",
    "\n",
    "    wind_direction_filler =  pd.DataFrame(weather_data.groupby(['site_id','day','month'])['wind_direction'].median(), columns=['wind_direction'])\n",
    "    weather_data.update(wind_direction_filler, overwrite=False)\n",
    "\n",
    "    wind_speed_filler =  pd.DataFrame(weather_data.groupby(['site_id','day','month'])['wind_speed'].median(), columns=['wind_speed'])\n",
    "    weather_data.update(wind_speed_filler, overwrite=False)\n",
    "\n",
    "    precip_depth_filler = weather_data.groupby(['site_id','day','month'])['precip_depth_1_hr'].median()\n",
    "    precip_depth_filler = pd.DataFrame(precip_depth_filler.fillna(method='ffill'), columns=['precip_depth_1_hr'])\n",
    "\n",
    "    weather_data.update(precip_depth_filler, overwrite=False)\n",
    "\n",
    "    weather_data = weather_data.reset_index()\n",
    "    weather_data = weather_data.drop(['datetime','day','week','month'], axis=1)\n",
    "\n",
    "    return weather_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train weather data processing\n",
    "weather_train = weather_data_parser(weather_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 757.31 MB\n",
      "Memory usage after optimization is: 322.24 MB\n",
      "Decreased by 57.45%\n",
      "Memory usage of dataframe is 9.65 MB\n",
      "Memory usage after optimization is: 2.66 MB\n",
      "Decreased by 72.46%\n",
      "Memory usage of dataframe is 0.07 MB\n",
      "Memory usage after optimization is: 0.02 MB\n",
      "Decreased by 73.76%\n"
     ]
    }
   ],
   "source": [
    "# Memory optimization\n",
    "train = reduce_mem_usage(train, use_float16=True)\n",
    "weather_train = reduce_mem_usage(weather_train, use_float16=True)\n",
    "metadata = reduce_mem_usage(metadata, use_float16=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge train data \n",
    "train = train.merge(metadata, on='building_id', how='left')\n",
    "train = train.merge(weather_train, on=['site_id', 'timestamp'], how='left')\n",
    "\n",
    "del weather_train; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for train and test data processing\n",
    "def data_parser(data) -> pd.DataFrame:\n",
    "    data.sort_values('timestamp')\n",
    "    data.reset_index(drop=True)\n",
    "    \n",
    "    data['timestamp'] = pd.to_datetime(data['timestamp'], format='%Y-%m-%d %H:%M:%S')\n",
    "    data['weekday'] = data['timestamp'].dt.weekday\n",
    "    data['hour'] = data['timestamp'].dt.hour\n",
    "    \n",
    "    data['square_feet'] =  np.log1p(data['square_feet']) \n",
    "    \n",
    "    data = data.drop(['timestamp', 'sea_level_pressure',\n",
    "        'wind_direction', 'wind_speed', 'year_built', 'floor_count'], axis=1)\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    encoder = LabelEncoder()\n",
    "    data['primary_use'] = encoder.fit_transform(data['primary_use'])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data processing\n",
    "train = data_parser(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define target and predictors\n",
    "target = np.log1p(train['meter_reading'])\n",
    "features = train.drop(['meter_reading'], axis = 1) \n",
    "\n",
    "del train; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process categorical features\n",
    "categorical_features = ['building_id', 'site_id', 'meter', 'primary_use']\n",
    "\n",
    "encoder = category_encoders.CountEncoder(cols=categorical_features)\n",
    "encoder.fit(features)\n",
    "features = encoder.transform(features)\n",
    "\n",
    "features_size = features.shape[0]\n",
    "for feature in categorical_features:\n",
    "    features[feature] = features[feature] / features_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing data imputation\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "imputer.fit(features)\n",
    "features = imputer.transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training \n",
    "\n",
    "As a simple example we will create meta-regressor by stacking together regularized Ridge and Lasso regression models with LightGBM regressor. In this example there is minimum hyperparameters tuning and just three regressors are stacked together for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regressors\n",
    "lightgbm = LGBMRegressor(objective='regression', learning_rate=0.05, num_leaves=1024,\n",
    "    feature_fraction=0.8, bagging_fraction=0.9, bagging_freq=5) \n",
    "\n",
    "ridge = Ridge(alpha=0.3)\n",
    "lasso = Lasso(alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.1669 of fold: 0\n",
      "RMSE: 1.1831 of fold: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold = KFold(n_splits=2, shuffle=False)\n",
    "\n",
    "models = []\n",
    "\n",
    "for idx, (train_idx, val_idx) in enumerate(kfold.split(features)):\n",
    "    \n",
    "    train_features, train_target = features[train_idx], target[train_idx]\n",
    "    val_features, val_target = features[val_idx], target[val_idx]\n",
    "    \n",
    "    model = StackingRegressor(regressors=(lightgbm, ridge, lasso),\n",
    "        meta_regressor=lightgbm, use_features_in_secondary=True)\n",
    "\n",
    "    model.fit(np.array(train_features), np.array(train_target))\n",
    "    models.append(model)\n",
    "\n",
    "    print('RMSE: {:.4f} of fold: {}'.format(\n",
    "        np.sqrt(mean_squared_error(val_target, model.predict(np.array(val_features)))), idx))\n",
    "\n",
    "    del train_features, train_target, val_features, val_target; gc.collect()\n",
    "\n",
    "del features, target; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data import and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import test data\n",
    "test = pd.read_csv(f'{PATH}test.csv')\n",
    "weather_test = pd.read_csv(f'{PATH}weather_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_ids = test['row_id']\n",
    "test.drop('row_id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test weather data processing\n",
    "weather_test = weather_data_parser(weather_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 954.38 MB\n",
      "Memory usage after optimization is: 199.59 MB\n",
      "Decreased by 79.09%\n",
      "Memory usage of dataframe is 19.25 MB\n",
      "Memory usage after optimization is: 5.30 MB\n",
      "Decreased by 72.45%\n"
     ]
    }
   ],
   "source": [
    "# Memory optimization\n",
    "test = reduce_mem_usage(test, use_float16=True)\n",
    "weather_test = reduce_mem_usage(weather_test, use_float16=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge test data\n",
    "test = test.merge(metadata, on='building_id', how='left')\n",
    "test = test.merge(weather_test, on=['site_id', 'timestamp'], how='left')\n",
    "\n",
    "del metadata; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data processing\n",
    "test = data_parser(test)\n",
    "\n",
    "test = encoder.transform(test)\n",
    "for feature in categorical_features:\n",
    "    test[feature] = test[feature] / features_size\n",
    "\n",
    "test = imputer.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions and create submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions = 0\n",
    "for model in models:\n",
    "    predictions += np.expm1(model.predict(np.array(test))) / len(models)\n",
    "    del model; gc.collect()\n",
    "\n",
    "del test, models; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'row_id': row_ids,\n",
    "    'meter_reading': np.clip(predictions, 0, a_max=None)\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False, float_format='%.4f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
