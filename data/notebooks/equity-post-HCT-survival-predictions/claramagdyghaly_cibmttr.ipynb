{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0f18399",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-03T07:35:48.489990Z",
     "iopub.status.busy": "2025-03-03T07:35:48.489511Z",
     "iopub.status.idle": "2025-03-03T07:39:06.105346Z",
     "shell.execute_reply": "2025-03-03T07:39:06.104032Z"
    },
    "papermill": {
     "duration": 197.622005,
     "end_time": "2025-03-03T07:39:06.107595",
     "exception": false,
     "start_time": "2025-03-03T07:35:48.485590",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m720/720\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 16897.2520 - val_loss: 5626.7012\n",
      "Epoch 2/50\n",
      "\u001b[1m720/720\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 3366.7190 - val_loss: 569.7400\n",
      "Epoch 3/50\n",
      "\u001b[1m720/720\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 716.3185 - val_loss: 554.2471\n",
      "Epoch 4/50\n",
      "\u001b[1m720/720\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 706.4249 - val_loss: 553.8989\n",
      "Epoch 5/50\n",
      "\u001b[1m720/720\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 690.8177 - val_loss: 548.5469\n",
      "Epoch 6/50\n",
      "\u001b[1m720/720\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 683.3339 - val_loss: 548.3948\n",
      "Epoch 7/50\n",
      "\u001b[1m720/720\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 686.4713 - val_loss: 552.4958\n",
      "Epoch 8/50\n",
      "\u001b[1m720/720\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 683.5595 - val_loss: 548.0825\n",
      "Epoch 9/50\n",
      "\u001b[1m720/720\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 670.6823 - val_loss: 548.1863\n",
      "Epoch 10/50\n",
      "\u001b[1m720/720\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 680.8858 - val_loss: 547.3176\n",
      "Epoch 11/50\n",
      "\u001b[1m720/720\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 668.7446 - val_loss: 547.5831\n",
      "Epoch 12/50\n",
      "\u001b[1m720/720\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 657.0316 - val_loss: 549.7308\n",
      "Epoch 13/50\n",
      "\u001b[1m720/720\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 669.2548 - val_loss: 549.0060\n",
      "Epoch 14/50\n",
      "\u001b[1m720/720\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 653.1495 - val_loss: 552.2695\n",
      "Epoch 15/50\n",
      "\u001b[1m720/720\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 667.2374 - val_loss: 547.2548\n",
      "Epoch 16/50\n",
      "\u001b[1m720/720\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 655.7697 - val_loss: 544.4606\n",
      "Epoch 17/50\n",
      "\u001b[1m720/720\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 650.0632 - val_loss: 546.5292\n",
      "Epoch 18/50\n",
      "\u001b[1m720/720\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 674.7310 - val_loss: 546.4892\n",
      "Epoch 19/50\n",
      "\u001b[1m720/720\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 664.7938 - val_loss: 548.7031\n",
      "Epoch 20/50\n",
      "\u001b[1m720/720\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 649.2547 - val_loss: 543.4320\n",
      "Epoch 21/50\n",
      "\u001b[1m720/720\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 657.6254 - val_loss: 544.9252\n",
      "Epoch 22/50\n",
      "\u001b[1m720/720\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 658.1514 - val_loss: 547.3466\n",
      "Epoch 23/50\n",
      "\u001b[1m720/720\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 654.7949 - val_loss: 544.7399\n",
      "Epoch 24/50\n",
      "\u001b[1m720/720\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 651.7600 - val_loss: 546.3167\n",
      "Epoch 25/50\n",
      "\u001b[1m720/720\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 639.8229 - val_loss: 546.8956\n",
      "Epoch 26/50\n",
      "\u001b[1m720/720\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 637.8722 - val_loss: 546.8558\n",
      "Epoch 27/50\n",
      "\u001b[1m720/720\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 636.7212 - val_loss: 545.9507\n",
      "Epoch 28/50\n",
      "\u001b[1m720/720\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 653.9551 - val_loss: 546.3163\n",
      "Epoch 29/50\n",
      "\u001b[1m720/720\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 639.8596 - val_loss: 544.5566\n",
      "Epoch 30/50\n",
      "\u001b[1m720/720\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 648.5222 - val_loss: 547.0890\n",
      "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "NN c-index:  0.5961\n",
      "XGB c-index: 0.5773\n",
      "RF c-index:  0.5795\n",
      "Cat c-index: 0.5964\n",
      "NN strat c-index:  0.5903\n",
      "XGB strat c-index: 0.5704\n",
      "RF strat c-index:  0.5706\n",
      "Cat strat c-index: 0.5910\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step\n",
      "Submission file saved as submission.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Load Data\n",
    "# ---------------------------\n",
    "train_path = \"/kaggle/input/equity-post-HCT-survival-predictions/train.csv\"\n",
    "test_path = \"/kaggle/input/equity-post-HCT-survival-predictions/test.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "# Save and drop the ID column\n",
    "train_ids = train_df[\"ID\"]\n",
    "test_ids = test_df[\"ID\"]\n",
    "train_df.drop(columns=[\"ID\"], inplace=True)\n",
    "test_df.drop(columns=[\"ID\"], inplace=True)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Define Target and Positive \"Risk Score\"\n",
    "# ---------------------------\n",
    "#   - 'efs_time' is the survival time.\n",
    "#   - 'efs' is the event indicator (0 = censored, 1 = event).\n",
    "#   - We'll define a \"risk_score\" so that higher values = shorter survival.\n",
    "#     Example: risk_score = max_survival - efs_time\n",
    "y_true_all = train_df[[\"efs_time\", \"efs\"]]\n",
    "\n",
    "max_survival = train_df[\"efs_time\"].max()\n",
    "risk_score = max_survival - train_df[\"efs_time\"]  # Higher for shorter survival\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Prepare Features\n",
    "# ---------------------------\n",
    "if \"race_group\" in train_df.columns:\n",
    "    # Keep race_group for stratified evaluation if needed\n",
    "    race_group_series = train_df[\"race_group\"]\n",
    "    X = train_df.drop(columns=[\"efs_time\", \"efs\", \"race_group\"])\n",
    "else:\n",
    "    race_group_series = None\n",
    "    X = train_df.drop(columns=[\"efs_time\", \"efs\"])\n",
    "\n",
    "# Drop race_group from test if present\n",
    "if \"race_group\" in test_df.columns:\n",
    "    test_df.drop(columns=[\"race_group\"], inplace=True)\n",
    "\n",
    "# Drop any remaining non-numeric columns\n",
    "non_numeric_cols = X.select_dtypes(exclude=[np.number]).columns\n",
    "X.drop(columns=non_numeric_cols, inplace=True)\n",
    "test_df.drop(columns=test_df.select_dtypes(exclude=[np.number]).columns, inplace=True)\n",
    "\n",
    "# Fill missing values\n",
    "X.fillna(X.mean(), inplace=True)\n",
    "test_df.fillna(test_df.mean(), inplace=True)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "test_scaled = scaler.transform(test_df)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Train-Test Split\n",
    "# ---------------------------\n",
    "X_train, X_val, y_train_risk, y_val_risk, y_train_true, y_val_true = train_test_split(\n",
    "    X_scaled, risk_score, y_true_all, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "if race_group_series is not None:\n",
    "    race_group_train, race_group_val = train_test_split(race_group_series, test_size=0.2, random_state=42)\n",
    "else:\n",
    "    race_group_val = None\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Concordance Index Functions\n",
    "# ---------------------------\n",
    "def concordance_index(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the concordance index comparing survival times with predicted risk scores.\n",
    "    Higher risk => shorter survival time.\n",
    "    \"\"\"\n",
    "    y_true = y_true.to_numpy()\n",
    "    times = y_true[:, 0]  # efs_time\n",
    "    n_pairs = 0\n",
    "    concordant_pairs = 0\n",
    "\n",
    "    for i in range(len(times)):\n",
    "        for j in range(i + 1, len(times)):\n",
    "            if times[i] != times[j]:\n",
    "                n_pairs += 1\n",
    "                # With risk_score = max_survival - survival_time:\n",
    "                # if times[i] > times[j], we expect y_pred[i] < y_pred[j].\n",
    "                if (times[i] > times[j] and y_pred[i] < y_pred[j]) or \\\n",
    "                   (times[i] < times[j] and y_pred[i] > y_pred[j]):\n",
    "                    concordant_pairs += 1\n",
    "\n",
    "    return concordant_pairs / n_pairs if n_pairs > 0 else 0.5\n",
    "\n",
    "def stratified_concordance_index(y_true, y_pred, race_group):\n",
    "    \"\"\"\n",
    "    Compute stratified c-index = mean(c-index per race_group) - std(c-index per race_group).\n",
    "    \"\"\"\n",
    "    unique_race_groups = np.unique(race_group)\n",
    "    c_indices = []\n",
    "    for r in unique_race_groups:\n",
    "        idx = (race_group == r)\n",
    "        if np.sum(idx) < 2:\n",
    "            continue\n",
    "        c_idx = concordance_index(y_true[idx], y_pred[idx])\n",
    "        c_indices.append(c_idx)\n",
    "    c_indices = np.array(c_indices)\n",
    "    if len(c_indices) == 0:\n",
    "        return 0.5\n",
    "    return c_indices.mean() - c_indices.std()\n",
    "\n",
    "# ---------------------------\n",
    "# 6. Train 4 Models (NN, XGBoost, RF, CatBoost)\n",
    "# ---------------------------\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Neural Network Regressor\n",
    "nn_model = Sequential([\n",
    "    Dense(256, activation=\"relu\", input_shape=(X_train.shape[1],)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation=\"linear\")\n",
    "])\n",
    "nn_model.compile(optimizer=Adam(learning_rate=0.001), loss=\"mse\")\n",
    "nn_callbacks = [EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)]\n",
    "\n",
    "nn_model.fit(X_train, y_train_risk, epochs=50, batch_size=32,\n",
    "             validation_data=(X_val, y_val_risk),\n",
    "             verbose=1, callbacks=nn_callbacks)\n",
    "\n",
    "nn_val_preds = nn_model.predict(X_val).flatten()\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "xgb_model = XGBRegressor(learning_rate=0.001, random_state=42)\n",
    "xgb_model.fit(X_train, y_train_risk)\n",
    "xgb_val_preds = xgb_model.predict(X_val)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "rf_model.fit(X_train, y_train_risk)\n",
    "rf_val_preds = rf_model.predict(X_val)\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "cat_model = CatBoostRegressor(verbose=0, random_state=42)\n",
    "cat_model.fit(X_train, y_train_risk)\n",
    "cat_val_preds = cat_model.predict(X_val)\n",
    "\n",
    "# Evaluate each on validation set\n",
    "nn_ci = concordance_index(y_val_true, nn_val_preds)\n",
    "xgb_ci = concordance_index(y_val_true, xgb_val_preds)\n",
    "rf_ci = concordance_index(y_val_true, rf_val_preds)\n",
    "cat_ci = concordance_index(y_val_true, cat_val_preds)\n",
    "\n",
    "print(f\"NN c-index:  {nn_ci:.4f}\")\n",
    "print(f\"XGB c-index: {xgb_ci:.4f}\")\n",
    "print(f\"RF c-index:  {rf_ci:.4f}\")\n",
    "print(f\"Cat c-index: {cat_ci:.4f}\")\n",
    "\n",
    "if race_group_val is not None:\n",
    "    nn_strat_ci = stratified_concordance_index(y_val_true, nn_val_preds, race_group_val.to_numpy())\n",
    "    xgb_strat_ci = stratified_concordance_index(y_val_true, xgb_val_preds, race_group_val.to_numpy())\n",
    "    rf_strat_ci = stratified_concordance_index(y_val_true, rf_val_preds, race_group_val.to_numpy())\n",
    "    cat_strat_ci = stratified_concordance_index(y_val_true, cat_val_preds, race_group_val.to_numpy())\n",
    "\n",
    "    print(f\"NN strat c-index:  {nn_strat_ci:.4f}\")\n",
    "    print(f\"XGB strat c-index: {xgb_strat_ci:.4f}\")\n",
    "    print(f\"RF strat c-index:  {rf_strat_ci:.4f}\")\n",
    "    print(f\"Cat strat c-index: {cat_strat_ci:.4f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 7. Predict on Test & SCALE to Smaller Range\n",
    "# ---------------------------\n",
    "# Predict risk on the test set\n",
    "test_preds_nn = nn_model.predict(test_scaled).flatten()\n",
    "test_preds_xgb = xgb_model.predict(test_scaled)\n",
    "test_preds_rf = rf_model.predict(test_scaled)\n",
    "test_preds_cat = cat_model.predict(test_scaled)\n",
    "\n",
    "# Ensemble: average\n",
    "final_test_preds = (test_preds_nn + test_preds_xgb + test_preds_rf + test_preds_cat) / 4\n",
    "\n",
    "# EXAMPLE: Min-Max scale these ensemble scores to a smaller range, e.g. [0, 2].\n",
    "# (Adjust the range if you want them around [0,1], [0,2], etc.)\n",
    "min_pred = final_test_preds.min()\n",
    "max_pred = final_test_preds.max()\n",
    "scaled_test_preds = (final_test_preds - min_pred) / (max_pred - min_pred) * 2.0\n",
    "\n",
    "# ---------------------------\n",
    "# 8. Create Submission File\n",
    "# ---------------------------\n",
    "# \"scaled_test_preds\" now are smaller values, e.g. 0.5, 1.2, 0.8, ...\n",
    "submission = pd.DataFrame({\"ID\": test_ids, \"prediction\": scaled_test_preds})\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Submission file saved as submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaf2914",
   "metadata": {
    "papermill": {
     "duration": 0.082924,
     "end_time": "2025-03-03T07:39:06.264058",
     "exception": false,
     "start_time": "2025-03-03T07:39:06.181134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10381525,
     "sourceId": 70942,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 202.705526,
   "end_time": "2025-03-03T07:39:08.267426",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-03T07:35:45.561900",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
