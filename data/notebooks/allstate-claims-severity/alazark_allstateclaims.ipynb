{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "538d136d-7996-ae1e-4d22-a6ac3742bc3e"
      },
      "source": [
        "Allstate Claims analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3dc17d8d-31d4-ffd5-aa3d-51dae02e4b42"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import matplotlib.pyplot as plt # for plotting\n",
        "# Any results you write to the current directory are saved as output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "68a87654-c30b-d69b-e8c7-a8241e532f9a"
      },
      "outputs": [],
      "source": [
        "#read data from CSV file\n",
        "train = pd.read_csv(\"../input/train.csv\")\n",
        "test = pd.read_csv(\"../input/test.csv\")\n",
        "#Some info about the data\n",
        "#train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "108b94c1-4145-cda2-f3bd-2b7ccecafb53"
      },
      "outputs": [],
      "source": [
        "#checking some of the categorical data\n",
        "print(train.cat1.value_counts())\n",
        "print(train.cat51.value_counts())\n",
        "print(train.cat101.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "851cf109-aa03-415d-ab16-a516c32aa1a6"
      },
      "outputs": [],
      "source": [
        "#Data housekeeping\n",
        "#Category variables 116\n",
        "train_cat = train.ix[:,'cat1':'cat116']\n",
        "#Continuous variable 14\n",
        "train_cont = train.ix[:,'cont1':'cont14']\n",
        "#train_cat_dummy = pd.get_dummies(train_cat)\n",
        "#print(train_cat.head(2))\n",
        "#print(train_cat_dummy.head(2))\n",
        "#Number of training samples\n",
        "print(\"number of traning samples : {}\".format(train.shape[0]))\n",
        "print(\"number of test samples: {}\".format(test.shape[0]))\n",
        "#count the number of unique values under the categorical variables\n",
        "print(\"number of unique categorical values : {}\".format(len(pd.unique(train_cat[train_cat.columns[1:]].values.ravel()))))\n",
        "#Check if there are any null values\n",
        "#print(train.isnull().values.any())\n",
        "#print(test.isnull().values.any())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "102a901c-7c02-7791-7423-f28d2fa75398"
      },
      "outputs": [],
      "source": [
        "#correlation between continuous predictors and traget\n",
        "train_corr_loss = train.corr()[\"loss\"]\n",
        "ax = train_corr_loss.iloc[1:-1].plot(kind='bar',title=\"continuous features correlation with target\", figsize=(5,5), fontsize=12)\n",
        "ax.set_ylabel(\"correlation value\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "47c8bea2-d527-9ca4-2068-dda394e4553c"
      },
      "source": [
        "As we can see cont2 has the higher correlation with target followed by cont8,cont3,cont11 and cont12.\n",
        "cont13 has the lowest correlation.In general, all these features have low correlation values to the target, < 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "150bbd0e-8cf0-ac3e-cf68-141215a140ff"
      },
      "outputs": [],
      "source": [
        "#classifcation just considering the continous features(why?)\n",
        "y_train1 = np.asarray(train['loss'])\n",
        "X_test1 = test.ix[:,'cont1':'cont14']\n",
        "lreg = LinearRegression()\n",
        "lreg.fit(train_cont,y_train1)\n",
        "y_pred = lreg.predict(X_test1)\n",
        "print(\"Training set score: {:.2f}\".format(lreg.score(train_cont, y_train1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9fe494b3-2e63-a902-01ee-1970260b7e70"
      },
      "source": [
        "The fact that a lot of features are missing implies too simple model(under fit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "70659f5b-1882-e0a3-7151-9f22bcaa42d1"
      },
      "outputs": [],
      "source": [
        "#Categorical features analysis\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "catFeatures = []\n",
        "for colName in train_cat.columns:\n",
        "    le = LabelEncoder()\n",
        "    le.fit(train_cat[colName].unique())\n",
        "    train_cat[colName] = le.transform(train_cat[colName])\n",
        "train_cat.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5a7e8e3b-a911-090a-5795-d8c9b95bb3f6"
      },
      "outputs": [],
      "source": [
        "catFeatures = []\n",
        "test_cat = test.ix[:,'cat1':'cat101']\n",
        "for colName in test_cat.columns:\n",
        "    le = LabelEncoder()\n",
        "    le.fit(test_cat[colName].unique())\n",
        "    test_cat[colName] = le.transform(test_cat[colName])\n",
        "test_cat.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9a6aa207-ac80-e14c-a82e-ed9e4f69508d"
      },
      "outputs": [],
      "source": [
        "test_cont = test.ix[:,'cont1':'cont14']\n",
        "X_train = train_cat.ix[:,'cat1':'cat101'].join(train_cont)\n",
        "X_test = test_cat.join(test_cont)\n",
        "lreg.fit(X_train,y_train1)\n",
        "y_pred = lreg.predict(X_test)\n",
        "print(\"Linear: Training set score: {:.2f}\".format(lreg.score(X_train, y_train1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fe79f27e-a1c4-5aa5-85f6-2f579ff9cd89"
      },
      "outputs": [],
      "source": [
        "#Categorical features analysis using  dummy variables\n",
        "train_cat_dummy = pd.get_dummies(train_cat.ix[:,'cat1':'cat101'])\n",
        "test_cat_dummy = pd.get_dummies(test_cat)\n",
        "\n",
        "print(\"new dummy DF shapes(train)(test):{}{}\".format(train_cat_dummy.shape,test_cat_dummy.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4a44f78d-a71b-385a-6963-ec23c788ff4c"
      },
      "outputs": [],
      "source": [
        "X_train = train_cat_dummy.join(train_cont)\n",
        "X_test = test_cat_dummy.join(test_cont)\n",
        "lreg.fit(X_train,y_train1)\n",
        "y_pred = lreg.predict(X_test)\n",
        "print(\"Linear: Training set score: {:.2f}\".format(lreg.score(X_train, y_train1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "07590e4a-01e2-47ca-baf6-38e3eff00314"
      },
      "source": [
        "\"The kernel was killed for trying to exceed the memory limit of\n",
        " 8589934592; you can use the restart button in the toolbar to try.\n",
        "\"\n",
        "It looks like I have to find a way to reduce the number of features so as to reduce the memory requirement for prediction. So I randomly truncated the number of categorical features, including the first 30 gives 32% score for the training data prediction.similarly, including 80 of the categorical features  gives score of 47%, 95|49%, 101|49%,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4ddd121c-a9df-0be6-87ae-16ae35f8932b"
      },
      "outputs": [],
      "source": [
        "#Other SGD regressor\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "reg= SGDRegressor()\n",
        "reg.fit(X_train, y_train1)\n",
        "y_pred = reg.predict(X_test)\n",
        "print(\"SGD: Training set score: {:.2f}\".format(reg.score(X_train, y_train1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2433af5d-b0ea-0aa5-e658-ec89808e7df7"
      },
      "outputs": [],
      "source": [
        "#Using decisionTreeRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "tree = DecisionTreeRegressor(max_depth=100, random_state=0)\n",
        "tree.fit(X_train,y_train1)\n",
        "print(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train1)))\n",
        "y_pred = tree.predict(X_test)\n",
        "print(y_pred[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "8b2eef54-b45d-5c55-7c3a-5f3dcc322c08"
      },
      "source": [
        "The result shows decision tree algorithm gives a better performance. By tweaking the value max_depth, it's possible to get a better result to 82.6%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cc095ff5-596b-7e76-856e-cf516e1a5a35"
      },
      "outputs": [],
      "source": [
        "#checking out important features\n",
        "n_features = X_train.shape[1]\n",
        "imp = tree.feature_importances_\n",
        "top_imps = imp[imp > 0.015]\n",
        "indices_imps = np.where(imp > 0.015)\n",
        "plt.barh(range(len(top_imps)),top_imps, align=\"center\")\n",
        "plt.yticks(np.arange(len(top_imps)), X_train.columns[indices_imps])\n",
        "plt.xlabel(\"Feature importance\")\n",
        "plt.ylabel(\"Feature\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b01f27b2-f198-2994-a60f-c4f444d5e46f"
      },
      "source": [
        "Above are some of the most important features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2de01714-0be6-0a44-3d69-6a29281239b5"
      },
      "outputs": [],
      "source": [
        "#RandomForest regressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "rfr = RandomForestRegressor(n_estimators=190, random_state=50)\n",
        "rfr.fit(X_train,y_train1)\n",
        "print(\"Accuracy on training set: {:.3f}\".format(rfr.score(X_train, y_train1)))\n",
        "y_pred2 = rfr.predict(X_test)\n",
        "print(y_pred2[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fd0d956f-ac0d-6d0b-3c35-7df4d940ab53"
      },
      "outputs": [],
      "source": [
        "#Gradient boosting classifier\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "grbr = GradientBoostingRegressor(n_estimators=200, random_state=0, max_depth=10, learning_rate=0.01)\n",
        "grbr.fit(X_train, y_train1)\n",
        "print(\"Accuracy on training set: {:.3f}\".format(grbr.score(X_train, y_train1)))\n",
        "y_pred3 = grbr.predict(X_test)\n",
        "print(y_pred2[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9c50a2e8-10c2-7c87-704c-87dd22b2d673"
      },
      "source": [
        "Gradient boosted decision trees are the most powerful and widely used models for supervised learning.The main drawback is requirement of careful tuning of the parameters, and training may take a long time.Other supervised models I will experiment with are the Kernelized Support Vector Machines(SVMs) and neural networks(including deep learning and convolutional neural networks)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c8af2833-cc55-d215-ab68-2432c1f77f24"
      },
      "outputs": [],
      "source": [
        "submission7 = pd.DataFrame({\n",
        "        \"id\": test[\"id\"],\n",
        "        \"loss\": y_pred2\n",
        "    })\n",
        "submission7.to_csv('sample_submission6.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}