{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/allstate-claims-severity/train.csv\n",
      "/kaggle/input/allstate-claims-severity/train.csv.zip\n",
      "/kaggle/input/allstate-claims-severity/test.csv.zip\n",
      "/kaggle/input/allstate-claims-severity/sample_submission.csv.zip\n",
      "/kaggle/input/allstate-claims-severity/test.csv\n",
      "/kaggle/input/allstate-claims-severity/sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "data_dir = '/kaggle/input/allstate-claims-severity/'\n",
    "\n",
    "train_data_path = data_dir+'train.csv'\n",
    "test_data_path = data_dir+'test.csv'\n",
    "submission_csv_path = data_dir+'sample_submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Encoding categorical features . . .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Label Encode all categorical features\n",
    "def get_labelEncoded_dataframes(data_dir):\n",
    "    '''\n",
    "    creates a label encoded dataframe out of the categorical features using sklearns's LabelEncoder\n",
    "    saves new dataframe in object_dir\n",
    "    skips creating new dataframe if already exists\n",
    "    '''\n",
    "    print('Label Encoding categorical features . . .')\n",
    "    train_data = pd.read_csv(data_dir+'train.csv')\n",
    "    test_data = pd.read_csv(data_dir+'test.csv')\n",
    "    cat_cols = [x for x in train_data.columns if x.startswith('cat')]\n",
    "\n",
    "    for col in cat_cols:\n",
    "        le = LabelEncoder()\n",
    "        train_data[col] = le.fit_transform(train_data[col])\n",
    "        # update::\n",
    "        # Test data had some values in some cateogorical features that were unseen in train data\n",
    "        # the next 2 lines fix that :|\n",
    "        test_data[col] = test_data[col].map(lambda s: 'UNK' if s not in le.classes_ else s)\n",
    "        le.classes_ = np.append(le.classes_, 'UNK')\n",
    "        test_data[col] = le.transform(test_data[col])\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "train_data, test_data = get_labelEncoded_dataframes(data_dir)\n",
    "submission = pd.read_csv(submission_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data.iloc[:,1:-1]\n",
    "Y = train_data.iloc[:,-1]\n",
    "\n",
    "\n",
    "# get categorical and continuous features names\n",
    "cat_cols = [x for x in train_data.columns if x.startswith('cat')]\n",
    "cont_cols = [x for x in train_data.columns if x.startswith('cont')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features with f_regression score > 100\n",
      "['cat32', 'cat49', 'cat114', 'cat112', 'cat61', 'cont8', 'cat20', 'cat34', 'cat52', 'cat104', 'cat83', 'cat116', 'cat99', 'cat51', 'cat19', 'cat47', 'cont4', 'cat58', 'cat67', 'cont6', 'cat18', 'cat84', 'cat59', 'cat33', 'cat95', 'cat46', 'cat43', 'cat44', 'cat30', 'cat53', 'cat26', 'cat78', 'cat66', 'cat100', 'cat65', 'cat71', 'cat106', 'cat45', 'cat75', 'cat17', 'cat85', 'cat29', 'cat102', 'cat8', 'cat41', 'cat76', 'cat25', 'cat24', 'cat94', 'cat38', 'cont12', 'cont11', 'cat14', 'cat82', 'cat4', 'cat5', 'cat50', 'cont3', 'cat105', 'cat6', 'cont7', 'cat28', 'cat40', 'cont2', 'cat111', 'cat103', 'cat73', 'cat36', 'cat23', 'cat90', 'cat16', 'cat3', 'cat9', 'cat13', 'cat1', 'cat11', 'cat72', 'cat2', 'cat81', 'cat89', 'cat7', 'cat10', 'cat12', 'cat57', 'cat87', 'cat101', 'cat79', 'cat80']\n",
      "features with mutual_information score > 100\n",
      "['cat32', 'cat93', 'cat68', 'cat9', 'cat53', 'cat108', 'cat52', 'cont8', 'cat85', 'cat66', 'cat48', 'cat54', 'cat110', 'cat98', 'cat40', 'cont10', 'cat44', 'cont12', 'cat45', 'cat34', 'cat103', 'cat86', 'cat5', 'cat18', 'cat30', 'cat46', 'cat116', 'cat76', 'cat95', 'cat109', 'cat15', 'cont13', 'cont5', 'cat42', 'cat25', 'cat1', 'cont7', 'cat92', 'cat41', 'cat39', 'cat26', 'cat29', 'cat77', 'cat112', 'cat83', 'cat111', 'cont6', 'cat114', 'cat24', 'cat6', 'cont14', 'cont9', 'cat37', 'cat106', 'cat74', 'cat4', 'cont4', 'cat91', 'cat17', 'cat113', 'cat14', 'cat51', 'cat7', 'cat8', 'cat90', 'cat104', 'cat73', 'cont3', 'cat12', 'cat58', 'cat10', 'cat3', 'cat2', 'cat115', 'cat11', 'cat82', 'cat13', 'cat88', 'cat101', 'cat102', 'cat80', 'cat81']\n",
      "# feautres selected:  113\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "from sklearn.feature_selection     import    f_regression, mutual_info_regression\n",
    "\n",
    "# f_regression\n",
    "##############\n",
    "f_reg_res = {}\n",
    "fval, pval = f_regression(X, Y)\n",
    "for i,c in enumerate(X.columns):\n",
    "  f_reg_res[c] = fval[i]\n",
    "\n",
    "# sort the features according to f_regression scores\n",
    "sorted_res = [[k,v] for k, v in sorted(f_reg_res.items(), key=lambda item: item[1])]\n",
    "sorted(sorted_res, key = lambda x: x[1])\n",
    "\n",
    "# remove features that scored too low\n",
    "high_score_features_F = [x[0] for x in list(filter(lambda x: x[1]>100, sorted_res))]\n",
    "print(\"features with f_regression score > 100\")\n",
    "print(high_score_features_F)\n",
    "\n",
    "\n",
    "\n",
    "# mutual_information\n",
    "####################\n",
    "# sampling a subset of data, as mutual_info calculation is intensive\n",
    "sample = train_data.sample(10000)\n",
    "x = sample.iloc[:,:-1]\n",
    "y = sample.iloc[:,-1]\n",
    "\n",
    "mutinf_res = {}\n",
    "mi = mutual_info_regression(x, y)\n",
    "for i,c in enumerate(X.columns):\n",
    "  mutinf_res[c] = mi[i]\n",
    "\n",
    "# sort the features according to mutual_information scores\n",
    "sorted_res = [[k,v] for k, v in sorted(mutinf_res.items(), key=lambda item: item[1])]\n",
    "sorted(sorted_res, key = lambda x: x[1])\n",
    "\n",
    "# remove features that scored too low\n",
    "high_score_features_MI = [x[0] for x in list(filter(lambda x: x[1]>0.001, sorted_res))]\n",
    "print(\"features with mutual_information score > 100\")\n",
    "print(high_score_features_MI)\n",
    "\n",
    "# get intersection of features which score high on both of these tests\n",
    "# i.e. we are discarding features that did not do well in both the tests\n",
    "common_features_union = list(set(high_score_features_F).union(set(high_score_features_MI)))\n",
    "print(\"# feautres selected: \", common_features_union.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features that would be used:  ['cat105', 'cat19', 'cat28', 'cat99', 'cat4', 'cat113', 'cont7', 'cont12', 'cont3', 'cat78', 'cat39', 'cat58', 'cont6', 'cont4', 'cat7', 'cat81', 'cat9', 'cat110', 'cat109', 'cont13', 'cat116', 'cat20', 'cat52', 'cat67', 'cat2', 'cat115', 'cat10', 'cat51', 'cat86', 'cat112', 'cont11', 'cat71', 'cat11', 'cat12', 'cat13', 'cont2', 'cat17', 'cat40', 'cat98', 'cat77', 'cat29', 'cat61', 'cat90', 'cat114', 'cat33', 'cat26', 'cat3', 'cat6', 'cat111', 'cat83', 'cat37', 'cat75', 'cat106', 'cat91', 'cat66', 'cat30', 'cat41', 'cat25', 'cat24', 'cat47', 'cont14', 'cat16', 'cat15', 'cat23', 'cat36', 'cat95', 'cat76', 'cont5', 'cat100', 'cat49', 'cat57', 'cat87', 'cat94', 'cat54', 'cat45', 'cat44', 'cat88', 'cat43', 'cat59', 'cat50', 'cat92', 'cat65', 'cont8', 'cont9', 'cont10', 'cat42', 'cat38', 'cat101', 'cat93', 'cat74', 'cat80', 'cat14', 'cat82', 'cat32', 'cat84', 'cat8', 'cat79', 'cat18', 'cat72', 'cat102', 'cat68', 'cat34', 'cat5', 'cat53', 'cat73', 'cat89', 'cat104', 'cat48', 'cat108', 'cat46', 'cat103', 'cat85', 'cat1']\n",
      "# features:  113\n"
     ]
    }
   ],
   "source": [
    "print(\"Features that would be used: \", common_features_union)\n",
    "print(\"# features: \", common_features_union.__len__())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, LeakyReLU\n",
    "from keras.preprocessing import text\n",
    "from keras import utils\n",
    "\n",
    "# set hyperparameters for MLP\n",
    "class NN:\n",
    "    def __init__(self):\n",
    "        self.in_shape = common_features_union.__len__()\n",
    "        self.num_layers = 3\n",
    "        self.nodes = [2048,1024, 1]\n",
    "        self.activations = ['relu', 'relu', 'relu']\n",
    "        self.dropouts = [0.2,0.15,0]\n",
    "        self.loss = 'mean_squared_logarithmic_error'\n",
    "        self.optimizer = keras.optimizers.RMSprop(5e-4)\n",
    "\n",
    "\n",
    "\n",
    "def sequential_MLP(nn):\n",
    "    model = Sequential()\n",
    "    for i in range(nn.num_layers):\n",
    "        if i==0: # add input shape if first layer\n",
    "            model.add(Dense(nn.nodes[i], activation=nn.activations[i], input_shape=(nn.in_shape,) ))\n",
    "        else:\n",
    "            model.add(Dense(nn.nodes[i], activation=nn.activations[i]))\n",
    "        if(nn.dropouts[i] != 0): # skip adding dropout if dropout == 0\n",
    "            model.add(Dropout(rate=nn.dropouts[i]))            \n",
    "    model.compile(optimizer=nn.optimizer, loss=nn.loss, metrics=['mae'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 141238 samples, validate on 47080 samples\n",
      "Epoch 1/1\n",
      "141238/141238 [==============================] - 3s 20us/step - loss: 0.9597 - mae: 1724.0690 - val_loss: 0.4067 - val_mae: 1426.7174\n",
      "Train on 141238 samples, validate on 47080 samples\n",
      "Epoch 1/1\n",
      "141238/141238 [==============================] - 2s 14us/step - loss: 0.3454 - mae: 1274.4697 - val_loss: 0.3395 - val_mae: 1268.1921\n",
      "Train on 141238 samples, validate on 47080 samples\n",
      "Epoch 1/1\n",
      "141238/141238 [==============================] - 2s 13us/step - loss: 0.3336 - mae: 1241.8767 - val_loss: 0.3358 - val_mae: 1251.1632\n",
      "Train on 141238 samples, validate on 47080 samples\n",
      "Epoch 1/1\n",
      "141238/141238 [==============================] - 2s 13us/step - loss: 0.3279 - mae: 1227.7994 - val_loss: 0.3204 - val_mae: 1215.1016\n",
      "Train on 141238 samples, validate on 47080 samples\n",
      "Epoch 1/1\n",
      "141238/141238 [==============================] - 2s 13us/step - loss: 0.3240 - mae: 1218.8560 - val_loss: 0.3166 - val_mae: 1206.4996\n",
      "Train on 141238 samples, validate on 47080 samples\n",
      "Epoch 1/1\n",
      "141238/141238 [==============================] - 2s 12us/step - loss: 0.3205 - mae: 1212.3313 - val_loss: 0.3247 - val_mae: 1208.8512\n",
      "Train on 141238 samples, validate on 47080 samples\n",
      "Epoch 1/1\n",
      "141238/141238 [==============================] - 2s 12us/step - loss: 0.3185 - mae: 1208.1692 - val_loss: 0.3251 - val_mae: 1207.2321\n",
      "Train on 141238 samples, validate on 47080 samples\n",
      "Epoch 1/1\n",
      "141238/141238 [==============================] - 2s 12us/step - loss: 0.3154 - mae: 1201.2538 - val_loss: 0.3261 - val_mae: 1206.2754\n",
      "Train on 141238 samples, validate on 47080 samples\n",
      "Epoch 1/1\n",
      "141238/141238 [==============================] - 2s 13us/step - loss: 0.3128 - mae: 1195.6331 - val_loss: 0.3110 - val_mae: 1190.4359\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "nn = NN()\n",
    "model = sequential_MLP(nn)\n",
    "\n",
    "\n",
    "for i in range(45):\n",
    "  if i%5 == 0: verbose=True\n",
    "  else: verbose = False\n",
    "  model.fit(X[common_features_union], Y, epochs=1, batch_size=512, validation_split=0.25, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(test_data[common_features_union])\n",
    "\n",
    "\n",
    "submission = pd.read_csv('/kaggle/input/allstate-claims-severity/sample_submission.csv')\n",
    "submission['loss'] = test_predictions\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
