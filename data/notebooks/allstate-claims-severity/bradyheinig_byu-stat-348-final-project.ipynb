{"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"4.4.0"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5325,"databundleVersionId":88895,"sourceType":"competition"}],"dockerImageVersionId":30749,"isInternetEnabled":true,"language":"r","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Allstate is an insurance company that pays out claims to customers who get into car accidents. Depending on the severity of the accident, the cost of the insurance claim varies. This competition aims to predict claim severity given a variety of claim features\nOur target is ‘loss’, or the dollar amount lost by Allstate due to this claim. There are two types of features in the given data, 116 categorical features and 14 continuous numeric features. Submissions to this competition are evaluated by the mean absolute error (MAE) between the predicted loss and the actual loss. In this notebook, I will be implementing and evaluating several models to try and predict Allstate's claim losses with the highest possible accuracy.\n","metadata":{}},{"cell_type":"markdown","source":"First, the necessary libraries are loaded in.","metadata":{}},{"cell_type":"code","source":"#Load in Libraries\nlibrary(bonsai)\nlibrary(lightgbm)\nlibrary(tidymodels)\nlibrary(embed) \nlibrary(vroom)\nlibrary(tidyverse)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T03:32:58.422039Z","iopub.execute_input":"2024-12-08T03:32:58.424151Z","iopub.status.idle":"2024-12-08T03:33:02.966794Z","shell.execute_reply":"2024-12-08T03:33:02.964927Z"},"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Here, I load in the test and train datasets.","metadata":{}},{"cell_type":"code","source":"train_data <- vroom(\"/kaggle/input/allstate-claims-severity/train.csv\")\ntest_data <- vroom(\"/kaggle/input/allstate-claims-severity/test.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T03:35:17.834750Z","iopub.execute_input":"2024-12-08T03:35:17.868169Z","iopub.status.idle":"2024-12-08T03:35:19.258719Z","shell.execute_reply":"2024-12-08T03:35:19.256898Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The tidymodels package in R allows us to create feature-engineered workflows that can be applied to a variety of models. This package will be used throughout this project. In the training data, there is a uniquely identifying ID column that does not lend us any explainability for modeling purposes, so I changed the data type to reflect its role. I also assigned all of the categorical features in the training set as factors and target-encoded them for use in future modeling. I then normalized all of the numeric features on a scale from 0 to 1.","metadata":{}},{"cell_type":"code","source":"recipe <- recipe(loss~., data=train_data) %>%\n  update_role(id, new_role=\"id\") %>%\n  step_mutate_at(all_nominal_predictors(), fn = as.factor) %>%\n  step_lencode_glm(all_nominal_predictors(), outcome = vars(loss)) %>% \n  step_range(all_numeric_predictors(), min=0, max=1) #scale to [0,1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T03:38:16.497738Z","iopub.execute_input":"2024-12-08T03:38:16.499330Z","iopub.status.idle":"2024-12-08T03:38:16.541374Z","shell.execute_reply":"2024-12-08T03:38:16.539505Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"First, a penalized regression model is fit on the training data. I used a penalty of 5 and a mixture of 0.75, which leans the model more towards lasso regularization. The penalized regression model is then fit to the training data.","metadata":{}},{"cell_type":"code","source":"#Set model and tuning\npreg_model <- linear_reg(penalty=5, mixture=0.75) %>% \n  set_engine(\"glmnet\") \n\npreg_wf <- workflow() %>%\n  add_recipe(recipe) %>%\n  add_model(preg_model) %>%\n  fit(data=train_data)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Once the model has been fit, predictions are made on the test data and the predictions are submitted to measure the out of sample MAE.","metadata":{}},{"cell_type":"code","source":"preg_preds <- predict(preg_wf, new_data=test_data)\n\nkaggle_submission <- preg_preds %>%\n  bind_cols(., test_data) %>% #Bind predictions with test data\n  select(id, .pred) %>% #Just keep datetime and prediction variables\n  rename(loss=.pred)\n\n## Write out the file\nvroom_write(x=kaggle_submission, file=\"./PenalizedRegPreds.csv\", delim=\",\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The MAE for this penalized regression model was 1291.94, which is alright but can definitely be improved. I'll try some more models to see if we can decrease our score.","metadata":{}},{"cell_type":"markdown","source":"Next, a regression tree was fit to the training data. I chose to tune the tree depth, cost complexity, and minimum number of samples until node split hyperparameters. The other hyperparameters were set at their default values.","metadata":{}},{"cell_type":"code","source":"tree_mod <- decision_tree(tree_depth = tune(),\n                        cost_complexity = tune(),\n                        min_n=tune()) %>% \n  set_engine(\"rpart\") %>% \n  set_mode(\"regression\")\n\n\ntree_wf <- workflow() %>%\n  add_recipe(recipe) %>%\n  add_model(tree_mod) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"I then performed K-fold cross-validation to tune the hyperparameters, using 3 folds and 9 different hyperparameter combinations. ","metadata":{}},{"cell_type":"code","source":"grid_of_tuning_params <- grid_regular(tree_depth(),\n                                      cost_complexity(),\n                                      min_n(),\n                                      levels = 3) ## L^2 total tuning possibilities\n## Split data for CV\nfolds <- vfold_cv(train_data, v = 3, repeats=1)\n\n## Run the CV1\nCV_results <- tree_wf %>%\n  tune_grid(resamples=folds,\n            grid=grid_of_tuning_params,\n            metrics=metric_set(mae)) #Or leave metrics NULL","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The model with the best performing hyperparameters relative to the resulting MAE was chosen, and the final model was fit using these hyperparameters to the training data.","metadata":{}},{"cell_type":"code","source":"## Find Best Tuning Parameters13\nbestTune <- CV_results %>%\n  select_best(metric =\"mae\")\n\nfinal_wf <-\n  tree_wf %>%\n  finalize_workflow(bestTune) %>%\n  fit(data=train_data)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Once the final model was fit, it was used to make predictions on the test data and submitted to kaggle to determine the out-of-sample MAE.","metadata":{}},{"cell_type":"code","source":"preds <- predict(final_wf, new_data=test_data)\n\nkaggle_submission <- preds %>%\n  bind_cols(., test_data) %>% #Bind predictions with test data\n  select(id, .pred) %>% #Just keep datetime and prediction variables\n  rename(loss=.pred)\n\n## Write out the file\nvroom_write(x=kaggle_submission, file=\"./TreePreds.csv\", delim=\",\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The MAE for this model was 1312.74, which was slightly worse than the penalized regression, but I believe my score can still be improved.","metadata":{}},{"cell_type":"markdown","source":"Lastly, I chose to fit a boosted tree regression model to the data. There are several packages that can be used to run a boosted tree model, but I have found that Light GBM runs the quickest without decreasing performance. The hyperparameters that will be tuned the depth of the trees, the number of trees, and the learning rate. Other parameters, such as the number of leaves and the minimum number of samples required to split a node, could be tuned, but to avoid overfitting I am setting those at their default values.","metadata":{}},{"cell_type":"code","source":"boost_model <- boost_tree(tree_depth=tune(),\n                          trees=tune(),\n                          learn_rate=tune()) %>%\n  set_engine(\"lightgbm\") %>% \n  set_mode(\"regression\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T03:38:23.366774Z","iopub.execute_input":"2024-12-08T03:38:23.368581Z","iopub.status.idle":"2024-12-08T03:38:23.383442Z","shell.execute_reply":"2024-12-08T03:38:23.381579Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The workflow is set up with the previously defined recipe and the boosted tree model with the specified hyperparameters to be tuned.","metadata":{}},{"cell_type":"code","source":"# Set up the workflow\ngbm_wf <- workflow() %>%\n  add_recipe(recipe) %>%\n  add_model(boost_model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To tune and measure the performance of our model, I performed k-fold cross-validation with 3 folds. For tuning, I created a regular grid with 9 combinations of hyperparameters. The performance of each hyperparameter combination was measured by mean absolute error (MAE), which is the same performance metric used by the competition.","metadata":{}},{"cell_type":"code","source":"cv_folds <- vfold_cv(train_data, v = 3, repeats = 1)\n\ngrid <- grid_regular(tree_depth(),\n                            trees(),\n                            learn_rate(),\n                            levels = 3)\n\ntuned_results <- gbm_wf %>% \n    tune_grid(\n      resamples = cv_folds,\n      grid = grid,\n      metrics = metric_set(mae),\n      control = control_grid()\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T03:38:30.221525Z","iopub.execute_input":"2024-12-08T03:38:30.223225Z","iopub.status.idle":"2024-12-08T03:38:46.534422Z","shell.execute_reply":"2024-12-08T03:38:46.532583Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The hyperparameter combination with the best performance relative to its MAE was selected, and the boosted model was fit to the training data with these parameters.","metadata":{}},{"cell_type":"code","source":"# Get the best hyperparameters\nbest_params <- tuned_results %>% \n  select_best(metric = \"mae\")\n\n# Finalize the workflow with the best parameters\nfinal_workflow <- gbm_wf %>%  \n    finalize_workflow(best_params) %>%  \n    fit(data = train_data)","metadata":{"_uuid":"051d70d956493feee0c6d64651c6a088724dca2a","_execution_state":"idle","trusted":true,"execution":{"iopub.status.busy":"2024-12-06T16:30:12.750803Z","iopub.execute_input":"2024-12-06T16:30:12.787907Z","iopub.status.idle":"2024-12-06T16:30:19.532457Z","shell.execute_reply":"2024-12-06T16:30:19.522574Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now that the highest-performing model has been fit, predictions are made on the test set. Once made, the predictions are bound to the test set, and the predictions are submitted to obtain an out-of-sample MAE. ","metadata":{}},{"cell_type":"code","source":"# Make predictions on the test data\nboosted_preds <- predict(final_workflow, new_data = test_data)\n\nkaggle_submission <- boosted_preds %>%\n  bind_cols(., test_data) %>% #Bind predictions with test data\n  select(id, .pred) %>% #Just keep datetime and prediction variables\n  rename(loss=.pred)\n#rename pred to count (for submission to Kaggle)\nvroom_write(x=kaggle_submission, file=\"./boostedPreds.csv\", delim=\",\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The out-of-sample MAE for the boosted tree model is 1163.85, which made it the best performing out of all of the models I tried. Therefore, this was the model that I chose to use.","metadata":{}}]}