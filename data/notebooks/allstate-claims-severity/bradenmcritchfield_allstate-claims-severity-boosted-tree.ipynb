{"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"4.0.5"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5325,"databundleVersionId":88895,"sourceType":"competition"}],"dockerImageVersionId":30618,"isInternetEnabled":true,"language":"r","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"library(tidyverse)\nlibrary(tidymodels)\nlibrary(vroom)\nlibrary(embed)\nlibrary(bonsai)\nlibrary(lightgbm)\n\ntrain <- vroom(\"/kaggle/input/allstate-claims-severity/train.csv\")\ntest <- vroom(\"/kaggle/input/allstate-claims-severity/test.csv\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-12T18:48:59.861998Z","iopub.execute_input":"2023-12-12T18:48:59.864350Z","iopub.status.idle":"2023-12-12T18:49:08.850255Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build Recipe\nAfter loading the libraries and necessary data, we need to build our recipe. I removed ID, combined categorical variables that were too infrequent, target encoded categorical variables, removed variables that were highly correlated with each other, normalized numeric variables, and removed any variables that had zero variance.","metadata":{}},{"cell_type":"code","source":"my_recipe <- recipe(loss ~ ., data = train) %>%\n  step_rm(id) %>% #ID not predictive\n  step_other(all_nominal_predictors(), threshold = .001) %>% #combine categorical variables that are too small\n  step_lencode_mixed(all_nominal_predictors(), outcome = vars(loss)) %>% #target encode categorical variables\n  step_corr(all_numeric_predictors(), threshold = 0.6) %>% #remove variables with a high correlation with other variables\n  step_normalize(all_numeric_predictors())%>% #normalize variable values\n  step_zv(all_predictors())#remove any predictors with no variance","metadata":{"_uuid":"051d70d956493feee0c6d64651c6a088724dca2a","_execution_state":"idle","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Find Optimal Tuning Parameters\nAfter building our recipe, we need to build our model. A boosted tree has the tuning parameters of tree depth, number of trees, and the learn rate. We will tune each of these parameters using cross validation. The engine is light gbm (you can also use xg boost) and the mode is regression. We are optiizing our tuning parameters using Mean Absolute Error, because that is the metric the competition is measuring models with.","metadata":{}},{"cell_type":"code","source":"boost_model <- boost_tree(tree_depth=tune(),\n                          trees=tune(),\n                          learn_rate=tune()) %>%\nset_engine(\"lightgbm\") %>%\n  set_mode(\"regression\")\n\nBoost_wf <- workflow() %>%\n  add_recipe(my_recipe) %>%\n  add_model(boost_model)\n\n## CV tune, finalize and predict here and save results\n## Grid of values to tune over\ntuning_grid <- grid_regular(tree_depth(),\n                            trees(),\n                            learn_rate(),\n                            levels = 3) ## L^2 total tuning possibilities\n\n## Split data for CV15\n## Split data for CV15\nfolds <- vfold_cv(train, v = 5, repeats=1)\n\n## Run the CV\nCV_results <- Boost_wf %>%\n  tune_grid(resamples=folds,\n            grid=tuning_grid,\n            metrics=metric_set(mae)) #Or leave metrics NULL\n\n#Find the best tuning parameters\nbestTune <- CV_results %>%\n  select_best('mae')\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Make Predictions\nNow that we have the optimal tuning parameters in \"bestTune\", we can feed that into our final workflow to generate predictions.","metadata":{}},{"cell_type":"code","source":"final_wf <- Boost_wf %>%\n  finalize_workflow(bestTune) %>%\n  fit(data=train)\n\nAllstate_preds <- predict(final_wf, new_data=test)\n\n#format submission\nsubmission <- Allstate_preds %>%\n  mutate(id = test$id) %>%\n  mutate(loss = .pred) %>% #transform back to original scale\n  select(2, 3)\n\nvroom_write(submission, \"submission.csv\", delim = \",\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}