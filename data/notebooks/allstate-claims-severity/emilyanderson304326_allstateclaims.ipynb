{"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"4.0.5"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5325,"databundleVersionId":88895,"sourceType":"competition"}],"dockerImageVersionId":30618,"isInternetEnabled":true,"language":"r","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#load libraries\nlibrary(embed)\nlibrary(themis)\nlibrary(vroom)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(DataExplorer)\nlibrary(bonsai)\nlibrary(lightgbm)","metadata":{"_uuid":"051d70d956493feee0c6d64651c6a088724dca2a","_execution_state":"idle","execution":{"iopub.status.busy":"2023-12-08T18:53:45.263905Z","iopub.execute_input":"2023-12-08T18:53:45.266499Z","iopub.status.idle":"2023-12-08T18:53:45.296140Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#read in data\ntrain <- vroom(\"/kaggle/input/allstate-claims-severity/train.csv\")\ntest <- vroom(\"/kaggle/input/allstate-claims-severity/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-12-08T18:53:45.301143Z","iopub.execute_input":"2023-12-08T18:53:45.302999Z","iopub.status.idle":"2023-12-08T18:53:46.278066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Exploratory Data Analysis**","metadata":{}},{"cell_type":"code","source":"#eda\nplot_correlation(train, type = \"continuous\") #high correlation off the diagonal\n\nhist(train$loss) #right skewed\n\ntrain_ex <- train %>%\nmutate(loss_ex = log(loss))\nhist(train_ex$loss_ex) #log transform makes response more normal\n\ntrain_ex_2 <- train %>%\nmutate(loss_ex_2 = (loss+1)^.25) #transformation as suggested by other winners\nhist(train_ex_2$loss_ex_2)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-08T18:53:46.281056Z","iopub.execute_input":"2023-12-08T18:53:46.282574Z","iopub.status.idle":"2023-12-08T18:53:52.469298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Exploratory analysis shows that some variables have notably high correlation with one another. There are also a lot of variables available to us with these data. This signals that a feature engineering step may be needed to eliminate some variables with high correlation. \n\nFurther exploration shows a right-skewed response. A log transformation makes sense in this scenario with the large numbers. One suggestion from other winners of this competition was to do the following transformation on the response:(loss + 1)^.25. This also results in more normal looking data, however it is still slightly right skewed. After running some models with both the log transformation and the (loss+1)^.25 transformation, I have selected the latter to use in my final model. I will refer to this transformation as the \"winner\" transformation.","metadata":{}},{"cell_type":"markdown","source":"**Transform Response and Create Recipe**","metadata":{}},{"cell_type":"code","source":"train <- train %>%\n    mutate(loss = (loss+1)^.25)\n\nallstate_recipe <- recipe(loss ~ ., train) %>% #w/ winner transformation\n  step_lencode_mixed(all_nominal_predictors(), outcome = vars(loss))","metadata":{"execution":{"iopub.status.busy":"2023-12-08T18:53:52.472325Z","iopub.execute_input":"2023-12-08T18:53:52.473923Z","iopub.status.idle":"2023-12-08T18:53:52.663028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above recipe implements the winner transformation as well uses target encoding on all of the categorical predictors.","metadata":{}},{"cell_type":"markdown","source":"**Creating the Model**","metadata":{}},{"cell_type":"code","source":"#create model\nboost_model <- boost_tree(tree_depth=tune(),\n                          trees=tune(),\n                          learn_rate=tune(),\n                         mode = \"regression\") %>%\n              set_engine(\"lightgbm\") #or \"xgboost\" but lightgbm is faster\n\n#set workflow\nboost_wf <- workflow() %>%\n  add_recipe(allstate_recipe) %>%\n  add_model(boost_model)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T03:24:24.330386Z","iopub.execute_input":"2023-12-13T03:24:24.334086Z","iopub.status.idle":"2023-12-13T03:24:24.524549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is a boosted model using the lightgbm engine. Tree depth, the number of trees, and learning rate are parameters that will be tuned over. Since the response is numeric, the mode is regression. The model and recipe are then added to a workflow.","metadata":{}},{"cell_type":"markdown","source":"**Cross Validation**","metadata":{}},{"cell_type":"code","source":"#set up tuning grid\nboost_tuneGrid <- grid_regular(tree_depth(), trees(), learn_rate(), levels = 3)\n\n#set up cv\nboost_folds <- vfold_cv(train, v = 5, repeats = 1)\n\nCV_boost_results <- boost_wf %>%\n  tune_grid(resamples = boost_folds,\n            grid = boost_tuneGrid,\n            metrics = metric_set(mae))\n\n#find best tuning parameters\nbestTune_boost <- CV_boost_results %>%\n  select_best(\"mae\") #mean absolute error used by this particular Kaggle comp","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In order to find the optimal levels to set for tree depth, tree number, and learning rate, we must perform a cross validation. This will split the data into 5 subsections and use all combinations of those three parameters to see which combination results in the lowest mean absolute error. Mean absolute error was selected as the metric to determine how well the model did with predictions becuase this is what the Kaggle competition is using to score submissions.","metadata":{}},{"cell_type":"markdown","source":"**Fit the Model and Make Predictions**","metadata":{}},{"cell_type":"code","source":"#finalize workflow and fit it\nfinal_boost_wf <- boost_wf %>%\n  finalize_workflow(bestTune_boost) %>%\n  fit(train)\n\n#make predictions\npred_boost <- predict(final_boost_wf, new_data = test) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we implement the model with optmial tuning parameters to fit on the traning data. After the model is fit on the traning data, we then make predictions on the test data.  ","metadata":{}},{"cell_type":"markdown","source":"**Format for Kaggle**","metadata":{}},{"cell_type":"code","source":"#format for Kaggle\nboost_final <- pred_boost %>%\n  bind_cols(test) %>%\n  select(id,.pred) %>%\n  rename(loss = .pred) %>%\n  mutate(loss = loss^4-1)\n\nwrite_csv(boost_final, \"boostSubmission.csv\")\n\n##SCORE: 1,123.17","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The boosted model with the winner transformation and target encoding resulted in a score of 1,123.17. This is the 1,462nd best score of 3,048 competitors.","metadata":{}}]}