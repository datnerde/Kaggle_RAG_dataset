{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6c93c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from data_manager import DataManager\n",
    "import json\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b10542",
   "metadata": {},
   "source": [
    "# Create Compeition Collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7954ffe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported competition: titanic\n",
      "\n",
      "Summary: Successfully imported 1 of 1 competitions\n"
     ]
    }
   ],
   "source": [
    "def import_competitions_from_json(file_path: str):\n",
    "    \"\"\"Import competitions from JSON file using the new class structure\"\"\"\n",
    "    \n",
    "    # Initialize data manager\n",
    "    with DataManager() as dm:\n",
    "        # Load competitions data\n",
    "        with open(file_path, 'r') as file:\n",
    "            competitions_data = json.load(file)\n",
    "\n",
    "        success_count = 0\n",
    "        for competition_id, competition_info in competitions_data.items():\n",
    "            # Prepare competition record\n",
    "            competition_record = {\n",
    "                'competition_id': competition_id,\n",
    "                'title': competition_info.get('Title', competition_id.title()),\n",
    "                'description': competition_info.get('Description', ''),\n",
    "                'evaluation': competition_info.get('Evaluation', ''),\n",
    "                'competition_host': competition_info.get('Competition Host', []),\n",
    "                'price_award': competition_info.get('Prizes & Awards', []),\n",
    "                'entrants': competition_info.get('Entrants', 0),\n",
    "                'participants': competition_info.get('Participants', 0),\n",
    "                'teams': competition_info.get('Teams', 0),\n",
    "                'submissions': competition_info.get('Submissions', 0),\n",
    "                'tags': competition_info.get('Tags', []),\n",
    "                'competition_url': competition_info.get('competition_url', ''),\n",
    "                'last_updated': datetime.datetime.now()  # Automatically set\n",
    "            }\n",
    "            \n",
    "            # Add data description if available\n",
    "            if 'data' in competition_info:\n",
    "                data_info = competition_info['data']\n",
    "                competition_record.update({\n",
    "                    'data_description': data_info.get('Description', ''),\n",
    "                    'data_files_num': data_info.get('Files', ''),\n",
    "                    'data_size': data_info.get('Size', ''),\n",
    "                    'data_type': data_info.get('Type', '')\n",
    "                })\n",
    "            \n",
    "            # Use the CompetitionManager to create/update\n",
    "            try:\n",
    "                result = dm.competitions.create_or_update(competition_record)\n",
    "                if result:\n",
    "                    success_count += 1\n",
    "                    print(f\"Successfully imported competition: {competition_id}\")\n",
    "                else:\n",
    "                    print(f\"Failed to import competition: {competition_id}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error importing competition {competition_id}: {str(e)}\")\n",
    "\n",
    "        print(f\"\\nSummary: Successfully imported {success_count} of {len(competitions_data)} competitions\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    import_competitions_from_json(\n",
    "        '/Users/zhongming/Local Docs/Github/Kaggle_RAG_dataset/data/competitions_metadata.json'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa92524",
   "metadata": {},
   "source": [
    "# Create dataset collection for train and test set of competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eef0b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing competition: titanic\n",
      "Found 3 CSV files\n",
      "Importing test.csv as test dataset...\n",
      "Successfully imported test.csv\n",
      "Importing train.csv as train dataset...\n",
      "Successfully imported train.csv\n",
      "Importing gender_submission.csv as unknown dataset...\n",
      "Successfully imported gender_submission.csv\n",
      "Imported 3 of 3 datasets for competition titanic\n",
      "\n",
      "Final Summary:\n",
      "Total datasets found: 3\n",
      "Total datasets imported: 3\n",
      "Success rate: 100.00%\n"
     ]
    }
   ],
   "source": [
    "def import_datasets_from_directory(base_dir: str):\n",
    "    \"\"\"\n",
    "    Import datasets from CSV files in competition directories using the new class structure\n",
    "    \n",
    "    Args:\n",
    "        base_dir: Base directory containing competition folders with CSV files\n",
    "    \"\"\"\n",
    "    # Initialize data manager\n",
    "    with DataManager() as dm:\n",
    "        # Get all competition directories\n",
    "        competition_dirs = [d for d in os.listdir(base_dir) \n",
    "                          if os.path.isdir(os.path.join(base_dir, d))]\n",
    "\n",
    "        total_datasets = 0\n",
    "        total_imported = 0\n",
    "\n",
    "        # Process each competition directory\n",
    "        for competition_id in competition_dirs:\n",
    "            competition_dir = os.path.join(base_dir, competition_id)\n",
    "            \n",
    "            # Check if competition exists using CompetitionManager\n",
    "            if not dm.competitions.exists(competition_id):\n",
    "                print(f\"Competition {competition_id} does not exist in the database, skipping...\")\n",
    "                continue\n",
    "            \n",
    "            # Find all CSV files in the competition directory\n",
    "            csv_pattern = os.path.join(competition_dir, \"*.csv\")\n",
    "            csv_files = glob.glob(csv_pattern)\n",
    "            \n",
    "            if not csv_files:\n",
    "                print(f\"No CSV files found for competition {competition_id}\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\nProcessing competition: {competition_id}\")\n",
    "            print(f\"Found {len(csv_files)} CSV files\")\n",
    "            \n",
    "            # Import each CSV file\n",
    "            imported_count = 0\n",
    "            for csv_file in csv_files:\n",
    "                file_name = os.path.basename(csv_file)\n",
    "                base_name = os.path.splitext(file_name)[0]\n",
    "                \n",
    "                # Determine dataset type based on filename\n",
    "                dataset_type = 'unknown'\n",
    "                if 'train' in base_name.lower():\n",
    "                    dataset_type = 'train'\n",
    "                elif 'test' in base_name.lower():\n",
    "                    dataset_type = 'test'\n",
    "                elif 'submission' in base_name.lower() or 'submission' in base_name.lower():\n",
    "                    dataset_type = 'sample_submission'\n",
    "                \n",
    "                print(f\"Importing {file_name} as {dataset_type} dataset...\")\n",
    "                \n",
    "                # Use DatasetManager to import\n",
    "                try:\n",
    "                    if dm.datasets.import_csv(csv_file, competition_id, dataset_type):\n",
    "                        imported_count += 1\n",
    "                        print(f\"Successfully imported {file_name}\")\n",
    "                    else:\n",
    "                        print(f\"Failed to import {file_name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error importing {file_name}: {str(e)}\")\n",
    "            \n",
    "            print(f\"Imported {imported_count} of {len(csv_files)} datasets for competition {competition_id}\")\n",
    "            \n",
    "            total_datasets += len(csv_files)\n",
    "            total_imported += imported_count\n",
    "\n",
    "        print(f\"\\nFinal Summary:\")\n",
    "        print(f\"Total datasets found: {total_datasets}\")\n",
    "        print(f\"Total datasets imported: {total_imported}\")\n",
    "        print(f\"Success rate: {(total_imported/total_datasets)*100:.2f}%\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    base_directory = '/Users/zhongming/Local Docs/Github/Kaggle_RAG_dataset/data/dataset'\n",
    "    import_datasets_from_directory(base_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ac4d75",
   "metadata": {},
   "source": [
    "# Create notebook collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9026fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing competition: ashrae-energy-prediction\n",
      "✓ Imported notebook: vladimirsydor_add-leak\n",
      "✓ Imported notebook: yunishi0716_best-weight-searching3\n",
      "✓ Imported notebook: aleksthegreat_public-blend\n",
      "✓ Imported notebook: yamsam_ashrae-leak-validation-and-more\n",
      "✓ Imported notebook: wuliaokaola_ashrae-maybe-this-can-make-public-lb-some-useful\n",
      "✓ Imported notebook: vladimirsydor_bland-lgbm-on-leaks\n",
      "✓ Imported notebook: vladimirsydor_bland-by-leak\n",
      "✓ Imported notebook: rohanrao_ashrae-divide-and-conquer\n",
      "✓ Imported notebook: teeyee314_best-single-half-half-lgbm-1-07\n",
      "✓ Imported notebook: vladimirsydor_bland-lgbm-folds\n",
      "✓ Imported notebook: mimoudata_ashrae-2-lightgbm-without-leak-data\n",
      "✓ Imported notebook: aitude_ashrae-kfold-lightgbm-without-leak-1-08\n",
      "✓ Imported notebook: purist1024_ashrae-simple-data-cleanup-lb-1-08-no-leaks\n",
      "✓ Imported notebook: ragnar123_another-1-08-lb-no-leak\n",
      "✓ Imported notebook: mimoudata_ashrae-lightgbm-without-leak\n",
      "✓ Imported notebook: yunishi0716_k-folds-model\n",
      "✓ Imported notebook: hmendonca_4-ashrae-blended\n",
      "✓ Imported notebook: grapestone5321_ashrae-stacking-method\n",
      "✓ Imported notebook: mimoudata_ashrae-lightgbm-without-leak-data\n",
      "✓ Imported notebook: iwatatakuya_ashrae-kfold-lightgbm-without-building-id\n",
      "✓ Imported notebook: remisharoon_ashrae-gep-iii-rms-nb\n",
      "Notebook file not found: /Users/zhongming/Local Docs/Github/Kaggle_RAG_dataset/data/notebooks/ashrae-energy-prediction/kailex_ac-dc.ipynb\n",
      "✓ Imported notebook: rohanrao_ashrae-half-and-half\n",
      "✓ Imported notebook: litemort_implicit-merge-operation-by-litemort\n",
      "✓ Imported notebook: kulkarnivishwanath_ashrae-great-energy-predictor-iii-eda-model\n",
      "✓ Imported notebook: starl1ght_ashrae-stacked-regression-lasso-ridge-lgbm\n",
      "✓ Imported notebook: hiteshsom_ashrae-3-lightgbm\n",
      "✓ Imported notebook: nz0722_aligned-timestamp-lgbm-by-meter-type\n",
      "✓ Imported notebook: yunishi0716_3-folds-by-each-meter-type\n",
      "✓ Imported notebook: kaushal2896_ashrae-eda-fe-lightgbm-1-12\n",
      "✓ Imported notebook: viswajithkn_great-energy-prediction\n",
      "Notebook file not found: /Users/zhongming/Local Docs/Github/Kaggle_RAG_dataset/data/notebooks/ashrae-energy-prediction/wittmannf_keras-embedding-read-k-folds.ipynb\n",
      "✓ Imported notebook: corochann_ashrae-training-lgbm-by-meter-type\n",
      "✓ Imported notebook: amaity0_ashrae-fifth-try\n",
      "✓ Imported notebook: isaienkov_keras-nn-with-embeddings-for-cat-features-1-15\n",
      "✓ Imported notebook: patelatharva_prediction\n",
      "✓ Imported notebook: zeynepkurban_ashrae-2\n",
      "✓ Imported notebook: enigola_ashrae-ml-hw6-lgbm\n",
      "✓ Imported notebook: clementut_kernel4e51c0227f\n",
      "✓ Imported notebook: morituri_lgbm-baseline\n",
      "✓ Imported notebook: shukla84manish_energy-consumption\n",
      "✓ Imported notebook: madisj_kernel3377148266\n",
      "✓ Imported notebook: darisdzakwanhoesien2_ashrae-great-energy-predictor-iii\n",
      "✓ Imported notebook: kimtaegwan_what-s-your-cv-method\n",
      "✓ Imported notebook: vladimirsydor_randomforestbaseline\n",
      "✓ Imported notebook: mayer79_ashrae-lgb-starter-for-r\n",
      "✓ Imported notebook: jiaofenx_ashrae-great-energy-predictor-iii\n",
      "✓ Imported notebook: gouherdanishiitkgp_ashrae-basic-eda-and-feature-engineering\n",
      "✓ Imported notebook: yshiml_ashrae-simple-lgbm-optuna-baseline\n",
      "✓ Imported notebook: michelezoccali_ashrae-with-fast-ai-part-3\n",
      "Imported 48 of 48 notebooks for ashrae-energy-prediction\n",
      "\n",
      "Processing competition: allstate-claims-severity\n",
      "✓ Imported notebook: cuijamm_allstate-claims-severity-score-1113-12994\n",
      "Notebook file not found: /Users/zhongming/Local Docs/Github/Kaggle_RAG_dataset/data/notebooks/allstate-claims-severity/nitink12_prog-pyth2.ipynb\n",
      "Notebook file not found: /Users/zhongming/Local Docs/Github/Kaggle_RAG_dataset/data/notebooks/allstate-claims-severity/vishallakha_all-trump-state.ipynb\n",
      "Notebook file not found: /Users/zhongming/Local Docs/Github/Kaggle_RAG_dataset/data/notebooks/allstate-claims-severity/casalicchio_tuning-the-parameter-of-a-custom-objective-1120.ipynb\n",
      "Notebook file not found: /Users/zhongming/Local Docs/Github/Kaggle_RAG_dataset/data/notebooks/allstate-claims-severity/aliajouz_singel-model-lb-1117.ipynb\n",
      "✓ Imported notebook: deepdreamx_lgbm-only-featureinteraction-selected\n",
      "Notebook file not found: /Users/zhongming/Local Docs/Github/Kaggle_RAG_dataset/data/notebooks/allstate-claims-severity/xingobar_simple-xgboost-1120-769.ipynb\n",
      "Notebook file not found: /Users/zhongming/Local Docs/Github/Kaggle_RAG_dataset/data/notebooks/allstate-claims-severity/doanducqui_msubx.ipynb\n",
      "✓ Imported notebook: emilyanderson304326_allstateclaims\n",
      "Notebook file not found: /Users/zhongming/Local Docs/Github/Kaggle_RAG_dataset/data/notebooks/allstate-claims-severity/aliajouz_xgb-model.ipynb\n",
      "Notebook file not found: /Users/zhongming/Local Docs/Github/Kaggle_RAG_dataset/data/notebooks/allstate-claims-severity/nitink12_prog-python.ipynb\n",
      "✓ Imported notebook: tushvjti_eda-allstate\n",
      "Notebook file not found: /Users/zhongming/Local Docs/Github/Kaggle_RAG_dataset/data/notebooks/allstate-claims-severity/tobikaggle_h2o-dnn-averaging-in-r.ipynb\n",
      "✓ Imported notebook: harshitt21_allstate-claims-severity-eda-and-baseline\n",
      "✓ Imported notebook: aakash2121995_new-ensemble\n",
      "✓ Imported notebook: aakash2121995_notebookccc577aa6b\n",
      "✓ Imported notebook: venkateshprabhug_severity-of-insurance-claim\n",
      "Notebook file not found: /Users/zhongming/Local Docs/Github/Kaggle_RAG_dataset/data/notebooks/allstate-claims-severity/vecxoz_vecstack-demo.ipynb\n",
      "Notebook file not found: /Users/zhongming/Local Docs/Github/Kaggle_RAG_dataset/data/notebooks/allstate-claims-severity/andreylarionov_simple-gradientboostingregressor-lb-1139.ipynb\n",
      "Notebook file not found: /Users/zhongming/Local Docs/Github/Kaggle_RAG_dataset/data/notebooks/allstate-claims-severity/nirupamkar_xgb-run1.ipynb\n",
      "✓ Imported notebook: raviprakash438_allstate-claims-severity\n",
      "Notebook file not found: /Users/zhongming/Local Docs/Github/Kaggle_RAG_dataset/data/notebooks/allstate-claims-severity/akki0206_xgboost-submission-1.ipynb\n",
      "Notebook file not found: /Users/zhongming/Local Docs/Github/Kaggle_RAG_dataset/data/notebooks/allstate-claims-severity/hbhargava2_xgboost-parameter-tuned-feature-engineering.ipynb\n",
      "✓ Imported notebook: christianrorholtmoe_all-state-claim-severity-neural-net\n",
      "✓ Imported notebook: bradyheinig_byu-stat-348-final-project\n",
      "Notebook file not found: /Users/zhongming/Local Docs/Github/Kaggle_RAG_dataset/data/notebooks/allstate-claims-severity/praveenhegde_xgboost-starter.ipynb\n",
      "Notebook file not found: /Users/zhongming/Local Docs/Github/Kaggle_RAG_dataset/data/notebooks/allstate-claims-severity/rrkc00_xgboost.ipynb\n",
      "✓ Imported notebook: nightshade7_allstate-severity-test\n",
      "✓ Imported notebook: summershan_allstate-car-claims-severity\n",
      "✓ Imported notebook: rahulpawade_allstate-claims-severity-xgboost-regression\n",
      "✓ Imported notebook: bradenmcritchfield_allstate-claims-severity-boosted-tree\n",
      "✓ Imported notebook: nathanchantry_acs-boosted-trees\n",
      "Notebook file not found: /Users/zhongming/Local Docs/Github/Kaggle_RAG_dataset/data/notebooks/allstate-claims-severity/kevinpalm_bumbling-around-in-tensorflow.ipynb\n",
      "✓ Imported notebook: floser_five-minute-model\n",
      "✓ Imported notebook: julienpantz_regression\n",
      "Notebook file not found: /Users/zhongming/Local Docs/Github/Kaggle_RAG_dataset/data/notebooks/allstate-claims-severity/mountaindata_allstate-randomforest.ipynb\n",
      "Notebook file not found: /Users/zhongming/Local Docs/Github/Kaggle_RAG_dataset/data/notebooks/allstate-claims-severity/victorwang_rf-model.ipynb\n",
      "Notebook file not found: /Users/zhongming/Local Docs/Github/Kaggle_RAG_dataset/data/notebooks/allstate-claims-severity/victorwang_rf-model-trial2.ipynb\n",
      "✓ Imported notebook: zachsabey_allstate-claims-rf\n",
      "✓ Imported notebook: natercox_acs-random-forest\n",
      "✓ Imported notebook: alazark_allstateclaims\n",
      "✓ Imported notebook: wjnkerst_notebookc186b1607b\n",
      "Notebook file not found: /Users/zhongming/Local Docs/Github/Kaggle_RAG_dataset/data/notebooks/allstate-claims-severity/samuelhaberthuer_how-about-a-linear-model.ipynb\n",
      "Notebook file not found: /Users/zhongming/Local Docs/Github/Kaggle_RAG_dataset/data/notebooks/allstate-claims-severity/andrecn_allstate-challenge-boosting-gbm.ipynb\n",
      "✓ Imported notebook: victoriarigby_allstate-linear\n",
      "✓ Imported notebook: arpytanshu_allstate-claims-severity-1260-mae\n",
      "Notebook file not found: /Users/zhongming/Local Docs/Github/Kaggle_RAG_dataset/data/notebooks/allstate-claims-severity/xingobar_keras-starer.ipynb\n",
      "Notebook file not found: /Users/zhongming/Local Docs/Github/Kaggle_RAG_dataset/data/notebooks/allstate-claims-severity/victorwang_rd-model.ipynb\n",
      "Imported 24 of 24 notebooks for allstate-claims-severity\n",
      "\n",
      "Processing competition: equity-post-HCT-survival-predictions\n",
      "✓ Imported notebook: mohammadmehdizare_notebooke9568e0edb\n",
      "✓ Imported notebook: samarthpujari_cibmtr-competition\n",
      "✓ Imported notebook: youssefelzahar_cibmtr-survival-forest-c-index-test-data-0-806\n",
      "✓ Imported notebook: vrushabhbidari_predict-1\n",
      "✓ Imported notebook: brianedwards_hct-loo-2\n",
      "✓ Imported notebook: akelsayed_ak-prediction-8\n",
      "✓ Imported notebook: ahmedsamir1598_cibmtr-2ndnote\n",
      "✓ Imported notebook: pedromaiorano_teste\n",
      "✓ Imported notebook: chaki18081999_eda-in-detail-and-random-forest-baseline\n",
      "✓ Imported notebook: yashsahu02_1-cibmtr-equity-in-post-hct-survival-predictions\n",
      "✓ Imported notebook: jaehun123_allogeneic-hct-survival-prediction\n",
      "✓ Imported notebook: trantraa_xgb-with-cross-validation\n",
      "✓ Imported notebook: abhinov037_cibmtr\n",
      "✓ Imported notebook: sheikhomerkashmiri_testing\n",
      "✓ Imported notebook: rohitdileep_lightgbm-kaplanmierfitter\n",
      "✓ Imported notebook: lordxerxes_benchmark-model\n",
      "✓ Imported notebook: claramagdyghaly_cibmttr\n",
      "✓ Imported notebook: shrishh_cibmtr-catboost\n",
      "✓ Imported notebook: muhammadimran112233_cibmtr-baseline-code\n",
      "✓ Imported notebook: inabower_cibmtr-submission-test\n",
      "✓ Imported notebook: sabyrbazarymbetov_cibmtr-baseline\n",
      "✓ Imported notebook: mhkamangoviii_lightgbm-catboost\n",
      "✓ Imported notebook: alicsahmed_cibmtr-machine-learning\n",
      "✓ Imported notebook: noorizzatnassar_noor-nassar\n",
      "✓ Imported notebook: garrickchinnis_equity-in-post-hct-survival-predictions\n",
      "✓ Imported notebook: polygot13_hct-medical-analysis\n",
      "✓ Imported notebook: akelsayed_ak-1-equity-in-post-hct-survival-predictions\n",
      "✓ Imported notebook: nyeinchansoe_cibmtr-xgboost-regression-0-558\n",
      "✓ Imported notebook: livaikira_cibmtr-equity-in-post-hct-survival-predictions\n",
      "✓ Imported notebook: jainilspatel_cibmtr-research-easy\n",
      "✓ Imported notebook: youssefelzahar_cibmtr-random-forest\n",
      "✓ Imported notebook: nabinoli2004_cibmtr\n",
      "✓ Imported notebook: gaganbajwaa_cimbtr-ensemble-kfold\n",
      "✓ Imported notebook: udaken10_classification-regression\n",
      "✓ Imported notebook: jonatanf_cibmtr-neural-network-jonatanf\n",
      "✓ Imported notebook: tomeverson_notebook87e39b37ea\n",
      "✓ Imported notebook: dianalionel_post-hct-survival-prediction\n",
      "✓ Imported notebook: rukmaltharaka_survival-prediction\n",
      "✓ Imported notebook: shresthajeevan_survival-analysis\n",
      "✓ Imported notebook: sheershsrivas_survival-analysis-with-nn\n",
      "✓ Imported notebook: steve179_submission-csv\n",
      "✓ Imported notebook: omarsalah123_notebookeb25fbb1d2\n",
      "✓ Imported notebook: javierojeda71_cibmtr-equity-in-post-hct-survival-predictions\n",
      "✓ Imported notebook: garvio282003_submission-nn-deepsurv-race-mlip26\n",
      "✓ Imported notebook: wjones3668_cibmtr-equity-in-post-hct-survival-predictions\n",
      "✓ Imported notebook: forgetish_cibmtr-simple-eda-and-data-cleaning\n",
      "✓ Imported notebook: nikitalemon_cibmtr\n",
      "✓ Imported notebook: codewithab_testing-hct-survival\n",
      "✓ Imported notebook: tianlulee_xgboost-solution-starter\n",
      "✓ Imported notebook: benjenkins96_understanding-survival-analysis\n",
      "Imported 50 of 50 notebooks for equity-post-HCT-survival-predictions\n",
      "\n",
      "Final Summary:\n",
      "Total notebooks processed: 122\n",
      "Total notebooks imported: 122\n",
      "Success rate: 100.00%\n"
     ]
    }
   ],
   "source": [
    "def import_notebooks_from_directory(base_dir: str):\n",
    "    \"\"\"\n",
    "    Import notebooks from competition directories using the new class structure\n",
    "    \n",
    "    Args:\n",
    "        base_dir: Base directory containing competition folders with notebook files\n",
    "    \"\"\"\n",
    "    # Initialize data manager\n",
    "    with DataManager() as dm:\n",
    "        # Get all competition directories\n",
    "        competition_dirs = [d for d in os.listdir(base_dir) \n",
    "                          if os.path.isdir(os.path.join(base_dir, d))]\n",
    "\n",
    "        total_notebooks = 0\n",
    "        total_imported = 0\n",
    "\n",
    "        # Process each competition directory\n",
    "        for competition_id in competition_dirs:\n",
    "            notebook_folder_path = os.path.join(base_dir, competition_id)\n",
    "            \n",
    "            # Check if competition exists using CompetitionManager\n",
    "            if not dm.competitions.exists(competition_id):\n",
    "                print(f\"Competition {competition_id} does not exist in the database, skipping...\")\n",
    "                continue\n",
    "            \n",
    "            # Define paths for notebooks and metadata\n",
    "            notebooks_dir = notebook_folder_path\n",
    "            metadata_dir = os.path.join(notebook_folder_path, \"metadata\")\n",
    "            metadata_file = os.path.join(metadata_dir, \"all_notebooks_metadata.json\")\n",
    "            \n",
    "            # Check if required directories exist\n",
    "            if not os.path.exists(notebooks_dir):\n",
    "                print(f\"No notebooks directory found for competition {competition_id}\")\n",
    "                continue\n",
    "                \n",
    "            if not os.path.exists(metadata_file):\n",
    "                print(f\"No metadata file found for competition {competition_id}\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\nProcessing competition: {competition_id}\")\n",
    "            \n",
    "            # Load metadata\n",
    "            try:\n",
    "                with open(metadata_file, 'r') as f:\n",
    "                    metadata_dict = json.load(f)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading metadata for {competition_id}: {str(e)}\")\n",
    "                continue\n",
    "            \n",
    "            success_count = 0\n",
    "            processed_count = 0\n",
    "            \n",
    "            # Process each notebook in metadata\n",
    "            for url, metadata in metadata_dict.items():\n",
    "                notebook_name = metadata.get('notebook_name')\n",
    "                if not notebook_name:\n",
    "                    print(f\"Skipping entry with missing notebook_name: {url}\")\n",
    "                    continue\n",
    "\n",
    "                # Construct notebook file path\n",
    "                notebook_file = os.path.join(notebooks_dir, f\"{notebook_name}.ipynb\")\n",
    "                \n",
    "                if not os.path.isfile(notebook_file):\n",
    "                    print(f\"Notebook file not found: {notebook_file}\")\n",
    "                    continue\n",
    "                \n",
    "                # Add URL back to metadata\n",
    "                metadata['url'] = url\n",
    "                \n",
    "                # Import notebook using NotebookManager\n",
    "                try:\n",
    "                    if dm.notebooks.import_from_file(notebook_file, competition_id, metadata):\n",
    "                        success_count += 1\n",
    "                        print(f\"✓ Imported notebook: {notebook_name}\")\n",
    "                    else:\n",
    "                        print(f\"✗ Failed to import notebook: {notebook_name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error importing notebook {notebook_name}: {str(e)}\")\n",
    "                \n",
    "                processed_count += 1\n",
    "            \n",
    "            print(f\"Imported {success_count} of {processed_count} notebooks for {competition_id}\")\n",
    "            \n",
    "            total_notebooks += processed_count\n",
    "            total_imported += success_count\n",
    "\n",
    "        # Print final summary\n",
    "        print(f\"\\nFinal Summary:\")\n",
    "        print(f\"Total notebooks processed: {total_notebooks}\")\n",
    "        print(f\"Total notebooks imported: {total_imported}\")\n",
    "        print(f\"Success rate: {(total_imported/total_notebooks)*100:.2f}%\" if total_notebooks > 0 else \"No notebooks processed\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    notebooks_base_dir = '/Users/zhongming/Local Docs/Github/Kaggle_RAG_dataset/data/notebooks'\n",
    "    import_notebooks_from_directory(notebooks_base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c9adf1",
   "metadata": {},
   "source": [
    "# Create User Profile Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8843515",
   "metadata": {},
   "outputs": [],
   "source": [
    "with DataManager() as dm:\n",
    "    # Create a user\n",
    "    user_id = dm.users.create(\n",
    "        username='data_scientist',\n",
    "        email='ds@example.com',\n",
    "        experience_level='intermediate'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de354b3",
   "metadata": {},
   "source": [
    "# Create History Tracking Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6309c032",
   "metadata": {},
   "outputs": [],
   "source": [
    "with DataManager() as dm:\n",
    "    # Set active competition\n",
    "    dm.users.set_active_competition(user_id, 'titanic')\n",
    "    dm.history.initialize_history(user_id, 'titanic')\n",
    "\n",
    "    # Log some interactions\n",
    "    dm.history.log_interaction(user_id, \"Starting Titanic analysis\", is_user=True)\n",
    "    dm.history.log_interaction(user_id, \"Here's some initial guidance\", is_user=False)\n",
    "\n",
    "    # Log a submission\n",
    "    dm.history.log_submission(user_id, {\n",
    "        'notebook_id': 'initial_analysis',\n",
    "        'score': 0.85,\n",
    "        'notes': 'First submission with basic model'\n",
    "    })\n",
    "    \n",
    "    # advance the user\n",
    "    dm.history.advance_user(user_id, 'titanic', 'advanced')\n",
    "    # Log a new interaction\n",
    "    dm.history.log_interaction(user_id, \"Advanced analysis on Titanic dataset\", is_user=True)\n",
    "    # Log a new submission\n",
    "    dm.history.log_submission(user_id, {\n",
    "        'notebook_id': 'advanced_analysis',\n",
    "        'score': 0.90,\n",
    "        'notes': 'Improved model with feature engineering'\n",
    "    })\n",
    "    #complete the history\n",
    "    dm.history.complete_competition(user_id, 'titanic')\n",
    "    \n",
    "    # exit the competition\n",
    "    dm.users.clear_active_competition(user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c8d2b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ee62b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac5aa0e7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8dafe7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aa282d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogenstudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
