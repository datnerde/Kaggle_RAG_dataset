{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4604a67d",
   "metadata": {},
   "source": [
    "# Import mongo db pakg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fff0eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import uuid\n",
    "import datetime\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209697f7",
   "metadata": {},
   "source": [
    "# Class to build database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0e07ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataManager:\n",
    "    def __init__(self, connection_string='mongodb://localhost:27017/', db_name='kaggle_platform'):\n",
    "        \"\"\"Initialize connection to MongoDB.\"\"\"\n",
    "        self.client = MongoClient(connection_string)\n",
    "        self.db = self.client[db_name]\n",
    "        \n",
    "        # Initialize collections\n",
    "        self.competitions = self.db['competitions']\n",
    "        self.notebooks = self.db['notebooks']\n",
    "        self.datasets = self.db['datasets']\n",
    "        self.user_profiles = self.db['user_profiles']\n",
    "        self.competition_history = self.db['competition_history']\n",
    "        \n",
    "        # Create indexes for faster lookups\n",
    "        self.competitions.create_index('competition_id', unique=True)\n",
    "        self.notebooks.create_index([('notebook_id', 1), ('competition_id', 1)], unique=True)\n",
    "        self.datasets.create_index([('dataset_id', 1), ('competition_id', 1)], unique=True)\n",
    "        self.user_profiles.create_index('user_id', unique=True)\n",
    "        self.competition_history.create_index([('user_id', 1)], unique=True)  # One active competition per user\n",
    "\n",
    "    # Competition methods\n",
    "    def create_competition(self, record):\n",
    "        \"\"\"Create a new competition entry.\"\"\"\n",
    "        try:\n",
    "            competition = {\n",
    "                'competition_id': record.get('competition_id'),\n",
    "                'title': record.get('title'),\n",
    "                'description': record.get('description'),\n",
    "                'evaluation': datetime.datetime.now(),\n",
    "                'competition_host': record.get('competition_host', datetime.datetime.now()),\n",
    "                'price_award': record.get('price_award'),\n",
    "                \"entrants\": record.get(\"entrants\"),\n",
    "                \"participants\": record.get(\"participants\"),\n",
    "                \"teams\": record.get(\"teams\"),\n",
    "                \"submissions\": record.get(\"submissions\"),\n",
    "                \"tags\": record.get(\"tags\"),\n",
    "                \"competition_url\": record.get(\"competition_url\"),\n",
    "                \"data_description\": record.get(\"data_description\"),\n",
    "                \"data_files_num\": record.get(\"data_files_num\"),\n",
    "                \"data_size\": record.get(\"data_size\"),\n",
    "                \"data_type\": record.get(\"data_type\"),\n",
    "            }\n",
    "            \n",
    "            self.competitions.update_one(\n",
    "                {'competition_id': competition['competition_id']},\n",
    "                {'$set': competition},\n",
    "                upsert=True\n",
    "            )\n",
    "            \n",
    "            return True\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating competition: {e}\")\n",
    "            return False\n",
    "\n",
    "    # Notebook methods\n",
    "    def import_notebook(self, file_path, competition_id,metadata=None):\n",
    "        \"\"\"Import a single Jupyter notebook (.ipynb) file into MongoDB and associate with a competition.\"\"\"\n",
    "        try:\n",
    "            # Check if competition exists\n",
    "            if not self.get_competition(competition_id):\n",
    "                print(f\"Competition {competition_id} doesn't exist\")\n",
    "                return False\n",
    "                \n",
    "            # Check if file exists and is a Jupyter notebook\n",
    "            if not os.path.isfile(file_path):\n",
    "                print(f\"File {file_path} does not exist\")\n",
    "                return False\n",
    "            \n",
    "            # Extract filename without extension as notebook_id\n",
    "            notebook_id = os.path.splitext(os.path.basename(file_path))[0]\n",
    "            \n",
    "            # Read the notebook file\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                notebook_content = json.load(f)\n",
    "            # Prepare notebook document with metadata\n",
    "            notebook_doc = {\n",
    "                'notebook_id': notebook_id,\n",
    "                'competition_id': competition_id,\n",
    "                'content': notebook_content,\n",
    "                'last_updated': datetime.datetime.now()\n",
    "            }\n",
    "            \n",
    "            # Add metadata if available\n",
    "            if metadata:\n",
    "                notebook_doc['metadata'] = {\n",
    "                    'url': metadata.get('url', ''),\n",
    "                    'score': metadata.get('score', 0),\n",
    "                    'votes': metadata.get('votes', 0),\n",
    "                    'comments': metadata.get('comments', 0),\n",
    "                    'date_created': metadata.get('date_created', ''),\n",
    "                    'sort_by': metadata.get('sort_by', '')\n",
    "                }\n",
    "            \n",
    "            # Store the notebook in MongoDB with competition association\n",
    "            self.notebooks.update_one(\n",
    "                {'notebook_id': notebook_id, 'competition_id': competition_id},\n",
    "                {'$set': notebook_doc},\n",
    "                upsert=True\n",
    "            )\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error importing notebook {file_path}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def import_notebook_folder(self, folder_path, competition_id):\n",
    "        \"\"\"Import all Jupyter notebooks from a folder into MongoDB for a specific competition.\"\"\"\n",
    "        # Make sure folder path exists\n",
    "        if not os.path.isdir(folder_path):\n",
    "            print(f\"Error: {folder_path} is not a valid directory\")\n",
    "            return 0, 0\n",
    "        metadata_file = os.path.join(folder_path,competition_id, \"metadata\", \"all_notebooks_metadata.json\")\n",
    "        metadata_dict = {}\n",
    "        \n",
    "        # Load metadata if available\n",
    "        with open(metadata_file, 'r') as f:\n",
    "            metadata_dict = json.load(f)\n",
    "        success_count = 0\n",
    "        for url, metadata in metadata_dict.items():\n",
    "            notebook_name = metadata.get('notebook_name')\n",
    "            if not notebook_name:\n",
    "                print(f\"Skipping entry with missing notebook_name in metadata: {url}\")\n",
    "                continue\n",
    "\n",
    "            # Possible file locations\n",
    "            possible_paths = [\n",
    "                os.path.join(folder_path,competition_id, f\"{notebook_name}.ipynb\"),\n",
    "            ]\n",
    "            notebook_file = next((p for p in possible_paths if os.path.isfile(p)), None)\n",
    "\n",
    "            if not notebook_file:\n",
    "                print(f\"Notebook file not found for {notebook_name}\")\n",
    "                continue\n",
    "\n",
    "            # Add URL back to metadata\n",
    "            metadata['url'] = url\n",
    "\n",
    "            if self.import_notebook(notebook_file, competition_id, metadata):\n",
    "                success_count += 1\n",
    "                print(f\"Imported notebook: {notebook_name}\")\n",
    "            else:\n",
    "                print(f\"Failed to import notebook: {notebook_name}\")\n",
    "        \n",
    "        print(f\"Successfully imported {success_count} of {len(metadata_dict)} notebooks\")\n",
    "        return success_count, len(metadata_dict)\n",
    "    \n",
    "    # Dataset methods\n",
    "    def import_dataset(self, file_path, competition_id, dataset_type='train'):\n",
    "        \"\"\"Import a dataset (CSV file) into MongoDB and associate with a competition.\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): Path to the CSV file\n",
    "            competition_id (str): ID of the competition to associate with\n",
    "            dataset_type (str): Type of dataset - 'train', 'test', or 'validation'\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Check if competition exists\n",
    "            if not self.get_competition(competition_id):\n",
    "                print(f\"Competition {competition_id} doesn't exist\")\n",
    "                return False\n",
    "                \n",
    "            # Check if file is a CSV\n",
    "            if not file_path.endswith('.csv'):\n",
    "                print(f\"Skipping {file_path}: Not a CSV file\")\n",
    "                return False\n",
    "            \n",
    "            # Extract filename without extension as dataset_id\n",
    "            dataset_id = os.path.splitext(os.path.basename(file_path))[0]\n",
    "            \n",
    "            # Read the CSV file\n",
    "            df = pd.read_csv(file_path)\n",
    "            records = df.to_dict('records')\n",
    "            \n",
    "            # Store dataset metadata\n",
    "            metadata = {\n",
    "                'dataset_id': dataset_id,\n",
    "                'competition_id': competition_id,\n",
    "                'dataset_type': dataset_type,\n",
    "                'original_filename': os.path.basename(file_path),\n",
    "                'row_count': len(records),\n",
    "                'column_names': list(df.columns),\n",
    "                'import_date': datetime.datetime.now()\n",
    "            }\n",
    "            \n",
    "            # Store the dataset in MongoDB\n",
    "            self.datasets.update_one(\n",
    "                {'dataset_id': dataset_id, 'competition_id': competition_id},\n",
    "                {\n",
    "                    '$set': {\n",
    "                        'metadata': metadata,\n",
    "                        'records': records\n",
    "                    }\n",
    "                },\n",
    "                upsert=True\n",
    "            )\n",
    "            \n",
    "            print(f\"Imported {dataset_type} dataset: {dataset_id} with {len(records)} records for competition: {competition_id}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error importing dataset {file_path}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    # User profile methods\n",
    "    def create_user_profile(self, username, email, additional_info=None):\n",
    "        \"\"\"Create a new user profile with a generated UUID.\"\"\"\n",
    "        try:\n",
    "            user_id = str(uuid.uuid4())\n",
    "            profile = {\n",
    "                'user_id': user_id,\n",
    "                'username': username,\n",
    "                'email': email,\n",
    "                'created_at': datetime.datetime.now(),\n",
    "                'last_login': datetime.datetime.now(),\n",
    "                'current_competition_id': None  # User is not in any competition initially\n",
    "            }\n",
    "            \n",
    "            # Add any additional info if provided\n",
    "            if additional_info and isinstance(additional_info, dict):\n",
    "                profile.update(additional_info)\n",
    "            \n",
    "            self.user_profiles.insert_one(profile)\n",
    "            print(f\"Created user profile for {username} with ID: {user_id}\")\n",
    "            return user_id\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating user profile: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_user_profile(self, user_id):\n",
    "        \"\"\"Retrieve user profile by user_id.\"\"\"\n",
    "        return self.user_profiles.find_one({'user_id': user_id}, {'_id': 0})\n",
    "    \n",
    "    def update_user_profile(self, user_id, update_data):\n",
    "        \"\"\"Update a user profile with new data.\"\"\"\n",
    "        try:\n",
    "            result = self.user_profiles.update_one(\n",
    "                {'user_id': user_id},\n",
    "                {'$set': update_data}\n",
    "            )\n",
    "            if result.modified_count > 0:\n",
    "                print(f\"Updated user profile for {user_id}\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"User profile {user_id} not found or no changes needed\")\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            print(f\"Error updating user profile: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def set_user_active_competition(self, user_id, competition_id):\n",
    "        \"\"\"Set the active competition for a user.\"\"\"\n",
    "        try:\n",
    "            # Verify both user and competition exist\n",
    "            user = self.user_profiles.find_one({'user_id': user_id})\n",
    "            if not user:\n",
    "                print(f\"User {user_id} not found\")\n",
    "                return False\n",
    "                \n",
    "            competition = self.competitions.find_one({'competition_id': competition_id})\n",
    "            if not competition:\n",
    "                print(f\"Competition {competition_id} not found\")\n",
    "                return False\n",
    "            \n",
    "            # Update user's active competition\n",
    "            result = self.user_profiles.update_one(\n",
    "                {'user_id': user_id},\n",
    "                {'$set': {\n",
    "                    'current_competition_id': competition_id,\n",
    "                    'competition_joined_at': datetime.datetime.now()\n",
    "                }}\n",
    "            )\n",
    "            \n",
    "            # Initialize or update the competition history for this user\n",
    "            self.competition_history.update_one(\n",
    "                {'user_id': user_id},\n",
    "                {'$set': {\n",
    "                    'user_id': user_id,\n",
    "                    'competition_id': competition_id,\n",
    "                    'start_time': datetime.datetime.now(),\n",
    "                    'last_activity': datetime.datetime.now(),\n",
    "                    'current_round': 1,\n",
    "                    'conversation_history': [],\n",
    "                    'submission_history': []\n",
    "                }},\n",
    "                upsert=True\n",
    "            )\n",
    "            \n",
    "            print(f\"Set active competition {competition_id} for user {user_id}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error setting active competition: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def get_user_active_competition(self, user_id):\n",
    "        \"\"\"Get the active competition for a user.\"\"\"\n",
    "        user = self.user_profiles.find_one({'user_id': user_id}, {'current_competition_id': 1, '_id': 0})\n",
    "        if user and user.get('current_competition_id'):\n",
    "            return self.competitions.find_one(\n",
    "                {'competition_id': user['current_competition_id']},\n",
    "                {'_id': 0}\n",
    "            )\n",
    "        return None\n",
    "    \n",
    "    # Competition history methods\n",
    "    def log_conversation(self, user_id, message, is_user=True):\n",
    "        \"\"\"Log a conversation message between user and chatbot for the active competition.\"\"\"\n",
    "        try:\n",
    "            # Get user's active competition\n",
    "            user = self.user_profiles.find_one({'user_id': user_id})\n",
    "            if not user or not user.get('current_competition_id'):\n",
    "                print(f\"User {user_id} not found or has no active competition\")\n",
    "                return False\n",
    "            \n",
    "            # Create message entry\n",
    "            message_entry = {\n",
    "                'timestamp': datetime.datetime.now(),\n",
    "                'is_user': is_user,\n",
    "                'content': message\n",
    "            }\n",
    "            \n",
    "            # Add message to conversation history\n",
    "            result = self.competition_history.update_one(\n",
    "                {'user_id': user_id},\n",
    "                {\n",
    "                    '$push': {'conversation_history': message_entry},\n",
    "                    '$set': {'last_activity': datetime.datetime.now()}\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            if result.modified_count > 0:\n",
    "                print(f\"Logged conversation for user {user_id}\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"Competition history for user {user_id} not found\")\n",
    "                return False\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error logging conversation: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def advance_competition_round(self, user_id):\n",
    "        \"\"\"Advance to the next round in the competition for a user.\"\"\"\n",
    "        try:\n",
    "            # Get current competition history\n",
    "            history = self.competition_history.find_one({'user_id': user_id})\n",
    "            if not history:\n",
    "                print(f\"Competition history for user {user_id} not found\")\n",
    "                return False\n",
    "            \n",
    "            # Advance to next round\n",
    "            current_round = history.get('current_round', 1)\n",
    "            next_round = current_round + 1\n",
    "            \n",
    "            result = self.competition_history.update_one(\n",
    "                {'user_id': user_id},\n",
    "                {\n",
    "                    '$set': {\n",
    "                        'current_round': next_round,\n",
    "                        'round_start_time': datetime.datetime.now()\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            if result.modified_count > 0:\n",
    "                print(f\"Advanced user {user_id} to round {next_round}\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"Failed to advance round for user {user_id}\")\n",
    "                return False\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error advancing competition round: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def log_submission(self, user_id, notebook_id=None, score=None, submission_notes=None):\n",
    "        \"\"\"Log a competition submission for a user.\"\"\"\n",
    "        try:\n",
    "            # Get user's active competition\n",
    "            user = self.user_profiles.find_one({'user_id': user_id})\n",
    "            if not user or not user.get('current_competition_id'):\n",
    "                print(f\"User {user_id} not found or has no active competition\")\n",
    "                return False\n",
    "            \n",
    "            competition_id = user['current_competition_id']\n",
    "            \n",
    "            # Get current competition history\n",
    "            history = self.competition_history.find_one({'user_id': user_id})\n",
    "            if not history:\n",
    "                print(f\"Competition history for user {user_id} not found\")\n",
    "                return False\n",
    "            \n",
    "            # Create submission entry\n",
    "            submission = {\n",
    "                'submission_id': str(uuid.uuid4()),\n",
    "                'timestamp': datetime.datetime.now(),\n",
    "                'round': history.get('current_round', 1),\n",
    "                'notebook_id': notebook_id,\n",
    "                'score': score,\n",
    "                'notes': submission_notes\n",
    "            }\n",
    "            \n",
    "            # Add submission to history\n",
    "            result = self.competition_history.update_one(\n",
    "                {'user_id': user_id},\n",
    "                {\n",
    "                    '$push': {'submission_history': submission},\n",
    "                    '$set': {'last_activity': datetime.datetime.now()}\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            if result.modified_count > 0:\n",
    "                print(f\"Logged submission for user {user_id} in competition {competition_id}\")\n",
    "                return submission['submission_id']\n",
    "            else:\n",
    "                print(f\"Failed to log submission for user {user_id}\")\n",
    "                return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error logging submission: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_user_competition_history(self, user_id):\n",
    "        \"\"\"Get the complete competition history for a user.\"\"\"\n",
    "        return self.competition_history.find_one({'user_id': user_id}, {'_id': 0})\n",
    "    \n",
    "    def get_user_conversation_history(self, user_id, limit=None):\n",
    "        \"\"\"Get conversation history for a user's active competition.\"\"\"\n",
    "        history = self.competition_history.find_one({'user_id': user_id}, {'conversation_history': 1, '_id': 0})\n",
    "        if history and 'conversation_history' in history:\n",
    "            conversations = history['conversation_history']\n",
    "            if limit:\n",
    "                return conversations[-limit:]\n",
    "            return conversations\n",
    "        return []\n",
    "    \n",
    "    def end_competition_session(self, user_id, final_score=None, notes=None):\n",
    "        \"\"\"End a user's competition session.\"\"\"\n",
    "        try:\n",
    "            # Get user's active competition\n",
    "            user = self.user_profiles.find_one({'user_id': user_id})\n",
    "            if not user or not user.get('current_competition_id'):\n",
    "                print(f\"User {user_id} not found or has no active competition\")\n",
    "                return False\n",
    "            \n",
    "            competition_id = user['current_competition_id']\n",
    "            \n",
    "            # Update competition history\n",
    "            end_time = datetime.datetime.now()\n",
    "            \n",
    "            result = self.competition_history.update_one(\n",
    "                {'user_id': user_id},\n",
    "                {\n",
    "                    '$set': {\n",
    "                        'end_time': end_time,\n",
    "                        'final_score': final_score,\n",
    "                        'completion_notes': notes,\n",
    "                        'status': 'completed'\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # Clear the user's active competition\n",
    "            self.user_profiles.update_one(\n",
    "                {'user_id': user_id},\n",
    "                {'$set': {'current_competition_id': None}}\n",
    "            )\n",
    "            \n",
    "            if result.modified_count > 0:\n",
    "                print(f\"Ended competition session for user {user_id}\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"Failed to end competition session for user {user_id}\")\n",
    "                return False\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error ending competition session: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def get_competition(self, competition_id):\n",
    "        \"\"\"Get competition details by ID.\"\"\"\n",
    "        return self.competitions.find_one({'competition_id': competition_id}, {'_id': 0})\n",
    "    \n",
    "    def list_competitions(self):\n",
    "        \"\"\"List all competitions.\"\"\"\n",
    "        return list(self.competitions.find({}, {'_id': 0}))\n",
    "    \n",
    "    def get_competition_datasets(self, competition_id):\n",
    "        \"\"\"Get all datasets associated with a competition.\"\"\"\n",
    "        return list(self.datasets.find({'competition_id': competition_id}, {'_id': 0}))\n",
    "    \n",
    "    def get_competition_notebooks(self, competition_id):\n",
    "        \"\"\"Get all notebooks associated with a competition.\"\"\"\n",
    "        return list(self.notebooks.find({'competition_id': competition_id}, {'_id': 0, 'content': 0}))\n",
    "    def get_notebook(self, notebook_id, competition_id):\n",
    "        \"\"\"Retrieve a notebook by its ID and competition ID.\"\"\"\n",
    "        return self.notebooks.find_one({\n",
    "            'notebook_id': notebook_id,\n",
    "            'competition_id': competition_id\n",
    "        }, {'_id': 0})\n",
    "    \n",
    "    def export_notebook(self, notebook_id, competition_id, output_path):\n",
    "        \"\"\"Export a notebook to a .ipynb file.\"\"\"\n",
    "        notebook = self.get_notebook(notebook_id, competition_id)\n",
    "        if notebook and 'content' in notebook:\n",
    "            try:\n",
    "                with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(notebook['content'], f)\n",
    "                print(f\"Notebook exported to {output_path}\")\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                print(f\"Error exporting notebook: {e}\")\n",
    "                return False\n",
    "        else:\n",
    "            print(f\"Notebook {notebook_id} for competition {competition_id} not found or has no content\")\n",
    "            return False\n",
    "        \n",
    "        \n",
    "    def get_dataset(self, dataset_id, competition_id):\n",
    "        \"\"\"Retrieve a dataset by its ID and competition ID.\"\"\"\n",
    "        return self.datasets.find_one({\n",
    "            'metadata.dataset_id': dataset_id,\n",
    "            'metadata.competition_id': competition_id\n",
    "        }, {'_id': 0})\n",
    "    \n",
    "    def get_competition_dataset_by_type(self, competition_id, dataset_type='train'):\n",
    "        \"\"\"Get a specific type of dataset for a competition.\"\"\"\n",
    "        return self.datasets.find_one({\n",
    "            'metadata.competition_id': competition_id,\n",
    "            'metadata.dataset_type': dataset_type\n",
    "        }, {'_id': 0})\n",
    "    \n",
    "    def get_dataset_as_dataframe(self, dataset_id, competition_id):\n",
    "        \"\"\"Retrieve a dataset as a pandas DataFrame.\"\"\"\n",
    "        dataset = self.get_dataset(dataset_id, competition_id)\n",
    "        if dataset and 'records' in dataset:\n",
    "            return pd.DataFrame(dataset['records'])\n",
    "        return None\n",
    "    \n",
    "    def export_dataset(self, dataset_id, competition_id, output_path):\n",
    "        \"\"\"Export a dataset to a CSV file.\"\"\"\n",
    "        df = self.get_dataset_as_dataframe(dataset_id, competition_id)\n",
    "        if df is not None:\n",
    "            try:\n",
    "                df.to_csv(output_path, index=False)\n",
    "                print(f\"Dataset exported to {output_path}\")\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                print(f\"Error exporting dataset: {e}\")\n",
    "                return False\n",
    "        else:\n",
    "            print(f\"Dataset {dataset_id} for competition {competition_id} not found\")\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "060a798a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = DataManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655e2c1b",
   "metadata": {},
   "source": [
    "# Create Compeition Collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3debc3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported competition: titanic\n",
      "\n",
      "Summary: Successfully imported 1 of 1 competitions\n"
     ]
    }
   ],
   "source": [
    "with open('/Users/zhongming/Local Docs/Github/Kaggle_RAG_dataset/data/test/competitions_metadata.json','r') as file:\n",
    "    competitions_data = json.load(file)\n",
    "\n",
    "# Import each competition\n",
    "success_count = 0\n",
    "for competition_id, competition_info in competitions_data.items():\n",
    "    # Create a competition record with required fields\n",
    "    competition_record = {\n",
    "        'competition_id': competition_id,\n",
    "        'title': competition_id.title(),  # Use ID as title if not provided\n",
    "        'description': competition_info.get('Description', ''),\n",
    "        'evaluation': competition_info.get('Evaluation', ''),\n",
    "        'competition_host': competition_info.get('Competition Host', []),\n",
    "        'price_award': competition_info.get('Prizes & Awards', []),\n",
    "        'entrants': competition_info.get('Entrants', 0),\n",
    "        'participants': competition_info.get('Participants', 0),\n",
    "        'teams': competition_info.get('Teams', 0),\n",
    "        'submissions': competition_info.get('Submissions', 0),\n",
    "        'tags': competition_info.get('Tags', []),\n",
    "        'competition_url': competition_info.get('competition_url', '')\n",
    "    }\n",
    "    \n",
    "    # Add data description if available\n",
    "    if 'data' in competition_info:\n",
    "        data_info = competition_info['data']\n",
    "        competition_record['data_description'] = data_info.get('Description', '')\n",
    "        competition_record['data_files_num'] = data_info.get('Files', '')\n",
    "        competition_record['data_size'] = data_info.get('Size', '')\n",
    "        competition_record['data_type'] = data_info.get('Type', '')\n",
    "    \n",
    "    # Create competition in MongoDB\n",
    "    if dm.create_competition(competition_record):\n",
    "        success_count += 1\n",
    "        print(f\"Successfully imported competition: {competition_id}\")\n",
    "    else:\n",
    "        print(f\"Failed to import competition: {competition_id}\")\n",
    "\n",
    "print(f\"\\nSummary: Successfully imported {success_count} of {len(competitions_data)} competitions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "928ffafe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'competition_id': 'titanic',\n",
       " 'competition_host': ['Kaggle'],\n",
       " 'competition_url': 'https://www.kaggle.com/competitions/titanic',\n",
       " 'data_description': 'Dataset Description\\nOverview\\nThe data has been split into two groups:\\ntraining set (train.csv)\\ntest set (test.csv)\\nThe training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the ‚Äúground truth‚Äù) for each passenger. Your model will be based on ‚Äúfeatures‚Äù like passengers‚Äô gender and class. You can also use feature engineering to create new features.\\nThe test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.\\nWe also include gender_submission.csv, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.\\nData Dictionary\\nVariable Definition Key\\nsurvival Survival 0 = No, 1 = Yes\\npclass Ticket class 1 = 1st, 2 = 2nd, 3 = 3rd\\nsex Sex\\nAge Age in years\\nsibsp # of siblings / spouses aboard the Titanic\\nparch # of parents / children aboard the Titanic\\nticket Ticket number\\nfare Passenger fare\\ncabin Cabin number\\nembarked Port of Embarkation C = Cherbourg, Q = Queenstown, S = Southampton\\nVariable Notes\\npclass: A proxy for socio-economic status (SES)\\n1st = Upper\\n2nd = Middle\\n3rd = Lower\\n\\nage: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\\n\\nsibsp: The dataset defines family relations in this way...\\nSibling = brother, sister, stepbrother, stepsister\\nSpouse = husband, wife (mistresses and fianc√©s were ignored)\\n\\nparch: The dataset defines family relations in this way...\\nParent = mother, father\\nChild = daughter, son, stepdaughter, stepson\\nSome children travelled only with a nanny, therefore parch=0 for them.',\n",
       " 'data_files_num': '3 files',\n",
       " 'data_size': '93.08 kB',\n",
       " 'data_type': 'csv',\n",
       " 'description': 'Description\\nlink\\nkeyboard_arrow_up\\nüëãüõ≥Ô∏è Ahoy, welcome to Kaggle! You‚Äôre in the right place.\\nThis is the legendary Titanic ML competition ‚Äì the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\\nIf you want to talk with other users about this competition, come join our Discord! We\\'ve got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here: https://discord.gg/kaggle\\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\\nRead on or watch the video below to explore more details. Once you‚Äôre ready to start competing, click on the \"Join Competition button to create an account and gain access to the competition data. Then check out Alexis Cook‚Äôs Titanic Tutorial that walks you through step by step how to make your first submission!\\nThe Challenge\\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\\nOn April 15, 1912, during her maiden voyage, the widely considered ‚Äúunsinkable‚Äù RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren‚Äôt enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\\nIn this challenge, we ask you to build a predictive model that answers the question: ‚Äúwhat sorts of people were more likely to survive?‚Äù using passenger data (ie name, age, gender, socio-economic class, etc).\\nRecommended Tutorial\\nWe highly recommend Alexis Cook‚Äôs Titanic Tutorial that walks you through making your very first submission step by step and this starter notebook to get started.\\nHow Kaggle‚Äôs Competitions Work\\nJoin the Competition\\nRead about the challenge description, accept the Competition Rules and gain access to the competition dataset.\\nGet to Work\\nDownload the data, build models on it locally or on Kaggle Notebooks (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.\\nMake a Submission\\nUpload your prediction as a submission on Kaggle and receive an accuracy score.\\nCheck the Leaderboard\\nSee how your model ranks against other Kagglers on our leaderboard.\\nImprove Your Score\\nCheck out the discussion forum to find lots of tutorials and insights from other competitors.\\nKaggle Lingo Video\\nYou may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out Dr. Rachael Tatman‚Äôs video on Kaggle Lingo to get up to speed!\\nWhat Data Will I Use in This Competition?\\nIn this competition, you‚Äôll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled train.csv and the other is titled test.csv.\\nTrain.csv will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the ‚Äúground truth‚Äù.\\nThe test.csv dataset contains similar information but does not disclose the ‚Äúground truth‚Äù for each passenger. It‚Äôs your job to predict these outcomes.\\nUsing the patterns you find in the train.csv data, predict whether the other 418 passengers on board (found in test.csv) survived.\\nCheck out the ‚ÄúData‚Äù tab to explore the datasets even further. Once you feel you‚Äôve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.\\nHow to Submit your Prediction to Kaggle\\nOnce you‚Äôre ready to make a submission and get on the leaderboard:\\nClick on the ‚ÄúSubmit Predictions‚Äù button\\nUpload a CSV file in the submission file format. You‚Äôre able to submit 10 submissions a day.\\nSubmission File Format:\\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.\\nThe file should have exactly 2 columns:\\nPassengerId (sorted in any order)\\nSurvived (contains your binary predictions: 1 for survived, 0 for deceased)\\nGot it! I‚Äôm ready to get started. Where do I get help if I need it?\\nFor Competition Help: Titanic Discussion Forum\\nKaggle doesn‚Äôt have a dedicated team to help troubleshoot your code so you‚Äôll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you\\'ll find that others will share a lot in turn!\\nA Last Word on Kaggle Notebooks\\nAs we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data & code.\\nIn every competition, you‚Äôll find many Notebooks shared with incredible insights. It‚Äôs an invaluable resource worth becoming familiar with. Check out this competition‚Äôs Notebooks here.\\nüèÉ\\u200d‚ôÄReady to Compete? Join the Competition Here!',\n",
       " 'entrants': 1363616,\n",
       " 'evaluation': datetime.datetime(2025, 4, 18, 13, 10, 37, 354000),\n",
       " 'participants': 16223,\n",
       " 'price_award': ['Knowledge', 'Does not award Points or Medals'],\n",
       " 'submissions': 60818,\n",
       " 'tags': ['Binary Classification',\n",
       "  'Tabular',\n",
       "  'Beginner',\n",
       "  'Categorization Accuracy'],\n",
       " 'teams': 16162,\n",
       " 'title': 'Titanic'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.get_competition('titanic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21653bbe",
   "metadata": {},
   "source": [
    "# Create dataset collection for train and test set of competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97ef1ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/Users/zhongming/Local Docs/Github/Kaggle_RAG_dataset/data/dataset' \n",
    "# Get all competition directories\n",
    "competition_dirs = [d for d in os.listdir(base_dir) \n",
    "                    if os.path.isdir(os.path.join(base_dir, d))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476c9709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing competition: titanic\n",
      "Found 3 CSV files\n",
      "Importing test.csv as test dataset...\n",
      "Imported test dataset: test with 418 records for competition: titanic\n",
      "Successfully imported test.csv\n",
      "Importing train.csv as train dataset...\n",
      "Imported train dataset: train with 891 records for competition: titanic\n",
      "Successfully imported train.csv\n",
      "Importing gender_submission.csv as unknown dataset...\n",
      "Imported unknown dataset: gender_submission with 418 records for competition: titanic\n",
      "Successfully imported gender_submission.csv\n",
      "Imported 3 of 3 datasets for competition titanic\n"
     ]
    }
   ],
   "source": [
    "base_dir = '/Users/zhongming/Local Docs/Github/Kaggle_RAG_dataset/data/dataset' \n",
    "# Get all competition directories\n",
    "competition_dirs = [d for d in os.listdir(base_dir) \n",
    "                    if os.path.isdir(os.path.join(base_dir, d))]\n",
    "\n",
    "total_datasets = 0\n",
    "total_imported = 0\n",
    "\n",
    "# Process each competition directory\n",
    "for competition_id in competition_dirs:\n",
    "    competition_dir = os.path.join(base_dir, competition_id)\n",
    "    \n",
    "    # Check if competition exists in the database\n",
    "    competition = dm.get_competition(competition_id)\n",
    "    if not competition:\n",
    "        print(f\"Competition {competition_id} does not exist in the database, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    # Find all CSV files in the competition directory\n",
    "    csv_pattern = os.path.join(competition_dir, \"*.csv\")\n",
    "    csv_files = glob.glob(csv_pattern)\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(f\"No CSV files found for competition {competition_id}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nProcessing competition: {competition_id}\")\n",
    "    print(f\"Found {len(csv_files)} CSV files\")\n",
    "    \n",
    "    # Import each CSV file\n",
    "    imported_count = 0\n",
    "    for csv_file in csv_files:\n",
    "        file_name = os.path.basename(csv_file)\n",
    "        base_name = os.path.splitext(file_name)[0]\n",
    "        \n",
    "        # Determine dataset type based on filename\n",
    "        dataset_type = 'unknown'\n",
    "        if 'train' in base_name.lower():\n",
    "            dataset_type = 'train'\n",
    "        elif 'test' in base_name.lower():\n",
    "            dataset_type = 'test'\n",
    "        elif 'valid' in base_name.lower() or 'val' in base_name.lower():\n",
    "            dataset_type = 'validation'\n",
    "        \n",
    "        print(f\"Importing {file_name} as {dataset_type} dataset...\")\n",
    "        \n",
    "        # Import dataset\n",
    "        if dm.import_dataset(csv_file, competition_id, dataset_type):\n",
    "            imported_count += 1\n",
    "            print(f\"Successfully imported {file_name}\")\n",
    "        else:\n",
    "            print(f\"Failed to import {file_name}\")\n",
    "    \n",
    "    print(f\"Imported {imported_count} of {len(csv_files)} datasets for competition {competition_id}\")\n",
    "    \n",
    "    total_datasets += len(csv_files)\n",
    "    total_imported += imported_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "deb9ad5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Montvila, Rev. Juozas</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211536</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Graham, Miss. Margaret Edith</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112053</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>B42</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>W./C. 6607</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111369</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C148</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Dooley, Mr. Patrick</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>370376</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows √ó 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass  \\\n",
       "0              1         0       3   \n",
       "1              2         1       1   \n",
       "2              3         1       3   \n",
       "3              4         1       1   \n",
       "4              5         0       3   \n",
       "..           ...       ...     ...   \n",
       "886          887         0       2   \n",
       "887          888         1       1   \n",
       "888          889         0       3   \n",
       "889          890         1       1   \n",
       "890          891         0       3   \n",
       "\n",
       "                                                  Name     Sex   Age  SibSp  \\\n",
       "0                              Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                               Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                             Allen, Mr. William Henry    male  35.0      0   \n",
       "..                                                 ...     ...   ...    ...   \n",
       "886                              Montvila, Rev. Juozas    male  27.0      0   \n",
       "887                       Graham, Miss. Margaret Edith  female  19.0      0   \n",
       "888           Johnston, Miss. Catherine Helen \"Carrie\"  female   NaN      1   \n",
       "889                              Behr, Mr. Karl Howell    male  26.0      0   \n",
       "890                                Dooley, Mr. Patrick    male  32.0      0   \n",
       "\n",
       "     Parch            Ticket     Fare Cabin Embarked  \n",
       "0        0         A/5 21171   7.2500   NaN        S  \n",
       "1        0          PC 17599  71.2833   C85        C  \n",
       "2        0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3        0            113803  53.1000  C123        S  \n",
       "4        0            373450   8.0500   NaN        S  \n",
       "..     ...               ...      ...   ...      ...  \n",
       "886      0            211536  13.0000   NaN        S  \n",
       "887      0            112053  30.0000   B42        S  \n",
       "888      2        W./C. 6607  23.4500   NaN        S  \n",
       "889      0            111369  30.0000  C148        C  \n",
       "890      0            370376   7.7500   NaN        Q  \n",
       "\n",
       "[891 rows x 12 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dm.get_competition_dataset_by_type('titanic','train')['records'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e79e10",
   "metadata": {},
   "source": [
    "# Create notebook collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "89ec482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = DataManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "53952e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/Users/zhongming/Local Docs/Github/Kaggle_RAG_dataset/data/notebooks'\n",
    "# Get all competition directories\n",
    "competition_dirs = [d for d in os.listdir(base_dir) \n",
    "                    if os.path.isdir(os.path.join(base_dir, d))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9d463890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported notebook: vladimirsydor_add-leak\n",
      "Imported notebook: yunishi0716_best-weight-searching3\n",
      "Imported notebook: aleksthegreat_public-blend\n",
      "Imported notebook: yamsam_ashrae-leak-validation-and-more\n",
      "Imported notebook: wuliaokaola_ashrae-maybe-this-can-make-public-lb-some-useful\n",
      "Imported notebook: vladimirsydor_bland-lgbm-on-leaks\n",
      "Imported notebook: vladimirsydor_bland-by-leak\n",
      "Imported notebook: rohanrao_ashrae-divide-and-conquer\n",
      "Imported notebook: teeyee314_best-single-half-half-lgbm-1-07\n",
      "Imported notebook: vladimirsydor_bland-lgbm-folds\n",
      "Imported notebook: mimoudata_ashrae-2-lightgbm-without-leak-data\n",
      "Imported notebook: aitude_ashrae-kfold-lightgbm-without-leak-1-08\n",
      "Imported notebook: purist1024_ashrae-simple-data-cleanup-lb-1-08-no-leaks\n",
      "Imported notebook: ragnar123_another-1-08-lb-no-leak\n",
      "Imported notebook: mimoudata_ashrae-lightgbm-without-leak\n",
      "Imported notebook: yunishi0716_k-folds-model\n",
      "Imported notebook: hmendonca_4-ashrae-blended\n",
      "Imported notebook: grapestone5321_ashrae-stacking-method\n",
      "Imported notebook: mimoudata_ashrae-lightgbm-without-leak-data\n",
      "Imported notebook: iwatatakuya_ashrae-kfold-lightgbm-without-building-id\n",
      "Imported notebook: remisharoon_ashrae-gep-iii-rms-nb\n",
      "Notebook file not found for kailex_ac-dc\n",
      "Imported notebook: rohanrao_ashrae-half-and-half\n",
      "Imported notebook: litemort_implicit-merge-operation-by-litemort\n",
      "Imported notebook: kulkarnivishwanath_ashrae-great-energy-predictor-iii-eda-model\n",
      "Imported notebook: starl1ght_ashrae-stacked-regression-lasso-ridge-lgbm\n",
      "Imported notebook: hiteshsom_ashrae-3-lightgbm\n",
      "Imported notebook: nz0722_aligned-timestamp-lgbm-by-meter-type\n",
      "Imported notebook: yunishi0716_3-folds-by-each-meter-type\n",
      "Imported notebook: kaushal2896_ashrae-eda-fe-lightgbm-1-12\n",
      "Imported notebook: viswajithkn_great-energy-prediction\n",
      "Notebook file not found for wittmannf_keras-embedding-read-k-folds\n",
      "Imported notebook: corochann_ashrae-training-lgbm-by-meter-type\n",
      "Imported notebook: amaity0_ashrae-fifth-try\n",
      "Imported notebook: isaienkov_keras-nn-with-embeddings-for-cat-features-1-15\n",
      "Imported notebook: patelatharva_prediction\n",
      "Imported notebook: zeynepkurban_ashrae-2\n",
      "Imported notebook: enigola_ashrae-ml-hw6-lgbm\n",
      "Imported notebook: clementut_kernel4e51c0227f\n",
      "Imported notebook: morituri_lgbm-baseline\n",
      "Imported notebook: shukla84manish_energy-consumption\n",
      "Imported notebook: madisj_kernel3377148266\n",
      "Imported notebook: darisdzakwanhoesien2_ashrae-great-energy-predictor-iii\n",
      "Imported notebook: kimtaegwan_what-s-your-cv-method\n",
      "Imported notebook: vladimirsydor_randomforestbaseline\n",
      "Imported notebook: mayer79_ashrae-lgb-starter-for-r\n",
      "Imported notebook: jiaofenx_ashrae-great-energy-predictor-iii\n",
      "Imported notebook: gouherdanishiitkgp_ashrae-basic-eda-and-feature-engineering\n",
      "Imported notebook: yshiml_ashrae-simple-lgbm-optuna-baseline\n",
      "Imported notebook: michelezoccali_ashrae-with-fast-ai-part-3\n",
      "Successfully imported 48 of 50 notebooks\n",
      "Imported notebook: cuijamm_allstate-claims-severity-score-1113-12994\n",
      "Notebook file not found for nitink12_prog-pyth2\n",
      "Notebook file not found for vishallakha_all-trump-state\n",
      "Notebook file not found for casalicchio_tuning-the-parameter-of-a-custom-objective-1120\n",
      "Notebook file not found for aliajouz_singel-model-lb-1117\n",
      "Imported notebook: deepdreamx_lgbm-only-featureinteraction-selected\n",
      "Notebook file not found for xingobar_simple-xgboost-1120-769\n",
      "Notebook file not found for doanducqui_msubx\n",
      "Imported notebook: emilyanderson304326_allstateclaims\n",
      "Notebook file not found for aliajouz_xgb-model\n",
      "Notebook file not found for nitink12_prog-python\n",
      "Imported notebook: tushvjti_eda-allstate\n",
      "Notebook file not found for tobikaggle_h2o-dnn-averaging-in-r\n",
      "Imported notebook: harshitt21_allstate-claims-severity-eda-and-baseline\n",
      "Imported notebook: aakash2121995_new-ensemble\n",
      "Imported notebook: aakash2121995_notebookccc577aa6b\n",
      "Imported notebook: venkateshprabhug_severity-of-insurance-claim\n",
      "Notebook file not found for vecxoz_vecstack-demo\n",
      "Notebook file not found for andreylarionov_simple-gradientboostingregressor-lb-1139\n",
      "Notebook file not found for nirupamkar_xgb-run1\n",
      "Imported notebook: raviprakash438_allstate-claims-severity\n",
      "Notebook file not found for akki0206_xgboost-submission-1\n",
      "Notebook file not found for hbhargava2_xgboost-parameter-tuned-feature-engineering\n",
      "Imported notebook: christianrorholtmoe_all-state-claim-severity-neural-net\n",
      "Imported notebook: bradyheinig_byu-stat-348-final-project\n",
      "Notebook file not found for praveenhegde_xgboost-starter\n",
      "Notebook file not found for rrkc00_xgboost\n",
      "Imported notebook: nightshade7_allstate-severity-test\n",
      "Imported notebook: summershan_allstate-car-claims-severity\n",
      "Imported notebook: rahulpawade_allstate-claims-severity-xgboost-regression\n",
      "Imported notebook: bradenmcritchfield_allstate-claims-severity-boosted-tree\n",
      "Imported notebook: nathanchantry_acs-boosted-trees\n",
      "Notebook file not found for kevinpalm_bumbling-around-in-tensorflow\n",
      "Imported notebook: floser_five-minute-model\n",
      "Imported notebook: julienpantz_regression\n",
      "Notebook file not found for mountaindata_allstate-randomforest\n",
      "Notebook file not found for victorwang_rf-model\n",
      "Notebook file not found for victorwang_rf-model-trial2\n",
      "Imported notebook: zachsabey_allstate-claims-rf\n",
      "Imported notebook: natercox_acs-random-forest\n",
      "Imported notebook: alazark_allstateclaims\n",
      "Imported notebook: wjnkerst_notebookc186b1607b\n",
      "Notebook file not found for samuelhaberthuer_how-about-a-linear-model\n",
      "Notebook file not found for andrecn_allstate-challenge-boosting-gbm\n",
      "Imported notebook: victoriarigby_allstate-linear\n",
      "Imported notebook: arpytanshu_allstate-claims-severity-1260-mae\n",
      "Notebook file not found for xingobar_keras-starer\n",
      "Notebook file not found for victorwang_rd-model\n",
      "Successfully imported 24 of 48 notebooks\n",
      "Imported notebook: mohammadmehdizare_notebooke9568e0edb\n",
      "Imported notebook: samarthpujari_cibmtr-competition\n",
      "Imported notebook: youssefelzahar_cibmtr-survival-forest-c-index-test-data-0-806\n",
      "Imported notebook: vrushabhbidari_predict-1\n",
      "Imported notebook: brianedwards_hct-loo-2\n",
      "Imported notebook: akelsayed_ak-prediction-8\n",
      "Imported notebook: ahmedsamir1598_cibmtr-2ndnote\n",
      "Imported notebook: pedromaiorano_teste\n",
      "Imported notebook: chaki18081999_eda-in-detail-and-random-forest-baseline\n",
      "Imported notebook: yashsahu02_1-cibmtr-equity-in-post-hct-survival-predictions\n",
      "Imported notebook: jaehun123_allogeneic-hct-survival-prediction\n",
      "Imported notebook: trantraa_xgb-with-cross-validation\n",
      "Imported notebook: abhinov037_cibmtr\n",
      "Imported notebook: sheikhomerkashmiri_testing\n",
      "Imported notebook: rohitdileep_lightgbm-kaplanmierfitter\n",
      "Imported notebook: lordxerxes_benchmark-model\n",
      "Imported notebook: claramagdyghaly_cibmttr\n",
      "Imported notebook: shrishh_cibmtr-catboost\n",
      "Imported notebook: muhammadimran112233_cibmtr-baseline-code\n",
      "Imported notebook: inabower_cibmtr-submission-test\n",
      "Imported notebook: sabyrbazarymbetov_cibmtr-baseline\n",
      "Imported notebook: mhkamangoviii_lightgbm-catboost\n",
      "Imported notebook: alicsahmed_cibmtr-machine-learning\n",
      "Imported notebook: noorizzatnassar_noor-nassar\n",
      "Imported notebook: garrickchinnis_equity-in-post-hct-survival-predictions\n",
      "Imported notebook: polygot13_hct-medical-analysis\n",
      "Imported notebook: akelsayed_ak-1-equity-in-post-hct-survival-predictions\n",
      "Imported notebook: nyeinchansoe_cibmtr-xgboost-regression-0-558\n",
      "Imported notebook: livaikira_cibmtr-equity-in-post-hct-survival-predictions\n",
      "Imported notebook: jainilspatel_cibmtr-research-easy\n",
      "Imported notebook: youssefelzahar_cibmtr-random-forest\n",
      "Imported notebook: nabinoli2004_cibmtr\n",
      "Imported notebook: gaganbajwaa_cimbtr-ensemble-kfold\n",
      "Imported notebook: udaken10_classification-regression\n",
      "Imported notebook: jonatanf_cibmtr-neural-network-jonatanf\n",
      "Imported notebook: tomeverson_notebook87e39b37ea\n",
      "Imported notebook: dianalionel_post-hct-survival-prediction\n",
      "Imported notebook: rukmaltharaka_survival-prediction\n",
      "Imported notebook: shresthajeevan_survival-analysis\n",
      "Imported notebook: sheershsrivas_survival-analysis-with-nn\n",
      "Imported notebook: steve179_submission-csv\n",
      "Imported notebook: omarsalah123_notebookeb25fbb1d2\n",
      "Imported notebook: javierojeda71_cibmtr-equity-in-post-hct-survival-predictions\n",
      "Imported notebook: garvio282003_submission-nn-deepsurv-race-mlip26\n",
      "Imported notebook: wjones3668_cibmtr-equity-in-post-hct-survival-predictions\n",
      "Imported notebook: forgetish_cibmtr-simple-eda-and-data-cleaning\n",
      "Imported notebook: nikitalemon_cibmtr\n",
      "Imported notebook: codewithab_testing-hct-survival\n",
      "Imported notebook: tianlulee_xgboost-solution-starter\n",
      "Imported notebook: benjenkins96_understanding-survival-analysis\n",
      "Successfully imported 50 of 50 notebooks\n",
      "\n",
      "Summary: Successfully imported 122 of 148 notebooks\n"
     ]
    }
   ],
   "source": [
    "total_notebooks = 0\n",
    "total_imported = 0\n",
    "\n",
    "# Process each competition directory\n",
    "for competition_id in competition_dirs:\n",
    "    # Import each notebook file\n",
    "    success_count, total_count = dm.import_notebook_folder(base_dir, competition_id)\n",
    "    \n",
    "    total_notebooks += total_count\n",
    "    total_imported += success_count\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\nSummary: Successfully imported {total_imported} of {total_notebooks} notebooks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2866a8d",
   "metadata": {},
   "source": [
    "# Create user profile collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4ec3a011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created user profile for test_user with ID: 8c13b49d-8a41-4dea-9bd4-32f96466a08e\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'8c13b49d-8a41-4dea-9bd4-32f96466a08e'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.create_user_profile('test_user', 'test@test.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e455c385",
   "metadata": {},
   "source": [
    "# Create competition_history collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35d92be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b76e0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c525f50c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35906ee8",
   "metadata": {},
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogenstudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
